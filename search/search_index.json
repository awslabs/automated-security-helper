{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>ASH (Automated Security Helper) is a security scanning tool designed to help you identify potential security issues in your code, infrastructure, and IAM configurations as early as possible in your development process.</p> <ul> <li>ASH is not a replacement for human review or team/customer security standards</li> <li>It leverages lightweight, open-source tools for flexibility and portability</li> <li>ASH v3 has been completely rewritten in Python with significant improvements to usability and functionality</li> </ul> <p></p>"},{"location":"#key-features-in-ash-v3","title":"Key Features in ASH v3","text":"<ul> <li>Python-based CLI: ASH now has a Python-based CLI entrypoint while maintaining backward compatibility with the shell script entrypoint</li> <li>Multiple Execution Modes: Run ASH in <code>local</code>, <code>container</code>, or <code>precommit</code> mode depending on your needs</li> <li>Enhanced Configuration: Support for YAML/JSON configuration files with overrides via CLI parameters</li> <li>Improved Reporting: Multiple report formats including JSON, Markdown, HTML, and CSV</li> <li>Pluggable Architecture: Extend ASH with custom plugins, scanners, and reporters</li> <li>Unified Output Format: Standardized output format that can be exported to multiple formats (SARIF, JSON, HTML, Markdown, CSV)</li> <li>UV Package Management: ASH now uses UV for faster dependency resolution and tool isolation</li> </ul>"},{"location":"#built-in-scanners","title":"Built-In Scanners","text":"<p>ASH v3 integrates multiple open-source security tools as scanners. Tools like Bandit, Checkov, and Semgrep are managed via UV's tool isolation system, which automatically installs and runs them in isolated environments without affecting your project dependencies:</p> Scanner Type Languages/Frameworks Installation (Local Mode) Bandit SAST Python Managed via UV tool isolation (auto-installed: <code>bandit&gt;=1.7.0</code>) Semgrep SAST Python, JavaScript, TypeScript, Java, Go, C#, Ruby, PHP, Kotlin, Swift, Bash, and more Managed via UV tool isolation (auto-installed: <code>semgrep&gt;=1.125.0</code>) detect-secrets Secrets All text files Included with ASH Checkov IaC, SAST Terraform, CloudFormation, Kubernetes, Dockerfile, ARM Templates, Serverless, Helm, and more Managed via UV tool isolation (auto-installed: <code>checkov&gt;=3.2.0,&lt;4.0.0</code>) cfn_nag IaC CloudFormation <code>gem install cfn-nag</code> cdk-nag IaC CloudFormation Included with ASH npm-audit SCA JavaScript/Node.js Install Node.js/npm Grype SCA Python, JavaScript/Node.js, Java, Go, Ruby, and more See Grype Installation Syft SBOM Python, JavaScript/Node.js, Java, Go, Ruby, and more See Syft Installation"},{"location":"#prerequisites","title":"Prerequisites","text":""},{"location":"#runtime-requirements","title":"Runtime Requirements","text":"Mode Requirements Notes Local Python 3.10+, UV package manager Some scanners require additional tools (see table above) Container Any OCI-compatible container runtime (Finch, Docker, Podman, etc.) On Windows: WSL2 is typically required Precommit Python 3.10+, UV package manager Subset of scanners, optimized for speed"},{"location":"#installation-options","title":"Installation Options","text":""},{"location":"#quick-install-recommended","title":"Quick Install (Recommended)","text":"<pre><code># Install with pipx (isolated environment)\npipx install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n\n# Use as normal\nash --help\n</code></pre>"},{"location":"#other-installation-methods","title":"Other Installation Methods","text":"Click to expand other installation options  #### Using `uvx`  <pre><code># Linux/macOS\ncurl -sSf https://astral.sh/uv/install.sh | sh\nalias ash=\"uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\"\n\n# Windows PowerShell\nirm https://astral.sh/uv/install.ps1 | iex\nfunction ash { uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2 $args }\n</code></pre>  #### Using `pip`  <pre><code>pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n</code></pre>  #### Clone the Repository  <pre><code>git clone https://github.com/awslabs/automated-security-helper.git --branch v3.2.2\ncd automated-security-helper\npip install .\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code># Run a scan in local mode (Python only)\nash --mode local\n\n# Run a scan in container mode (all tools)\nash --mode container\n\n# Run a scan in precommit mode (fast subset of tools)\nash --mode precommit\n</code></pre>"},{"location":"#sample-output","title":"Sample Output","text":"<pre><code>                                                 ASH Scan Results Summary\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Scanner        \u2503 Suppressed \u2503 Critical \u2503 High \u2503 Medium \u2503 Low \u2503 Info \u2503 Duration \u2503 Actionable \u2503 Result \u2503 Threshold       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 bandit         \u2502 7          \u2502 0        \u2502 1    \u2502 0      \u2502 56  \u2502 0    \u2502 19.9s    \u2502 1          \u2502 FAILED \u2502 MEDIUM (global) \u2502\n\u2502 cdk-nag        \u2502 0          \u2502 0        \u2502 30   \u2502 0      \u2502 0   \u2502 5    \u2502 48.7s    \u2502 30         \u2502 FAILED \u2502 MEDIUM (global) \u2502\n\u2502 cfn-nag        \u2502 0          \u2502 0        \u2502 0    \u2502 15     \u2502 0   \u2502 0    \u2502 45.1s    \u2502 15         \u2502 FAILED \u2502 MEDIUM (global) \u2502\n\u2502 checkov        \u2502 10         \u2502 0        \u2502 25   \u2502 0      \u2502 0   \u2502 0    \u2502 38.9s    \u2502 25         \u2502 FAILED \u2502 MEDIUM (global) \u2502\n\u2502 detect-secrets \u2502 0          \u2502 0        \u2502 48   \u2502 0      \u2502 0   \u2502 0    \u2502 18.9s    \u2502 48         \u2502 FAILED \u2502 MEDIUM (global) \u2502\n\u2502 grype          \u2502 0          \u2502 0        \u2502 2    \u2502 1      \u2502 0   \u2502 0    \u2502 40.3s    \u2502 3          \u2502 FAILED \u2502 MEDIUM (global) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                     source-dir: '.'\n                                              output-dir: '.ash/ash_output'\n\n=== ASH Scan Completed in 1m 6s: Next Steps ===\nView detailed findings...\n  - SARIF: '.ash/ash_output/reports/ash.sarif'\n  - JUnit: '.ash/ash_output/reports/ash.junit.xml'\n  - ASH aggregated results JSON available at: '.ash/ash_output/ash_aggregated_results.json'\n\n=== Actionable findings detected! ===\nTo investigate...\n  1. Open one of the summary reports for a user-friendly table of the findings:\n    - HTML report of all findings: '.ash/ash_output/reports/ash.html'\n    - Markdown summary: '.ash/ash_output/reports/ash.summary.md'\n    - Text summary: '.ash/ash_output/reports/ash.summary.txt'\n  2. Use ash report to view a short text summary of the scan in your terminal\n  3. Use ash inspect findings to explore the findings interactively\n  4. Review scanner-specific reports and outputs in the '.ash/ash_output/scanners' directory\n\n=== ASH Exit Codes ===\n  0: Success - No actionable findings or not configured to fail on findings\n  1: Error during execution\n  2: Actionable findings detected when configured with `fail_on_findings: true`. Default is True. Current value: True\nERROR (2) Exiting due to 122 actionable findings found in ASH scan\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<p>ASH v3 uses a YAML configuration file (<code>.ash/ash.yaml</code>) with support for JSON Schema validation:</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/awslabs/automated-security-helper/refs/heads/main/automated_security_helper/schemas/AshConfig.json\nproject_name: my-project\nglobal_settings:\n  severity_threshold: MEDIUM\n  ignore_paths:\n    - path: 'tests/test_data'\n      reason: 'Test data only'\nscanners:\n  bandit:\n    enabled: true\n    options:\n      confidence_level: high\nreporters:\n  markdown:\n    enabled: true\n    options:\n      include_detailed_findings: true\n</code></pre>"},{"location":"#using-ash-with-pre-commit","title":"Using ASH with pre-commit","text":"<p>Add this to your <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n  - repo: https://github.com/awslabs/automated-security-helper\n    rev: v3.0.0\n    hooks:\n      - id: ash-simple-scan\n</code></pre> <p>Run with:</p> <pre><code>pre-commit run ash-simple-scan --all-files\n</code></pre>"},{"location":"#output-files","title":"Output Files","text":"<p>ASH v3 produces several output files in the <code>.ash/ash_output/</code> directory:</p> <ul> <li><code>ash_aggregated_results.json</code>: Complete machine-readable results</li> <li><code>reports/ash.summary.txt</code>: Human-readable text summary</li> <li><code>reports/ash.summary.md</code>: Markdown summary for GitHub PRs and other platforms</li> <li><code>reports/ash.html</code>: Interactive HTML report</li> <li><code>reports/ash.csv</code>: CSV report for filtering and sorting findings</li> </ul>"},{"location":"#faq","title":"FAQ","text":"How do I run ASH on Windows?  ASH v3 can run directly on Windows in local mode with Python 3.10+. Simply install ASH using pip, pipx, or uvx and run with `--mode local`. For container mode, you'll need WSL2 and a container runtime like Docker Desktop, Rancher Desktop, or Podman Desktop.  How do I run ASH in CI/CD pipelines?  ASH can be run in container mode in any CI/CD environment that supports containers. See the [tutorials](docs/content/tutorials/running-ash-in-ci.md) for examples.  How do I exclude files from scanning?  ASH respects `.gitignore` files. You can also configure ignore paths in your `.ash/ash.yaml` configuration file.  How do I run ASH in an offline/air-gapped environment?  Build an offline image with `ash --mode container --offline --offline-semgrep-rulesets p/ci --no-run`, push to your private registry, then use `ash --mode container --offline --no-build` in your air-gapped environment.  I am trying to scan a CDK application, but ASH does not show CDK Nag scan results -- why is that?  ASH uses CDK Nag underneath to apply NagPack rules to *CloudFormation templates* via the `CfnInclude` CDK construct. This is purely a mechanism to ingest a bare CloudFormation template and apply CDK NagPacks to it; doing this against a template emitted by another CDK application causes a collision in the `CfnInclude` construct due to the presence of the `BootstrapVersion` parameter on the template added by CDK. For CDK applications, we recommend integrating CDK Nag directly in your CDK code. ASH will still apply other CloudFormation scanners (cfn-nag, checkov) against templates synthesized via CDK, but the CDK Nag scanner will not scan those templates."},{"location":"#feedback-and-contributing","title":"Feedback and Contributing","text":"<ul> <li>Create an issue in the GitHub repository</li> <li>See CONTRIBUTING for contribution guidelines</li> </ul>"},{"location":"#security","title":"Security","text":"<p>See CONTRIBUTING for security issue reporting information.</p>"},{"location":"#license","title":"License","text":"<p>This library is licensed under the Apache 2.0 License. See the LICENSE file.</p>"},{"location":"contributing/","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>"},{"location":"contributing/#reporting-bugsfeature-requests","title":"Reporting Bugs/Feature Requests","text":"<p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul>"},{"location":"contributing/#contributing-via-pull-requests","title":"Contributing via Pull Requests","text":"<p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the <code>main</code> branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Create a branch in your fork where the branch name is something meaningful.  We encourage    the use of <code>feature/&lt;short-description&gt;</code>, <code>bugfix/&lt;short-description&gt;</code>, <code>hotfix/&lt;short-description&gt;</code>, and so on for branch naming.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code,    it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated continuous integration (CI) failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional documentation on forking a repository and creating a pull request.</p>"},{"location":"contributing/#finding-contributions-to-work-on","title":"Finding contributions to work on","text":"<p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"contributing/#security-issue-notifications","title":"Security issue notifications","text":"<p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p>"},{"location":"contributing/#licensing","title":"Licensing","text":"<p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p> <p>We may ask you to sign a Contributor License Agreement (CLA) for larger changes.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-ash","title":"What is ASH?","text":"<p>ASH (Automated Security Helper) is a security scanning tool designed to help you identify potential security issues in your code, infrastructure, and IAM configurations as early as possible in your development process.</p>"},{"location":"faq/#whats-new-in-ash-v3","title":"What's new in ASH v3?","text":"<p>ASH v3 has been completely rewritten in Python with significant improvements: - Python-based CLI with multiple execution modes - Enhanced configuration system - Improved reporting formats - Customizable plugin system - Better Windows support - Programmatic API for integration</p>"},{"location":"faq/#is-ash-a-replacement-for-human-security-reviews","title":"Is ASH a replacement for human security reviews?","text":"<p>No. ASH is designed to help identify common security issues early in the development process, but it's not a replacement for human security reviews or team/customer security standards.</p>"},{"location":"faq/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"faq/#how-do-i-install-ash-v3","title":"How do I install ASH v3?","text":"<p>You have several options: <pre><code># Using uvx (recommended)\nalias ash=\"uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\"\n\n# Using pipx\npipx install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n\n# Using pip\npip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n</code></pre></p>"},{"location":"faq/#what-are-the-prerequisites-for-ash-v3","title":"What are the prerequisites for ASH v3?","text":"<ul> <li>For local mode: Python 3.10 or later, UV package manager</li> <li>For container mode: Any OCI-compatible container runtime (Docker, Podman, Finch, etc.)</li> <li>On Windows with container mode: WSL2 is typically required</li> </ul>"},{"location":"faq/#how-do-i-run-ash-on-windows","title":"How do I run ASH on Windows?","text":"<p>ASH v3 can run directly on Windows in local mode with Python 3.10+. Simply install ASH using pip, pipx, or uvx and run with <code>--mode local</code>. For container mode, you'll need WSL2 and a container runtime like Docker Desktop, Rancher Desktop, or Podman Desktop.</p>"},{"location":"faq/#usage","title":"Usage","text":""},{"location":"faq/#what-are-the-different-execution-modes-in-ash-v3","title":"What are the different execution modes in ASH v3?","text":"<p>ASH v3 supports three execution modes: - Local Mode: Runs entirely in the local Python process - Container Mode: Runs non-Python scanners in a container - Precommit Mode: Runs a subset of fast scanners optimized for pre-commit hooks</p>"},{"location":"faq/#how-do-i-run-a-basic-scan","title":"How do I run a basic scan?","text":"<pre><code># Run in local mode (Python-based scanners only)\nash --mode local\n\n# Run in container mode (all scanners)\nash --mode container\n\n# Run in precommit mode (fast subset of scanners)\nash --mode precommit\n</code></pre>"},{"location":"faq/#how-do-i-specify-which-files-to-scan","title":"How do I specify which files to scan?","text":"<pre><code># Scan a specific directory\nash --source-dir /path/to/code\n\n# Configure ignore paths in .ash/.ash.yaml\nglobal_settings:\n  ignore_paths:\n    - path: 'tests/test_data'\n      reason: 'Test data only'\n</code></pre>"},{"location":"faq/#how-do-i-exclude-files-from-scanning","title":"How do I exclude files from scanning?","text":"<p>ASH respects <code>.gitignore</code> files. You can also configure ignore paths in your <code>.ash/.ash.yaml</code> configuration file.</p>"},{"location":"faq/#how-do-i-run-specific-scanners","title":"How do I run specific scanners?","text":"<pre><code># Run only specific scanners\nash --scanners bandit,semgrep\n\n# Exclude specific scanners\nash --exclude-scanners cfn-nag,cdk-nag\n</code></pre>"},{"location":"faq/#how-do-i-generate-specific-report-formats","title":"How do I generate specific report formats?","text":"<pre><code># Generate specific report formats\nash --output-formats markdown,html,json\n\n# Generate a report from existing results\nash report --format html --output-dir ./my-scan-results\n</code></pre>"},{"location":"faq/#configuration","title":"Configuration","text":""},{"location":"faq/#where-is-the-ash-configuration-file-located","title":"Where is the ASH configuration file located?","text":"<p>By default, ASH looks for a configuration file in the following locations (in order): 1. <code>.ash/.ash.yaml</code> 2. <code>.ash/.ash.yml</code> 3. <code>.ash.yaml</code> 4. <code>.ash.yml</code></p>"},{"location":"faq/#how-do-i-create-a-configuration-file","title":"How do I create a configuration file?","text":"<pre><code># Initialize a new configuration file\nash config init\n</code></pre>"},{"location":"faq/#how-do-i-override-configuration-values-at-runtime","title":"How do I override configuration values at runtime?","text":"<pre><code># Enable a specific scanner\nash --config-overrides 'scanners.bandit.enabled=true'\n\n# Change severity threshold\nash --config-overrides 'global_settings.severity_threshold=LOW'\n</code></pre>"},{"location":"faq/#scanners-and-tools","title":"Scanners and Tools","text":""},{"location":"faq/#what-security-scanners-are-included-in-ash-v3","title":"What security scanners are included in ASH v3?","text":"<p>ASH v3 integrates multiple open-source security tools. Tools like Bandit, Checkov, and Semgrep are managed via UV's tool isolation system, which automatically installs and runs them in isolated environments: - Bandit (Python SAST) - Managed via UV tool isolation (auto-installed: <code>bandit&gt;=1.7.0</code>) - Semgrep (Multi-language SAST) - Managed via UV tool isolation (auto-installed: <code>semgrep&gt;=1.125.0</code>) - detect-secrets (Secret detection) - Included with ASH - Checkov (IaC scanning) - Managed via UV tool isolation (auto-installed: <code>checkov&gt;=3.2.0,&lt;4.0.0</code>) - cfn_nag (CloudFormation scanning) - Requires separate installation - cdk-nag (CloudFormation scanning) - Included with ASH - npm-audit (JavaScript/Node.js SCA) - Requires Node.js/npm - Grype (Multi-language SCA) - Requires separate installation - Syft (SBOM generation) - Requires separate installation</p>"},{"location":"faq/#i-am-trying-to-scan-a-cdk-application-but-ash-does-not-show-cdk-nag-scan-results-why-is-that","title":"I am trying to scan a CDK application, but ASH does not show CDK Nag scan results -- why is that?","text":"<p>ASH uses CDK Nag underneath to apply NagPack rules to CloudFormation templates via the <code>CfnInclude</code> CDK construct. This is purely a mechanism to ingest a bare CloudFormation template and apply CDK NagPacks to it; doing this against a template emitted by another CDK application causes a collision in the <code>CfnInclude</code> construct due to the presence of the <code>BootstrapVersion</code> parameter on the template added by CDK. For CDK applications, we recommend integrating CDK Nag directly in your CDK code. ASH will still apply other CloudFormation scanners (cfn-nag, checkov) against templates synthesized via CDK, but the CDK Nag scanner will not scan those templates.</p>"},{"location":"faq/#how-do-i-add-custom-scanners","title":"How do I add custom scanners?","text":"<p>You can create custom scanners by implementing the scanner plugin interface and adding your plugin module to the ASH configuration:</p> <pre><code># .ash/.ash.yaml\nash_plugin_modules:\n  - my_ash_plugins\n</code></pre>"},{"location":"faq/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"faq/#how-do-i-run-ash-in-cicd-pipelines","title":"How do I run ASH in CI/CD pipelines?","text":"<p>ASH can be run in container mode in any CI/CD environment that supports containers. See the tutorials for examples.</p>"},{"location":"faq/#how-do-i-use-ash-with-pre-commit","title":"How do I use ASH with pre-commit?","text":"<p>Add this to your <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n  - repo: https://github.com/awslabs/automated-security-helper\n    rev: v3.0.0\n    hooks:\n      - id: ash-simple-scan\n</code></pre>"},{"location":"faq/#how-do-i-fail-ci-builds-on-security-findings","title":"How do I fail CI builds on security findings?","text":"<pre><code># Exit with non-zero code if findings are found\nash --mode local --fail-on-findings\n</code></pre>"},{"location":"faq/#advanced-usage","title":"Advanced Usage","text":""},{"location":"faq/#how-do-i-run-ash-in-an-offlineair-gapped-environment","title":"How do I run ASH in an offline/air-gapped environment?","text":"<p>Build an offline image with <code>ash --mode container --offline --offline-semgrep-rulesets p/ci --no-run</code>, push to your private registry, then use <code>ash --mode container --offline --no-build</code> in your air-gapped environment.</p>"},{"location":"faq/#can-i-use-ash-programmatically","title":"Can I use ASH programmatically?","text":"<p>Yes, ASH v3 can be used programmatically in Python:</p> <pre><code>from automated_security_helper.interactions.run_ash_scan import run_ash_scan\nfrom automated_security_helper.core.enums import RunMode\n\nresults = run_ash_scan(\n    source_dir=\"/path/to/code\",\n    output_dir=\"/path/to/output\",\n    mode=RunMode.local\n)\n</code></pre>"},{"location":"faq/#how-do-i-customize-the-container-image","title":"How do I customize the container image?","text":"<pre><code># Specify a custom container image\nexport ASH_IMAGE_NAME=\"my-registry/ash:custom\"\nash --mode container\n\n# Build a custom image\nash build-image --build-target ci --custom-containerfile ./my-dockerfile\n</code></pre>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#ash-is-not-finding-any-files-to-scan","title":"ASH is not finding any files to scan","text":"<p>Ensure you're running ASH inside the folder you intend to scan or using the <code>--source-dir</code> parameter. If the folder where the files reside is part of a git repository, ensure the files are added (committed) before running ASH.</p>"},{"location":"faq/#im-getting-command-not-found-errors-for-scanners-in-local-mode","title":"I'm getting \"command not found\" errors for scanners in local mode","text":"<p>Some scanners require external dependencies. Either install the required dependencies locally or use container mode (<code>--mode container</code>).</p>"},{"location":"faq/#ash-is-running-slowly","title":"ASH is running slowly","text":"<p>Try these options: - Use <code>--mode precommit</code> for faster scans - Use <code>--scanners</code> to run only specific scanners - Use <code>--strategy parallel</code> (default) to run scanners in parallel</p>"},{"location":"faq/#some-scanners-seem-to-be-missing-from-my-scan-results","title":"Some scanners seem to be missing from my scan results","text":"<p>ASH v3 includes a comprehensive Scanner Validation System that monitors scanner registration, enablement, and execution throughout the scan process. If scanners are missing:</p> <ol> <li>Check the scan logs for validation warnings about missing or disabled scanners</li> <li>Verify scanner dependencies are installed (for local mode)</li> <li>Check your configuration for excluded scanners</li> <li>Use <code>--debug</code> to see detailed validation information</li> <li>Run the integration verification test: <code>python verify_integration.py</code> (if available in your installation)</li> </ol> <p>The validation system will log specific reasons why scanners might be disabled (missing dependencies, configuration exclusions, etc.) and attempt automatic recovery where possible. Additionally, the system ensures all originally registered scanners appear in the final results with appropriate status, even if they failed or were disabled during the scan.</p> <p>For detailed information about the scanner validation system, see the Scanner Validation System developer guide.</p>"},{"location":"faq/#how-do-i-debug-ash","title":"How do I debug ASH?","text":"<pre><code># Enable debug logging\nash --debug\n\n# Enable verbose logging\nash --verbose\n</code></pre>"},{"location":"faq/#ai-integration-and-mcp","title":"AI Integration and MCP","text":""},{"location":"faq/#what-is-mcp-and-how-does-ash-support-it","title":"What is MCP and how does ASH support it?","text":"<p>Model Context Protocol (MCP) is a standardized way for AI applications to access external tools and data sources. ASH includes an MCP server that allows AI assistants to perform security scans, monitor progress, and analyze results through natural language interactions.</p>"},{"location":"faq/#how-do-i-set-up-ash-with-mcp","title":"How do I set up ASH with MCP?","text":"<ol> <li>Install UV and Python 3.10+:</li> <li>Install <code>uv</code> from Astral or the GitHub README</li> <li> <p>Install Python 3.10+ using <code>uv python install 3.10</code> (or a more recent version)</p> </li> <li> <p>Configure your AI client to use the ASH MCP server:</p> </li> </ol> <p>Amazon Q Developer CLI - Add to <code>~/.aws/amazonq/mcp.json</code>:    <pre><code>{\n  \"mcpServers\": {\n    \"ash\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from=git+https://github.com/awslabs/automated-security-helper@v3.0.0\",\n        \"ash\",\n        \"mcp\"\n      ],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n</code></pre></p> <p>Claude Desktop - Add to <code>claude_desktop_config.json</code>:    <pre><code>{\n  \"mcpServers\": {\n    \"ash-security\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from=git+https://github.com/awslabs/automated-security-helper@v3.0.0\",\n        \"ash\",\n        \"mcp\"\n      ]\n    }\n  }\n}\n</code></pre></p> <p>Cline (VS Code):    <pre><code>{\n  \"mcpServers\": {\n    \"ash\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from=git+https://github.com/awslabs/automated-security-helper@v3.0.0\",\n        \"ash\",\n        \"mcp\"\n      ],\n      \"disabled\": false,\n      \"autoApprove\": [\n        \"get_scan_progress\",\n        \"list_active_scans\",\n        \"get_scan_results\"\n      ]\n    }\n  }\n}\n</code></pre></p>"},{"location":"faq/#what-can-i-do-with-ash-through-mcp","title":"What can I do with ASH through MCP?","text":"<ul> <li>Start security scans with natural language commands</li> <li>Monitor scan progress in real-time</li> <li>Analyze and prioritize security findings</li> <li>Generate security reports and remediation plans</li> <li>Manage multiple concurrent scans</li> <li>Configure resource limits and performance settings</li> <li>Monitor system resource usage and health</li> </ul>"},{"location":"faq/#how-do-i-configure-resource-management-for-the-mcp-server","title":"How do I configure resource management for the MCP server?","text":"<p>ASH v3 includes comprehensive resource management that can be configured through your ASH configuration file:</p> <pre><code># .ash/ash.yaml\nmcp-resource-management:\n  max_concurrent_scans: 5          # Limit simultaneous scans\n  max_concurrent_tasks: 25         # Limit async tasks\n  thread_pool_max_workers: 6       # Thread pool size\n  scan_timeout_seconds: 2400       # 40 minute scan timeout\n  memory_warning_threshold_mb: 2048 # Memory usage warnings\n  enable_health_checks: true       # Enable monitoring\n</code></pre> <p>This prevents memory leaks, manages system resources, and ensures stable operation under load.</p>"},{"location":"faq/#im-getting-maximum-concurrent-scans-exceeded-errors","title":"I'm getting \"Maximum concurrent scans exceeded\" errors","text":"<p>This is a resource protection feature. You can adjust the limits in your configuration:</p> <pre><code># .ash/ash.yaml\nmcp-resource-management:\n  max_concurrent_scans: 10  # Increase from default of 3\n  max_concurrent_tasks: 50  # Increase task limit if needed\n</code></pre> <p>Or wait for existing scans to complete before starting new ones.</p>"},{"location":"faq/#im-getting-mcp-dependency-errors","title":"I'm getting MCP dependency errors","text":"<p>MCP dependencies are included by default in ASH v3. If you're still getting errors:</p> <ol> <li>Check UV installation: Ensure UV is installed and available: <code>uv --version</code></li> <li>Check Python version: Ensure Python 3.10+ is available: <code>uv python list</code></li> <li>Test the MCP server: Try running the server directly:    <pre><code>uvx --from=git+https://github.com/awslabs/automated-security-helper@v3.0.0 ash mcp --help\n</code></pre></li> </ol>"},{"location":"faq/#how-do-i-test-the-ash-mcp-server","title":"How do I test the ASH MCP server?","text":"<pre><code># Test MCP server startup\nuvx --from=git+https://github.com/awslabs/automated-security-helper@v3.0.0 ash mcp --debug\n\n# Check ASH version\nuvx --from=git+https://github.com/awslabs/automated-security-helper@v3.0.0 ash --version\n</code></pre>"},{"location":"faq/#how-do-i-monitor-mcp-server-performance","title":"How do I monitor MCP server performance?","text":"<p>Enable resource monitoring in your configuration:</p> <pre><code># .ash/ash.yaml\nmcp-resource-management:\n  enable_health_checks: true\n  enable_resource_logging: true\n  log_resource_operations: true\n</code></pre> <p>Then ask your AI assistant: <pre><code>\"Show me the current resource usage of the MCP server\"\n\"How many scans are currently running?\"\n\"What's the memory usage of the security scanner?\"\n</code></pre></p> <p>For more details, see the MCP Tutorial.</p>"},{"location":"faq/#getting-help","title":"Getting Help","text":""},{"location":"faq/#where-can-i-find-more-documentation","title":"Where can I find more documentation?","text":"<p>Visit the ASH Documentation.</p>"},{"location":"faq/#how-do-i-report-issues-or-request-features","title":"How do I report issues or request features?","text":"<p>Create an issue on GitHub.</p>"},{"location":"faq/#how-do-i-contribute-to-ash","title":"How do I contribute to ASH?","text":"<p>See the CONTRIBUTING guide for contribution guidelines.</p>"},{"location":"developer-guide/scanner-validation-system/","title":"Scanner Validation System","text":"<p>The Scanner Validation System provides comprehensive monitoring and validation throughout the ASH scan lifecycle to ensure all expected scanners are properly registered, enabled, queued, executed, and included in results. This system addresses intermittent issues where security scanners might be silently dropped during the scan process.</p>"},{"location":"developer-guide/scanner-validation-system/#overview","title":"Overview","text":"<p>The Scanner Validation System was introduced to address reliability issues where scanners (particularly Checkov and Detect-Secrets on Windows) were not being included in ASH scans without clear indication of why. The system implements validation checkpoints at key phases of the scan lifecycle to provide comprehensive visibility and ensure scan completeness.</p>"},{"location":"developer-guide/scanner-validation-system/#architecture","title":"Architecture","text":"<p>The Scanner Validation System follows a checkpoint-based architecture that monitors scanner state throughout the scan process:</p> <ol> <li>Registration Validation: Validates which scanner plugins are discovered and registered</li> <li>Enablement Validation: Validates which scanners are enabled and captures reasons for disabled scanners</li> <li>Task Queue Validation: Validates that all enabled scanners have tasks in the execution queue</li> <li>Execution Completion Validation: Validates that all expected scanners completed execution</li> <li>Result Completeness Validation: Ensures all originally registered scanners appear in final results</li> </ol>"},{"location":"developer-guide/scanner-validation-system/#key-components","title":"Key Components","text":""},{"location":"developer-guide/scanner-validation-system/#scannervalidationmanager","title":"ScannerValidationManager","text":"<p>The <code>ScannerValidationManager</code> class orchestrates all validation activities and maintains state throughout the scan process.</p>"},{"location":"developer-guide/scanner-validation-system/#core-responsibilities","title":"Core Responsibilities","text":"<ul> <li>Track registered scanners and their states throughout the scan lifecycle</li> <li>Validate scanner enablement status and capture specific reasons for disabled scanners</li> <li>Monitor task queue population to ensure no scanners are silently dropped</li> <li>Verify scan execution completion for all expected scanners</li> <li>Ensure complete result inclusion with appropriate status for failed/missing scanners</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#key-methods","title":"Key Methods","text":""},{"location":"developer-guide/scanner-validation-system/#validate_registered_scannersscanner_classes-listtype-validationcheckpoint","title":"<code>validate_registered_scanners(scanner_classes: List[type]) -&gt; ValidationCheckpoint</code>","text":"<p>Validates and logs all registered scanner plugins at the beginning of the scan process.</p> <ul> <li>Extracts scanner names from plugin classes</li> <li>Updates scanner states with registration information</li> <li>Creates a validation checkpoint with registration results</li> <li>Logs summary of registered scanners in alphabetical order</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#validate_scanner_enablementenabled_scanners-excluded_scanners-dependency_errors-validationcheckpoint","title":"<code>validate_scanner_enablement(enabled_scanners, excluded_scanners, dependency_errors) -&gt; ValidationCheckpoint</code>","text":"<p>Validates which scanners are enabled and captures specific reasons for disabled scanners.</p> <ul> <li>Updates scanner states based on enablement status</li> <li>Captures specific reasons for disabled scanners (configuration, dependencies, exclusions)</li> <li>Creates validation checkpoint with enablement results</li> <li>Logs comprehensive enablement summary with reasons</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#validate_task_queuequeue_contents-listtuple-validationcheckpoint","title":"<code>validate_task_queue(queue_contents: List[tuple]) -&gt; ValidationCheckpoint</code>","text":"<p>Validates that all expected scanners have tasks in the execution queue.</p> <ul> <li>Extracts scanner names from queue contents</li> <li>Compares expected scanners (enabled) against actual queued scanners</li> <li>Updates scanner states to indicate queuing status</li> <li>Identifies missing scanners and unexpected scanners in the queue</li> <li>Creates validation checkpoint with queue validation results</li> <li>Logs detailed queue validation summary and any discrepancies</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#retry_scanner_registrationmissing_scanners-liststr-liststr","title":"<code>retry_scanner_registration(missing_scanners: List[str]) -&gt; List[str]</code>","text":"<p>Attempts to retry registration for scanners missing from the task queue.</p> <ul> <li>Re-validates scanner dependencies and configuration</li> <li>Attempts to re-enable scanners that can be recovered</li> <li>Updates scanner states with retry outcomes</li> <li>Returns list of successfully re-enabled scanners</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#ensure_complete_resultsaggregated_results-ashaggregatedresults-validationcheckpoint","title":"<code>ensure_complete_results(aggregated_results: AshAggregatedResults) -&gt; ValidationCheckpoint</code>","text":"<p>Ensures all originally registered scanners appear in the final aggregated results.</p> <ul> <li>Validates that all registered scanners are included in results</li> <li>Adds missing scanners with appropriate status (SKIPPED, FAILED, or MISSING)</li> <li>Includes specific failure reasons for missing scanners</li> <li>Handles scanners with missing dependencies, exclusions, or execution failures</li> <li>Updates scanner states to reflect inclusion in results</li> <li>Creates validation checkpoint with completeness validation results</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#generate_validation_report-str","title":"<code>generate_validation_report() -&gt; str</code>","text":"<p>Generates a comprehensive validation report with detailed information about all validation activities.</p> <ul> <li>Creates a detailed report of all validation checkpoints and scanner states</li> <li>Includes executive summary with key metrics and validation status</li> <li>Groups scanners by status (enabled, disabled, missing dependencies, etc.)</li> <li>Shows detailed checkpoint information with timestamps and discrepancies</li> <li>Provides scanner-specific information including failure reasons and metadata</li> <li>Includes dependency analysis with common missing dependencies</li> <li>Generates actionable recommendations based on validation findings</li> <li>Returns formatted string report suitable for logging or file output</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#data-models","title":"Data Models","text":""},{"location":"developer-guide/scanner-validation-system/#scannervalidationstate","title":"ScannerValidationState","text":"<p>Tracks the complete lifecycle state of a scanner during validation:</p> <pre><code>@dataclass\nclass ScannerValidationState:\n    name: str\n    plugin_class: Optional[type] = None\n    registration_status: str = \"unknown\"  # \"registered\", \"failed\", \"missing\"\n    enablement_status: str = \"unknown\"    # \"enabled\", \"disabled\", \"excluded\", \"missing_deps\"\n    enablement_reason: str = \"\"\n    dependency_errors: List[str] = field(default_factory=list)\n    queued_for_execution: bool = False\n    execution_completed: bool = False\n    included_in_results: bool = False\n    failure_reason: Optional[str] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"developer-guide/scanner-validation-system/#validationcheckpoint","title":"ValidationCheckpoint","text":"<p>Captures a validation snapshot at a specific point in the scan process:</p> <pre><code>@dataclass\nclass ValidationCheckpoint:\n    checkpoint_name: str\n    timestamp: datetime = field(default_factory=datetime.now)\n    expected_scanners: List[str] = field(default_factory=list)\n    actual_scanners: List[str] = field(default_factory=list)\n    discrepancies: List[str] = field(default_factory=list)\n    errors: List[str] = field(default_factory=list)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"developer-guide/scanner-validation-system/#validation-checkpoints","title":"Validation Checkpoints","text":""},{"location":"developer-guide/scanner-validation-system/#1-registration-validation","title":"1. Registration Validation","text":"<p>When: After plugin discovery, before scanner filtering Purpose: Validate all scanner plugins are properly discovered and registered Validates: - Total count of registered scanners - Scanner names and plugin classes - Registration success/failure status</p>"},{"location":"developer-guide/scanner-validation-system/#2-enablement-validation","title":"2. Enablement Validation","text":"<p>When: After scanner filtering, before task queue creation Purpose: Validate which scanners are enabled and capture reasons for disabled scanners Validates: - Enabled vs disabled scanner counts - Specific reasons for disabled scanners (config, dependencies, exclusions) - Dependency validation results with detailed error information</p>"},{"location":"developer-guide/scanner-validation-system/#3-task-queue-validation","title":"3. Task Queue Validation","text":"<p>When: After task queue population, before scanner execution Purpose: Validate that all enabled scanners have corresponding tasks in the queue Validates: - Expected scanners (enabled) vs actual scanners in queue - Missing scanners that should be queued but aren't - Unexpected scanners that are queued but not marked as enabled - Queue item counts and structure</p>"},{"location":"developer-guide/scanner-validation-system/#4-execution-completion-validation","title":"4. Execution Completion Validation","text":"<p>When: After scanner execution, before result aggregation Purpose: Validate that all expected scanners completed execution Validates: - Expected scanners vs completed scanners - Missing scanners that didn't complete execution - Execution status and completion tracking</p>"},{"location":"developer-guide/scanner-validation-system/#5-result-completeness-validation","title":"5. Result Completeness Validation","text":"<p>When: Before finalizing scan results Purpose: Ensure all originally registered scanners appear in final results Validates: - All registered scanners have entries in final results - Failed/missing scanners are included with appropriate status - Failure reasons are captured and included - Unexpected scanners in results are identified and tracked - Final completeness rate is calculated and logged</p>"},{"location":"developer-guide/scanner-validation-system/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"developer-guide/scanner-validation-system/#registration-retry-logic","title":"Registration Retry Logic","text":"<p>When scanners are missing from the task queue, the system attempts recovery:</p> <ol> <li>Dependency Re-validation: Check if missing dependencies have been resolved</li> <li>Configuration Re-check: Verify scanner configuration hasn't changed</li> <li>Re-enablement: Attempt to re-enable scanners that can be recovered</li> <li>State Updates: Update scanner states with retry outcomes</li> <li>Logging: Log all retry attempts and results for debugging</li> </ol>"},{"location":"developer-guide/scanner-validation-system/#graceful-degradation","title":"Graceful Degradation","text":"<p>When scanners fail or are missing:</p> <ol> <li>Result Inclusion: Include scanner entry in results with appropriate status</li> <li>Status Assignment: Assign appropriate status (SKIPPED for disabled/excluded, FAILED for execution failures, MISSING for unknown issues)</li> <li>Failure Reasons: Include detailed failure reason in scanner entry</li> <li>Dependency Tracking: Mark dependency satisfaction status for scanners with missing dependencies</li> <li>Exclusion Tracking: Mark exclusion status for scanners excluded by configuration</li> <li>Scan Continuity: Maintain scan process continuity despite individual scanner failures</li> </ol>"},{"location":"developer-guide/scanner-validation-system/#logging-and-debugging","title":"Logging and Debugging","text":"<p>The Scanner Validation System provides comprehensive logging at multiple levels:</p> <ul> <li>DEBUG: Detailed state transitions and checkpoint creation</li> <li>INFO: Validation summaries, scanner counts, and status information</li> <li>WARNING: Missing scanners, retry attempts, and validation discrepancies</li> <li>ERROR: Critical validation failures and unrecoverable errors</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#example-log-output","title":"Example Log Output","text":"<pre><code>INFO: Starting validation of registered scanner plugins\nINFO: Found 6 registered scanner plugins:\nINFO:   - bandit\nINFO:   - checkov\nINFO:   - detect-secrets\nINFO:   - grype\nINFO:   - semgrep\nINFO:   - syft\nINFO: Starting validation of scanner enablement status\nINFO: Scanner enablement summary:\nINFO:   Enabled scanners (4): bandit, detect-secrets, grype, semgrep\nINFO:   Excluded scanners (1): syft\nINFO:   Scanners with dependency issues (1):\nINFO:     - checkov: Missing required tool: terraform\nINFO: Starting validation of scanner task queue\nINFO: Task queue validation summary:\nINFO:   Expected scanners in queue (4): bandit, detect-secrets, grype, semgrep\nINFO:   Actual scanners in queue (3): bandit, detect-secrets, semgrep\nWARNING: Scanner 'grype' is enabled but missing from task queue\nINFO: \u2705 Task queue validation passed - all expected scanners are queued\nINFO: Starting validation of result completeness\nINFO: Result completeness validation summary:\nINFO:   Originally registered scanners (6): bandit, checkov, detect-secrets, grype, semgrep, syft\nINFO:   Scanners in current results (4): bandit, detect-secrets, grype, semgrep\nINFO: Found 2 scanners missing from results, adding them:\nINFO:   + Added 'checkov' with status SKIPPED: Missing dependencies: Missing required tool: terraform\nINFO:   + Added 'syft' with status SKIPPED: Scanner was excluded by configuration\nINFO: \ud83d\udd27 Result completeness validation completed with 2 adjustments\nINFO:    Added missing scanners: checkov, syft\nINFO: \ud83d\udcca Final result completeness rate: 100.0% (6/6)\n</code></pre>"},{"location":"developer-guide/scanner-validation-system/#integration-with-scanphase","title":"Integration with ScanPhase","text":"<p>The Scanner Validation System integrates with the existing <code>ScanPhase</code> class at key checkpoints. The <code>ScannerValidationManager</code> is initialized during <code>ScanPhase</code> construction and used throughout the scan lifecycle:</p>"},{"location":"developer-guide/scanner-validation-system/#initialization","title":"Initialization","text":"<pre><code>class ScanPhase(EnginePhase):\n    def __init__(self, plugin_context, plugins=None, progress_display=None, asharp_model=None):\n        \"\"\"Initialize the ScanPhase with validation manager.\n\n        Args:\n            plugin_context: Plugin context with paths and configuration\n            plugins: List of plugins to use\n            progress_display: Progress display to use for reporting progress\n            asharp_model: AshAggregatedResults to update with results\n        \"\"\"\n        super().__init__(plugin_context, plugins or [], progress_display, asharp_model)\n        self.validation_manager = ScannerValidationManager(plugin_context)\n</code></pre>"},{"location":"developer-guide/scanner-validation-system/#validation-integration-points","title":"Validation Integration Points","text":"<p>The validation manager is integrated at key points during scan execution:</p> <pre><code>def _execute_phase(self, aggregated_results, **kwargs):\n    # 1. Registration validation (planned)\n    scanner_classes = self.plugins\n    # self.validation_manager.validate_registered_scanners(scanner_classes)\n\n    # 2. Enablement validation (planned)\n    enabled, excluded, dep_errors = self._filter_scanners(scanner_classes)\n    # self.validation_manager.validate_scanner_enablement(enabled, excluded, dep_errors)\n\n    # 3. Task queue validation (planned)\n    queue_contents = self._populate_task_queue(enabled)\n    # checkpoint = self.validation_manager.validate_task_queue(queue_contents)\n\n    # 4. Retry missing scanners if needed (planned)\n    # if checkpoint.get_missing_scanners():\n    #     self.validation_manager.retry_scanner_registration(checkpoint.get_missing_scanners())\n\n    # 5. Execute scanners\n    completed = self._execute_scanners(queue_contents)\n\n    # 6. Execution completion validation (planned)\n    # self.validation_manager.validate_execution_completion(completed)\n\n    # 7. Result completeness validation (planned)\n    # self.validation_manager.ensure_complete_results(aggregated_results)\n</code></pre>"},{"location":"developer-guide/scanner-validation-system/#current-implementation-status","title":"Current Implementation Status","text":"<p>As of the current implementation:</p> <ul> <li>\u2705 Infrastructure: <code>ScannerValidationManager</code> is initialized and available in <code>ScanPhase</code></li> <li>\u2705 Registration Validation: Scanner registration validation is active after plugin discovery</li> <li>\u2705 Enablement Validation: Scanner enablement validation is active during scanner filtering</li> <li>\u2705 Task Queue Validation: Task queue validation is integrated and active after queue population</li> <li>\u2705 Execution Completion Validation: Execution completion validation is integrated and active after scanner execution</li> <li>\u2705 Result Completeness Validation: Result completeness validation is integrated and active before finalizing results</li> <li>\u2705 Core Validation Methods: All validation methods are implemented and tested</li> <li>\u2705 Integration Tests: Comprehensive integration tests validate all validation checkpoints</li> <li>\u2705 Error Handling: Graceful error handling and recovery mechanisms are implemented</li> </ul> <p>The validation manager is fully integrated and actively validates all aspects of the scanner lifecycle. All five validation checkpoints are operational and provide comprehensive visibility into scanner status throughout the scan process.</p>"},{"location":"developer-guide/scanner-validation-system/#integration-with-scanphase_1","title":"Integration with ScanPhase","text":"<p>The Scanner Validation System is fully integrated with the existing <code>ScanPhase</code> class at all key checkpoints. The <code>ScannerValidationManager</code> is initialized during <code>ScanPhase</code> construction and used throughout the scan lifecycle.</p>"},{"location":"developer-guide/scanner-validation-system/#initialization_1","title":"Initialization","text":"<pre><code>class ScanPhase(EnginePhase):\n    def __init__(self, plugin_context, plugins=None, progress_display=None, asharp_model=None):\n        \"\"\"Initialize the ScanPhase with validation manager.\"\"\"\n        super().__init__(plugin_context, plugins or [], progress_display, asharp_model)\n        self.validation_manager = ScannerValidationManager(plugin_context)\n</code></pre>"},{"location":"developer-guide/scanner-validation-system/#validation-integration-points_1","title":"Validation Integration Points","text":"<p>The validation manager is integrated at all key points during scan execution:</p> <pre><code>def _execute_phase(self, aggregated_results, **kwargs):\n    # 1. Registration validation - ACTIVE\n    scanner_classes = self.plugins\n    if scanner_classes:\n        self.validation_manager.validate_registered_scanners(scanner_classes)\n\n    # 2. Enablement validation - ACTIVE\n    enabled, excluded, dep_errors = self._filter_scanners(scanner_classes)\n    self.validation_manager.validate_scanner_enablement(enabled, excluded, dep_errors)\n\n    # 3. Task queue validation - ACTIVE\n    self._validate_task_queue(aggregated_results)\n\n    # 4. Execute scanners\n    results = self._execute_scanners()\n\n    # 5. Execution completion validation - ACTIVE\n    self._validate_execution_completion(aggregated_results)\n\n    # 6. Result completeness validation - ACTIVE\n    self._validate_result_completeness(aggregated_results)\n</code></pre>"},{"location":"developer-guide/scanner-validation-system/#validation-methods-in-scanphase","title":"Validation Methods in ScanPhase","text":"<p>The <code>ScanPhase</code> class includes several validation methods that integrate with the validation manager:</p>"},{"location":"developer-guide/scanner-validation-system/#_validate_task_queueaggregated_results","title":"<code>_validate_task_queue(aggregated_results)</code>","text":"<ul> <li>Extracts queue contents and validates against expected scanners</li> <li>Handles retry logic for missing scanners</li> <li>Adds validation checkpoints to aggregated results</li> <li>Provides comprehensive error handling</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#_validate_execution_completionaggregated_results","title":"<code>_validate_execution_completion(aggregated_results)</code>","text":"<ul> <li>Validates that all expected scanners completed execution</li> <li>Identifies missing scanners and reports discrepancies</li> <li>Updates validation summary in metadata</li> <li>Handles graceful error recovery</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#_validate_result_completenessaggregated_results","title":"<code>_validate_result_completeness(aggregated_results)</code>","text":"<ul> <li>Ensures all registered scanners appear in final results</li> <li>Adds missing scanners with appropriate status</li> <li>Includes failure reasons for failed scanners</li> <li>Maintains complete scan coverage visibility</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#integration-verification","title":"Integration Verification","text":"<p>A verification script (<code>verify_integration.py</code>) is available to test the scanner validation integration:</p> <pre><code># Run the integration verification test\npython verify_integration.py\n</code></pre> <p>This script validates that: - The <code>ScannerValidationManager</code> is properly initialized in <code>ScanPhase</code> - Validation methods are available and callable - The integration points are working correctly</p>"},{"location":"developer-guide/scanner-validation-system/#validation-output-and-reporting","title":"Validation Output and Reporting","text":""},{"location":"developer-guide/scanner-validation-system/#validation-checkpoints-in-results","title":"Validation Checkpoints in Results","text":"<p>All validation checkpoints are automatically added to the <code>AshAggregatedResults</code> object and included in the final scan output. Each checkpoint contains:</p> <ul> <li>Checkpoint Name: Identifies the validation phase (e.g., \"registered_scanners\", \"task_queue_validation\")</li> <li>Timestamp: When the validation occurred</li> <li>Expected vs Actual Scanners: Lists of scanners expected and actually found</li> <li>Discrepancies: Detailed list of any issues found</li> <li>Metadata: Additional context and statistics</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#comprehensive-validation-report","title":"Comprehensive Validation Report","text":"<p>The Scanner Validation System can generate a comprehensive validation report using the <code>generate_validation_report()</code> method. This report provides:</p>"},{"location":"developer-guide/scanner-validation-system/#executive-summary","title":"Executive Summary","text":"<ul> <li>Total scanners tracked, registered, enabled, completed, and included in results</li> <li>Total validation checkpoints and count of checkpoints with issues</li> <li>Overall validation status indicator (\u2705 or \u26a0\ufe0f)</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#scanner-states-summary","title":"Scanner States Summary","text":"<p>Scanners are grouped by status for easy analysis: - Registered &amp; Enabled: Scanners that are properly configured and ready - Registered &amp; Disabled: Scanners that are registered but disabled - Missing Dependencies: Scanners that cannot run due to missing dependencies - Excluded: Scanners that are excluded by configuration - Failed: Scanners that encountered failures during execution - Unknown Status: Scanners with unclear status</p>"},{"location":"developer-guide/scanner-validation-system/#validation-checkpoints-detail","title":"Validation Checkpoints Detail","text":"<p>For each validation checkpoint: - Checkpoint name and timestamp - Expected vs actual scanner counts - Specific discrepancies and errors found - Missing and unexpected scanners identified</p>"},{"location":"developer-guide/scanner-validation-system/#detailed-scanner-information","title":"Detailed Scanner Information","text":"<p>For each tracked scanner: - Registration and enablement status - Specific enablement reasons and dependency errors - Execution and result inclusion status - Failure reasons and metadata</p>"},{"location":"developer-guide/scanner-validation-system/#dependency-analysis","title":"Dependency Analysis","text":"<ul> <li>Count of scanners with dependency issues</li> <li>Most common missing dependencies across scanners</li> <li>Scanner-specific dependency error details</li> </ul>"},{"location":"developer-guide/scanner-validation-system/#actionable-recommendations","title":"Actionable Recommendations","text":"<p>Based on validation findings: - Suggestions for resolving dependency issues - Configuration review recommendations - Investigation guidance for failed scanners</p>"},{"location":"developer-guide/scanner-validation-system/#example-validation-report-output","title":"Example Validation Report Output","text":"<pre><code>================================================================================\nSCANNER VALIDATION REPORT\n================================================================================\n\nEXECUTIVE SUMMARY\n----------------------------------------\nTotal Scanners Tracked: 6\nRegistered Scanners: 6\nEnabled Scanners: 4\nCompleted Scanners: 4\nIncluded in Results: 6\nTotal Checkpoints: 5\nCheckpoints with Issues: 2\n\u26a0\ufe0f  VALIDATION ISSUES DETECTED\n\nSCANNER STATES SUMMARY\n----------------------------------------\nRegistered &amp; Enabled (4):\n  \u2022 bandit [Queued, Completed, In Results]\n  \u2022 detect-secrets [Queued, Completed, In Results]\n  \u2022 grype [Queued, Completed, In Results]\n  \u2022 semgrep [Queued, Completed, In Results]\n\nMissing Dependencies (1):\n  \u2022 checkov [In Results, Failed: Missing dependencies: Missing required tool: terraform]\n\nExcluded (1):\n  \u2022 syft [In Results]\n\nVALIDATION CHECKPOINTS\n----------------------------------------\n1. REGISTERED_SCANNERS\n   Timestamp: 2023-01-01 12:00:00\n   Expected: 6 scanners\n   Actual: 6 scanners\n   \u2705 No issues detected\n\n2. SCANNER_ENABLEMENT\n   Timestamp: 2023-01-01 12:00:01\n   Expected: 6 scanners\n   Actual: 4 scanners\n   \u26a0\ufe0f  ISSUES DETECTED:\n   Discrepancies:\n     - Scanner 'checkov' not enabled: Missing dependencies: Missing required tool: terraform\n     - Scanner 'syft' not enabled: Scanner is excluded by configuration\n   Missing: checkov, syft\n\nRECOMMENDATIONS\n----------------------------------------\n1. Install missing dependencies for 1 scanners\n2. Review configuration for 1 excluded scanners\n3. Investigate failure reasons for 1 scanners\n\n================================================================================\nReport generated at: 2023-01-01 12:00:32\n================================================================================\n</code></pre>"},{"location":"developer-guide/scanner-validation-system/#validation-summary-in-metadata","title":"Validation Summary in Metadata","text":"<p>The validation system adds a comprehensive summary to the scan metadata:</p> <pre><code>{\n  \"validation_summary\": {\n    \"registration_validation\": {\n      \"timestamp\": \"2023-01-01T12:00:00\",\n      \"total_registered\": 6,\n      \"has_issues\": false\n    },\n    \"enablement_validation\": {\n      \"timestamp\": \"2023-01-01T12:00:01\",\n      \"enabled_count\": 4,\n      \"excluded_count\": 1,\n      \"dependency_error_count\": 1,\n      \"has_issues\": true\n    },\n    \"task_queue_validation\": {\n      \"timestamp\": \"2023-01-01T12:00:02\",\n      \"expected_count\": 4,\n      \"actual_count\": 3,\n      \"missing_count\": 1,\n      \"successfully_retried\": 0,\n      \"has_issues\": true\n    },\n    \"execution_completion_validation\": {\n      \"timestamp\": \"2023-01-01T12:00:30\",\n      \"expected_count\": 4,\n      \"completed_count\": 4,\n      \"missing_count\": 0,\n      \"completion_rate\": 1.0,\n      \"has_issues\": false\n    },\n    \"result_completeness_validation\": {\n      \"timestamp\": \"2023-01-01T12:00:31\",\n      \"originally_registered\": 6,\n      \"in_results\": 6,\n      \"added_missing\": 2,\n      \"completeness_rate\": 1.0,\n      \"has_issues\": false\n    }\n  }\n}\n</code></pre>"},{"location":"developer-guide/scanner-validation-system/#accessing-validation-information","title":"Accessing Validation Information","text":"<p>Validation information can be accessed from the aggregated results:</p> <pre><code># Access validation checkpoints\nfor checkpoint in aggregated_results.validation_checkpoints:\n    print(f\"Checkpoint: {checkpoint['checkpoint_name']}\")\n    print(f\"Issues: {len(checkpoint.get('discrepancies', []))}\")\n\n# Access validation summary\nvalidation_summary = aggregated_results.metadata.validation_summary\nif validation_summary:\n    for phase, summary in validation_summary.items():\n        print(f\"{phase}: {'\u2705' if not summary.get('has_issues') else '\u26a0\ufe0f'}\")\n\n# Generate comprehensive validation report\nvalidation_manager = ScannerValidationManager(plugin_context)\n# ... perform validation activities ...\nvalidation_report = validation_manager.generate_validation_report()\nprint(validation_report)\n\n# Save validation report to file\nwith open('.ash/ash_output/validation_report.txt', 'w') as f:\n    f.write(validation_report)\n</code></pre>"},{"location":"developer-guide/scanner-validation-system/#benefits","title":"Benefits","text":"<p>The Scanner Validation System provides several key benefits:</p> <ol> <li>Reliability: Ensures all expected scanners are included in scans</li> <li>Visibility: Provides clear insight into scanner status throughout the scan process</li> <li>Debugging: Comprehensive logging helps troubleshoot intermittent scanner issues</li> <li>Recovery: Automatic retry logic can recover from transient failures</li> <li>Completeness: Guarantees all registered scanners appear in final results</li> <li>Consistency: Standardized validation approach across all scan phases</li> </ol>"},{"location":"developer-guide/scanner-validation-system/#future-enhancements","title":"Future Enhancements","text":"<p>Potential future enhancements to the Scanner Validation System:</p> <ol> <li>Metrics Collection: Collect validation metrics for monitoring and alerting</li> <li>Configurable Retry Logic: Allow configuration of retry attempts and strategies</li> <li>Validation Reporting: Generate detailed validation reports for audit purposes</li> <li>Performance Monitoring: Track validation overhead and optimize performance</li> <li>Custom Validation Rules: Allow custom validation rules for specific environments</li> </ol>"},{"location":"developer-guide/unified-metrics/","title":"Unified Metrics System","text":"<p>The Unified Metrics System provides a centralized implementation for calculating and accessing scanner metrics across all components of ASH. It serves as the single source of truth for scanner statistics, ensuring consistency across all reports and displays.</p>"},{"location":"developer-guide/unified-metrics/#overview","title":"Overview","text":"<p>Prior to the implementation of the Unified Metrics System, scanner statistics were calculated inconsistently across different parts of the application. This led to discrepancies in how scanner statistics were displayed in different report formats and in the terminal output.</p> <p>The Unified Metrics System addresses this issue by providing a centralized implementation for calculating scanner statistics that is used consistently across all components of ASH. This ensures that all reports and displays show the same statistics for each scanner.</p>"},{"location":"developer-guide/unified-metrics/#architecture","title":"Architecture","text":"<p>The Unified Metrics System follows a layered architecture:</p> <ol> <li>Core Statistics Calculation Layer: The <code>ScannerStatisticsCalculator</code> class is responsible for extracting raw data from the SARIF and scanner results.</li> <li>Unified Metrics Layer: The <code>unified_metrics</code> module processes the raw data to produce consistent metrics objects.</li> <li>Presentation Layer: Various reporters and displays consume the unified metrics to display statistics in different formats.</li> </ol>"},{"location":"developer-guide/unified-metrics/#key-components","title":"Key Components","text":""},{"location":"developer-guide/unified-metrics/#scannerstatisticscalculator","title":"ScannerStatisticsCalculator","text":"<p>The <code>ScannerStatisticsCalculator</code> class provides static methods for extracting and calculating scanner statistics from the final aggregated SARIF data. It is used by the unified metrics module but can also be used directly if more fine-grained control over the statistics calculation is needed.</p>"},{"location":"developer-guide/unified-metrics/#extract_scanner_statisticsasharp_model-ashaggregatedresults-dictstr-dictstr-any","title":"<code>extract_scanner_statistics(asharp_model: AshAggregatedResults) -&gt; Dict[str, Dict[str, Any]]</code>","text":"<p>Extract statistics for all scanners from the final aggregated SARIF file.</p>"},{"location":"developer-guide/unified-metrics/#extract_sarif_counts_for_scannerasharp_model-ashaggregatedresults-scanner_name-str-tupleint-int-int-int-int-int","title":"<code>extract_sarif_counts_for_scanner(asharp_model: AshAggregatedResults, scanner_name: str) -&gt; Tuple[int, int, int, int, int, int]</code>","text":"<p>Extract severity counts from the final processed SARIF data for a specific scanner.</p>"},{"location":"developer-guide/unified-metrics/#calculate_actionable_countcritical-int-high-int-medium-int-low-int-info-int-threshold-str-int","title":"<code>calculate_actionable_count(critical: int, high: int, medium: int, low: int, info: int, threshold: str) -&gt; int</code>","text":"<p>Calculate the number of actionable findings based on the threshold.</p>"},{"location":"developer-guide/unified-metrics/#get_scanner_threshold_infoasharp_model-ashaggregatedresults-scanner_name-str-tuplestr-str","title":"<code>get_scanner_threshold_info(asharp_model: AshAggregatedResults, scanner_name: str) -&gt; Tuple[str, str]</code>","text":"<p>Get the threshold and threshold source for a scanner.</p>"},{"location":"developer-guide/unified-metrics/#get_scanner_status_infoasharp_model-ashaggregatedresults-scanner_name-str-tuplebool-bool","title":"<code>get_scanner_status_info(asharp_model: AshAggregatedResults, scanner_name: str) -&gt; Tuple[bool, bool]</code>","text":"<p>Get scanner status information.</p>"},{"location":"developer-guide/unified-metrics/#get_scanner_statusasharp_model-ashaggregatedresults-scanner_name-str-str","title":"<code>get_scanner_status(asharp_model: AshAggregatedResults, scanner_name: str) -&gt; str</code>","text":"<p>Determine the status of a scanner based on its findings and configuration.</p>"},{"location":"developer-guide/unified-metrics/#get_summary_statisticsasharp_model-ashaggregatedresults-dictstr-any","title":"<code>get_summary_statistics(asharp_model: AshAggregatedResults) -&gt; Dict[str, Any]</code>","text":"<p>Calculate summary statistics across all scanners.</p>"},{"location":"developer-guide/unified-metrics/#unified-metrics-module","title":"Unified Metrics Module","text":"<p>The <code>unified_metrics</code> module provides a higher-level API for accessing scanner metrics. It defines the <code>ScannerMetrics</code> data structure and provides functions to generate unified scanner metrics from the final aggregated SARIF data.</p>"},{"location":"developer-guide/unified-metrics/#scannermetrics-named-tuple","title":"<code>ScannerMetrics</code> Named Tuple","text":"<p>The <code>ScannerMetrics</code> named tuple is the single source of truth for scanner metrics that should be used by all table generators and reporters. It contains comprehensive statistics for a single scanner, including counts for different severity levels, suppressed findings, actionable findings, and status information.</p>"},{"location":"developer-guide/unified-metrics/#get_unified_scanner_metricsasharp_model-ashaggregatedresults-listscannermetrics","title":"<code>get_unified_scanner_metrics(asharp_model: AshAggregatedResults) -&gt; List[ScannerMetrics]</code>","text":"<p>Generate unified scanner metrics from AshAggregatedResults.</p>"},{"location":"developer-guide/unified-metrics/#get_summary_metricsasharp_model-ashaggregatedresults-dictstr-any","title":"<code>get_summary_metrics(asharp_model: AshAggregatedResults) -&gt; Dict[str, Any]</code>","text":"<p>Get summary metrics from AshAggregatedResults.</p>"},{"location":"developer-guide/unified-metrics/#format_durationduration_seconds-optionalfloat-str","title":"<code>format_duration(duration_seconds: Optional[float]) -&gt; str</code>","text":"<p>Format duration in seconds to a human-readable string.</p>"},{"location":"developer-guide/unified-metrics/#usage","title":"Usage","text":""},{"location":"developer-guide/unified-metrics/#for-reporters-and-displays","title":"For Reporters and Displays","text":"<p>Reporters and displays should use the <code>get_unified_scanner_metrics</code> function to get scanner metrics:</p> <pre><code>from automated_security_helper.core.unified_metrics import get_unified_scanner_metrics\n\n# Get unified metrics for all scanners\nscanner_metrics = get_unified_scanner_metrics(asharp_model)\n\n# Access metrics for a specific scanner\nfor metrics in scanner_metrics:\n    if metrics.scanner_name == \"bandit\":\n        print(f\"Bandit found {metrics.actionable} actionable findings\")\n</code></pre>"},{"location":"developer-guide/unified-metrics/#for-summary-statistics","title":"For Summary Statistics","text":"<p>To get summary statistics across all scanners, use the <code>get_summary_metrics</code> function:</p> <pre><code>from automated_security_helper.core.unified_metrics import get_summary_metrics\n\n# Get summary metrics\nsummary = get_summary_metrics(asharp_model)\nprint(f\"Total findings: {summary['total_findings']}\")\nprint(f\"Actionable findings: {summary['total_actionable']}\")\n</code></pre>"},{"location":"developer-guide/unified-metrics/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use the Unified Metrics System: Always use the Unified Metrics System for calculating and displaying scanner statistics. This ensures consistency across all reports and displays.</p> </li> <li> <p>Avoid Direct SARIF Processing: Avoid processing SARIF data directly to extract statistics. Instead, use the <code>ScannerStatisticsCalculator</code> or the <code>unified_metrics</code> module.</p> </li> <li> <p>Consistent Status Reporting: Use the status values provided by the Unified Metrics System for consistent status reporting across all reports and displays.</p> </li> <li> <p>Threshold-Based Actionable Findings: Use the <code>actionable</code> field in the <code>ScannerMetrics</code> named tuple to determine if a scanner has actionable findings based on its threshold.</p> </li> <li> <p>Formatting Durations: Use the <code>format_duration</code> function to format scanner durations consistently across all reports and displays.</p> </li> </ol>"},{"location":"developer-guide/unified-metrics/#deprecated-components","title":"Deprecated Components","text":"<p>The following components are deprecated and should not be used in new code:</p> <ul> <li><code>scanner_metrics_calculator.py</code>: This module is kept for backward compatibility but should not be used in new code. All functionality has been moved to the <code>scanner_statistics_calculator.py</code> module.</li> </ul>"},{"location":"developer-guide/unified-metrics/#conclusion","title":"Conclusion","text":"<p>The Unified Metrics System provides a centralized implementation for calculating and accessing scanner metrics across all components of ASH. By using this system consistently, we ensure that all reports and displays show the same statistics for each scanner, improving the user experience and making the application more maintainable.</p>"},{"location":"developer-guide/uv-tool-management/","title":"UV Tool Management","text":"<p>ASH v3 uses UV's tool isolation system to manage security scanner dependencies automatically. This system provides consistent tool versions, avoids dependency conflicts, and enables seamless tool installation without affecting your project environment.</p>"},{"location":"developer-guide/uv-tool-management/#overview","title":"Overview","text":"<p>The UV tool management system in ASH provides:</p> <ul> <li>Automatic tool installation: Tools like Checkov, Semgrep, and Bandit are installed automatically when needed</li> <li>Version constraints: Ensures compatible tool versions with sensible defaults</li> <li>Isolation: Tools run in isolated environments without dependency conflicts</li> <li>Fallback mechanisms: Graceful fallback to pre-installed tools when UV is unavailable</li> <li>Offline support: Respects offline mode settings and uses pre-installed tools</li> <li>Comprehensive logging: Detailed installation and execution logging for troubleshooting</li> </ul>"},{"location":"developer-guide/uv-tool-management/#architecture","title":"Architecture","text":""},{"location":"developer-guide/uv-tool-management/#core-components","title":"Core Components","text":""},{"location":"developer-guide/uv-tool-management/#uvtoolrunner","title":"UVToolRunner","text":"<p>The main class responsible for UV tool operations:</p> <pre><code>from automated_security_helper.utils.uv_tool_runner import get_uv_tool_runner\n\nrunner = get_uv_tool_runner()\n</code></pre> <p>Key methods: - <code>is_uv_available()</code>: Check if UV is available on the system - <code>is_tool_installed(tool_name)</code>: Check if a specific tool is installed - <code>install_tool_with_version()</code>: Install a tool with version constraints - <code>run_tool()</code>: Execute a UV tool with arguments - <code>get_tool_version()</code>: Get version information for installed tools</p>"},{"location":"developer-guide/uv-tool-management/#uvtoolinstallationstatus","title":"UVToolInstallationStatus","text":"<p>Tracks the complete installation state of UV tools:</p> <pre><code>from automated_security_helper.models.uv_tool_installation import UVToolInstallationStatus\n\nstatus = UVToolInstallationStatus(\n    tool_name=\"checkov\",\n    is_installed=True,\n    installed_version=\"3.2.5\",\n    preferred_source=\"uv\"\n)\n</code></pre>"},{"location":"developer-guide/uv-tool-management/#uvtoolretryconfig","title":"UVToolRetryConfig","text":"<p>Configures retry logic for tool operations:</p> <pre><code>from automated_security_helper.utils.uv_tool_runner import UVToolRetryConfig\n\nretry_config = UVToolRetryConfig(\n    max_retries=3,\n    base_delay=1.0,\n    max_delay=60.0,\n    exponential_base=2.0,\n    jitter=True\n)\n</code></pre>"},{"location":"developer-guide/uv-tool-management/#scanner-integration","title":"Scanner Integration","text":""},{"location":"developer-guide/uv-tool-management/#uv-enabled-scanners","title":"UV-Enabled Scanners","text":"<p>Scanners that use UV tool management:</p> Scanner Default Version Constraint Installation Method Bandit <code>&gt;=1.7.0</code> <code>uv tool install bandit&gt;=1.7.0</code> Checkov <code>&gt;=3.2.0,&lt;4.0.0</code> <code>uv tool install checkov&gt;=3.2.0,&lt;4.0.0</code> Semgrep <code>&gt;=1.125.0</code> <code>uv tool install semgrep&gt;=1.125.0</code>"},{"location":"developer-guide/uv-tool-management/#scanner-configuration","title":"Scanner Configuration","text":"<p>Enable UV tool management in scanner plugins:</p> <pre><code>class MyScanner(ScannerPluginBase):\n    def model_post_init(self, context):\n        self.use_uv_tool = True  # Enable UV tool execution\n        self.command = \"my-tool\"\n\n        # Set up UV tool installation\n        self._setup_uv_tool_install_commands()\n\n        # Get tool version from UV\n        self.tool_version = self._get_uv_tool_version(\"my-tool\")\n</code></pre>"},{"location":"developer-guide/uv-tool-management/#version-constraints","title":"Version Constraints","text":"<p>Override default version constraints:</p> <pre><code>def _get_tool_version_constraint(self) -&gt; str | None:\n    \"\"\"Get version constraint for tool installation.\"\"\"\n    if self.config and self.config.options.tool_version:\n        return self.config.options.tool_version\n\n    # Use tool-specific default constraint\n    return \"&gt;=2.0.0,&lt;3.0.0\"\n</code></pre>"},{"location":"developer-guide/uv-tool-management/#installation-process","title":"Installation Process","text":""},{"location":"developer-guide/uv-tool-management/#automatic-installation-flow","title":"Automatic Installation Flow","text":"<ol> <li>UV Availability Check: Verify UV is installed and accessible</li> <li>Tool Status Check: Check if tool is already installed via UV</li> <li>Installation Attempt: Install tool with version constraints if needed</li> <li>Retry Logic: Retry installation with exponential backoff on failure</li> <li>Fallback: Use pre-installed tools if UV installation fails</li> </ol>"},{"location":"developer-guide/uv-tool-management/#installation-logging","title":"Installation Logging","text":"<p>The system provides comprehensive logging for troubleshooting:</p> <pre><code>[INSTALLATION_START] Initiating UV tool installation for 'checkov'\n[INSTALLATION_PROGRESS] UV availability confirmed in 0.123s\n[INSTALLATION_ATTEMPT] Starting installation of UV tool\nTool specification: checkov&gt;=3.2.0,&lt;4.0.0\n[INSTALLATION_SUCCESS] Successfully installed UV tool 'checkov'\nInstalled version: 3.2.5\nTotal installation time: 45.678s\n</code></pre>"},{"location":"developer-guide/uv-tool-management/#error-handling","title":"Error Handling","text":"<p>Common installation scenarios and their handling:</p> <ul> <li>UV Not Available: Falls back to pre-installed tools</li> <li>Network Issues: Retries with exponential backoff</li> <li>Version Conflicts: Logs detailed error information</li> <li>Timeout: Configurable timeout with graceful failure</li> <li>Offline Mode: Skips installation and uses existing tools</li> </ul>"},{"location":"developer-guide/uv-tool-management/#configuration","title":"Configuration","text":""},{"location":"developer-guide/uv-tool-management/#environment-variables","title":"Environment Variables","text":"<p>Control UV tool behavior with environment variables:</p> <pre><code># Disable automatic tool installation\nexport ASH_OFFLINE=true\n\n# Custom UV executable path\nexport UV_EXECUTABLE=/custom/path/to/uv\n</code></pre>"},{"location":"developer-guide/uv-tool-management/#scanner-options","title":"Scanner Options","text":"<p>Configure UV tool behavior per scanner:</p> <pre><code>scanners:\n  checkov:\n    enabled: true\n    options:\n      tool_version: \"&gt;=3.2.0,&lt;4.0.0\"  # Override version constraint\n      install_timeout: 300             # Installation timeout in seconds\n</code></pre>"},{"location":"developer-guide/uv-tool-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer-guide/uv-tool-management/#common-issues","title":"Common Issues","text":""},{"location":"developer-guide/uv-tool-management/#uv-not-found","title":"UV Not Found","text":"<pre><code>UV tool execution is required but UV is not available\n</code></pre> <p>Solutions: - Install UV: <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code> - Use offline mode: <code>ASH_OFFLINE=true ash --mode local</code> - Pre-install tools manually</p>"},{"location":"developer-guide/uv-tool-management/#installation-timeout","title":"Installation Timeout","text":"<pre><code>UV tool install timeout for checkov&gt;=3.2.0,&lt;4.0.0\n</code></pre> <p>Solutions: - Increase timeout: Configure <code>install_timeout</code> in scanner options - Check network connectivity - Use pre-installed tools in offline environments</p>"},{"location":"developer-guide/uv-tool-management/#version-conflicts","title":"Version Conflicts","text":"<pre><code>Failed to install UV tool checkov&gt;=3.2.0,&lt;4.0.0 after 3 attempts\n</code></pre> <p>Solutions: - Check version constraint compatibility - Install manually: <code>uv tool install checkov&gt;=3.2.0,&lt;4.0.0</code> - Use system-installed version</p>"},{"location":"developer-guide/uv-tool-management/#debug-information","title":"Debug Information","text":"<p>Enable verbose logging for detailed UV tool information:</p> <pre><code>ash --mode local --verbose\n</code></pre> <p>This provides: - UV availability status - Tool installation attempts and results - Version detection information - Fallback mechanism activation - Execution method selection (UV vs direct)</p>"},{"location":"developer-guide/uv-tool-management/#manual-tool-management","title":"Manual Tool Management","text":"<p>Pre-install tools to avoid automatic installation:</p> <pre><code># Install specific versions\nuv tool install bandit&gt;=1.7.0\nuv tool install checkov&gt;=3.2.0,&lt;4.0.0\nuv tool install semgrep&gt;=1.125.0\n\n# List installed tools\nuv tool list\n\n# Update tools\nuv tool upgrade bandit\n</code></pre>"},{"location":"developer-guide/uv-tool-management/#best-practices","title":"Best Practices","text":""},{"location":"developer-guide/uv-tool-management/#for-users","title":"For Users","text":"<ol> <li>Install UV: Ensure UV is available for optimal tool management</li> <li>Network Access: Provide internet access for automatic installations</li> <li>Version Pinning: Use specific version constraints for reproducible builds</li> <li>Offline Preparation: Pre-install tools for air-gapped environments</li> </ol>"},{"location":"developer-guide/uv-tool-management/#for-developers","title":"For Developers","text":"<ol> <li>Graceful Fallbacks: Always provide fallback to pre-installed tools</li> <li>Comprehensive Logging: Log installation attempts and results</li> <li>Version Constraints: Use sensible default version constraints</li> <li>Error Handling: Handle UV unavailability gracefully</li> <li>Testing: Test both UV and direct execution paths</li> </ol>"},{"location":"developer-guide/uv-tool-management/#integration-examples","title":"Integration Examples","text":""},{"location":"developer-guide/uv-tool-management/#custom-scanner-with-uv-support","title":"Custom Scanner with UV Support","text":"<pre><code>@ash_scanner_plugin\nclass CustomScanner(ScannerPluginBase[CustomScannerConfig]):\n    def model_post_init(self, context):\n        self.command = \"custom-tool\"\n        self.use_uv_tool = True\n\n        # Set up UV tool installation\n        self._setup_uv_tool_install_commands()\n\n        # Get version from UV or fallback\n        self.tool_version = self._get_uv_tool_version(\"custom-tool\")\n\n        super().model_post_init(context)\n\n    def _get_tool_version_constraint(self) -&gt; str | None:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n    def validate_plugin_dependencies(self) -&gt; bool:\n        # Validate UV tool availability\n        if not self._validate_uv_tool_availability():\n            return False\n\n        # Attempt installation if needed\n        if self.use_uv_tool and not self._is_uv_tool_installed():\n            if self._install_uv_tool(timeout=300):\n                self.dependencies_satisfied = True\n                return True\n\n        # Fallback to base validation\n        return super().validate_plugin_dependencies()\n</code></pre>"},{"location":"developer-guide/uv-tool-management/#testing-uv-tool-integration","title":"Testing UV Tool Integration","text":"<pre><code>@patch('automated_security_helper.utils.uv_tool_runner.get_uv_tool_runner')\ndef test_uv_tool_integration(mock_get_runner):\n    mock_runner = Mock()\n    mock_get_runner.return_value = mock_runner\n\n    # Configure mock behavior\n    mock_runner.is_uv_available.return_value = True\n    mock_runner.is_tool_installed.return_value = False\n    mock_runner.install_tool_with_version.return_value = True\n\n    # Test scanner initialization\n    scanner = CustomScanner(config=config, context=context)\n    assert scanner.validate_plugin_dependencies() is True\n\n    # Verify UV tool operations\n    mock_runner.install_tool_with_version.assert_called_once()\n</code></pre> <p>This UV tool management system provides a robust foundation for managing security scanner dependencies while maintaining flexibility and reliability across different environments.</p>"},{"location":"docs/advanced-usage/","title":"Advanced Usage","text":"<p>This guide covers advanced features and usage patterns for ASH v3.</p>"},{"location":"docs/advanced-usage/#execution-modes","title":"Execution Modes","text":"<p>ASH v3 supports three execution modes:</p>"},{"location":"docs/advanced-usage/#local-mode","title":"Local Mode","text":"<pre><code>ash --mode local\n</code></pre> <ul> <li>Runs entirely in the local Python process</li> <li>Only uses Python-based scanners by default</li> <li>Fastest execution but limited scanner coverage</li> <li>Ideal for quick checks during development</li> </ul>"},{"location":"docs/advanced-usage/#container-mode","title":"Container Mode","text":"<pre><code>ash --mode container\n</code></pre> <ul> <li>Runs non-Python scanners in a container</li> <li>Provides full scanner coverage</li> <li>Requires a container runtime (Docker, Podman, etc.)</li> <li>Ideal for comprehensive scans</li> </ul>"},{"location":"docs/advanced-usage/#precommit-mode","title":"Precommit Mode","text":"<pre><code>ash --mode precommit\n</code></pre> <ul> <li>Runs a subset of fast scanners</li> <li>Optimized for pre-commit hooks</li> <li>Includes only Python-based scanners + npm audit</li> <li>Ideal for git hooks and quick CI checks</li> </ul>"},{"location":"docs/advanced-usage/#custom-plugins","title":"Custom Plugins","text":"<p>ASH v3 supports custom plugins for extending functionality:</p>"},{"location":"docs/advanced-usage/#creating-custom-plugins","title":"Creating Custom Plugins","text":"<ol> <li>Create a Python module with your plugins:</li> </ol> <pre><code># my_ash_plugins/scanners.py\nfrom automated_security_helper.plugins.decorators import ash_scanner_plugin\nfrom automated_security_helper.base.scanner_plugin import ScannerPluginBase, ScannerPluginConfigBase\nfrom pydantic import Field\nfrom pathlib import Path\nfrom typing import List, Literal\n\nclass MyCustomScannerConfig(ScannerPluginConfigBase):\n    \"\"\"Configuration for MyCustomScanner\"\"\"\n    class Options:\n        custom_option: str = Field(default=\"default\", description=\"Custom option\")\n\n@ash_scanner_plugin\nclass MyCustomScanner(ScannerPluginBase):\n    \"\"\"Custom scanner implementation\"\"\"\n    name = \"my-custom-scanner\"\n    description = \"My custom security scanner\"\n    version = \"1.0.0\"\n\n    def scan(self, target: Path, target_type: Literal[\"source\", \"converted\"],\n             global_ignore_paths: List = [], config=None):\n        # Implement your scanning logic here\n        results = self._run_subprocess([\"my-scanner\", \"--target\", str(target)])\n        return results\n</code></pre> <ol> <li>Add your module to ASH configuration:</li> </ol> <pre><code>ash_plugin_modules:\n  - my_ash_plugins\n</code></pre> <ol> <li>Use your custom scanner:</li> </ol> <pre><code>ash --ash-plugin-modules my_ash_plugins\n</code></pre>"},{"location":"docs/advanced-usage/#offline-mode","title":"Offline Mode","text":"<p>For air-gapped environments:</p> <pre><code># Build an offline image\nash build-image --offline --offline-semgrep-rulesets p/ci\n\n# Run in offline mode\nash --mode container --offline\n</code></pre>"},{"location":"docs/advanced-usage/#customizing-scan-phases","title":"Customizing Scan Phases","text":"<p>ASH v3 executes scans in phases:</p> <pre><code># Run only specific phases\nash --phases convert,scan\n\n# Skip report generation\nash --phases convert,scan\n\n# Include inspection phase\nash --phases convert,scan,report,inspect\n</code></pre>"},{"location":"docs/advanced-usage/#using-existing-results","title":"Using Existing Results","text":"<p>You can generate reports from existing scan results:</p> <pre><code># Use existing results file\nash --use-existing --output-dir /path/to/results\n\n# Generate a specific report format\nash report --format html --output-dir /path/to/results\n</code></pre>"},{"location":"docs/advanced-usage/#interactive-findings-explorer","title":"Interactive Findings Explorer","text":"<p>ASH v3 includes an interactive TUI for exploring findings:</p> <pre><code># Launch the findings explorer\nash inspect findings --output-dir /path/to/results\n</code></pre>"},{"location":"docs/advanced-usage/#container-customization","title":"Container Customization","text":""},{"location":"docs/advanced-usage/#using-alternative-container-runtimes","title":"Using Alternative Container Runtimes","text":"<pre><code># Use Podman instead of Docker\nash --mode container --oci-runner podman\n\n# Use Finch\nash --mode container --oci-runner finch\n</code></pre>"},{"location":"docs/advanced-usage/#custom-container-images","title":"Custom Container Images","text":"<pre><code># Specify a custom container image\nexport ASH_IMAGE_NAME=\"my-registry/ash:custom\"\nash --mode container\n</code></pre>"},{"location":"docs/advanced-usage/#building-custom-images","title":"Building Custom Images","text":"<pre><code># Build a custom image\nash build-image --build-target ci --custom-containerfile ./my-dockerfile\n</code></pre>"},{"location":"docs/advanced-usage/#advanced-configuration-overrides","title":"Advanced Configuration Overrides","text":"<pre><code># Complex configuration overrides\nash --config-overrides 'scanners.semgrep.options.rules=[\"p/ci\", \"p/owasp-top-ten\"]'\nash --config-overrides 'global_settings.ignore_paths+=[{\"path\": \"build/\", \"reason\": \"Generated files\"}]'\n</code></pre>"},{"location":"docs/advanced-usage/#working-with-suppressions","title":"Working with Suppressions","text":""},{"location":"docs/advanced-usage/#adding-suppressions-via-config-overrides","title":"Adding Suppressions via Config Overrides","text":"<pre><code># Add a suppression rule\nash --config-overrides 'global_settings.suppressions+=[{\"rule_id\": \"RULE-123\", \"file_path\": \"src/example.py\", \"reason\": \"False positive\"}]'\n\n# Add a suppression with line range and expiration\nash --config-overrides 'global_settings.suppressions+=[{\"rule_id\": \"RULE-456\", \"file_path\": \"src/*.js\", \"line_start\": 10, \"line_end\": 15, \"reason\": \"Known issue\", \"expiration\": \"2025-12-31\"}]'\n</code></pre>"},{"location":"docs/advanced-usage/#temporarily-ignoring-suppressions","title":"Temporarily Ignoring Suppressions","text":"<pre><code># Run a scan ignoring all suppression rules\nash --ignore-suppressions\n\n# Useful for verifying if suppressed issues have been fixed\nash --ignore-suppressions --output-dir ./verification-scan\n</code></pre>"},{"location":"docs/advanced-usage/#programmatic-usage","title":"Programmatic Usage","text":"<p>ASH v3 can be used programmatically in Python:</p> <pre><code>from automated_security_helper.interactions.run_ash_scan import run_ash_scan\nfrom automated_security_helper.core.enums import RunMode, Strategy\n\n# Run a scan\nresults = run_ash_scan(\n    source_dir=\"/path/to/code\",\n    output_dir=\"/path/to/output\",\n    mode=RunMode.local,\n    strategy=Strategy.parallel,\n    scanners=[\"bandit\", \"semgrep\"],\n    config_overrides=[\"scanners.bandit.enabled=true\"]\n)\n\n# Access scan results\nprint(f\"Found {results.summary_stats.total_findings} findings\")\n</code></pre>"},{"location":"docs/advanced-usage/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"docs/advanced-usage/#github-actions","title":"GitHub Actions","text":"<pre><code>name: ASH Security Scan\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n      - name: Install ASH\n        run: pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n      - name: Run ASH scan\n        run: ash --mode local\n      - name: Upload scan results\n        uses: actions/upload-artifact@v3\n        with:\n          name: ash-results\n          path: .ash/ash_output\n</code></pre>"},{"location":"docs/advanced-usage/#gitlab-ci","title":"GitLab CI","text":"<pre><code>ash-scan:\n  image: python:3.10\n  script:\n    - pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n    - ash --mode local\n  artifacts:\n    paths:\n      - .ash/ash_output\n</code></pre>"},{"location":"docs/advanced-usage/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Run scanners in parallel (default)\nash --strategy parallel\n\n# Run scanners sequentially\nash --strategy sequential\n\n# Clean up temporary files after scan\nash --cleanup\n</code></pre>"},{"location":"docs/advanced-usage/#debugging","title":"Debugging","text":"<pre><code># Enable debug logging\nash --debug\n\n# Enable verbose logging\nash --verbose\n\n# Disable progress display\nash --progress false\n</code></pre>"},{"location":"docs/advanced-usage/#scanner-validation","title":"Scanner Validation","text":"<p>ASH v3 includes comprehensive scanner validation that monitors scanner registration, enablement, and execution throughout the scan process. When debugging scanner issues:</p> <ul> <li>Check logs for validation warnings about missing or disabled scanners</li> <li>Use <code>--debug</code> to see detailed validation checkpoint information</li> <li>Look for specific reasons why scanners might be disabled (dependencies, configuration, etc.)</li> <li>Run the integration verification test: <code>python verify_integration.py</code> (if available in your installation)</li> </ul> <p>The validation system includes result completeness validation that ensures all originally registered scanners appear in the final scan results, even if they failed or were disabled. This provides complete visibility into scan coverage and helps identify any scanners that may have been silently dropped during the scan process.</p> <p>For detailed information about the scanner validation system, see the Scanner Validation System developer guide.</p>"},{"location":"docs/cli-reference/","title":"CLI Reference","text":"<p>This page provides detailed information about the ASH command-line interface.</p>"},{"location":"docs/cli-reference/#common-parameters","title":"Common Parameters","text":"<p>These parameters are available across multiple ASH commands:</p> Parameter Description Default Environment Variable Commands <code>--source-dir</code> Path to the directory containing code to scan Current directory <code>ASH_SOURCE_DIR</code> <code>scan</code> <code>--output-dir</code> Path to store scan results <code>.ash/ash_output</code> <code>ASH_OUTPUT_DIR</code> <code>scan</code>, <code>report</code> <code>--config</code>, <code>-c</code> Path to ASH configuration file <code>.ash/.ash.yaml</code> <code>ASH_CONFIG</code> <code>scan</code>, <code>config</code>, <code>plugin</code> <code>--config-overrides</code> Override configuration values (can be used multiple times) <code>scan</code>, <code>config</code>, <code>plugin</code>, <code>report</code> <code>--ash-plugin-modules</code> List of Python modules to import containing ASH plugins <code>ASH_PLUGIN_MODULES</code> <code>scan</code>, <code>plugin</code> <code>--mode</code> Execution mode: <code>local</code>, <code>container</code>, or <code>precommit</code> <code>local</code> <code>ASH_MODE</code> <code>scan</code> <code>--debug</code>, <code>-d</code> Enable debug logging <code>False</code> <code>ASH_DEBUG</code> All commands <code>--verbose</code>, <code>-v</code> Enable verbose logging <code>False</code> <code>ASH_VERBOSE</code> All commands <code>--quiet</code> Suppress non-essential output <code>False</code> <code>ASH_QUIET</code> All commands <code>--no-color</code> Disable colored output <code>False</code> <code>ASH_NO_COLOR</code> All commands <code>--oci-runner</code>, <code>-o</code> OCI runner to use <code>docker</code> <code>ASH_OCI_RUNNER</code> <code>scan</code> (container mode)"},{"location":"docs/cli-reference/#config-overrides-syntax","title":"Config Overrides Syntax","text":"<p>The <code>--config-overrides</code> parameter allows you to modify configuration values without editing the configuration file:</p> <pre><code># Basic usage\nash --config-overrides 'scanners.bandit.enabled=true'\n\n# Multiple overrides\nash \\\n  --config-overrides 'scanners.bandit.enabled=true' \\\n  --config-overrides 'global_settings.severity_threshold=MEDIUM'\n\n# Append to lists\nash --config-overrides 'ash_plugin_modules+=[\"my_custom_plugin\"]'\n\n# Complex values using JSON syntax\nash --config-overrides 'global_settings.ignore_paths+=[{\"path\": \"build/\", \"reason\": \"Generated files\"}]'\n</code></pre>"},{"location":"docs/cli-reference/#core-commands","title":"Core Commands","text":"<p>ASH v3 provides several core commands:</p> <pre><code>ash [command] [options]\n</code></pre>"},{"location":"docs/cli-reference/#available-commands","title":"Available Commands","text":"Command Description <code>scan</code> Run security scans on source code (default command) <code>config</code> Manage ASH configuration <code>plugin</code> Manage ASH plugins <code>report</code> Generate reports from scan results <code>dependencies</code> Install dependencies for ASH plugins <code>inspect</code> Inspect and analyze ASH outputs and reports <code>build-image</code> Build the ASH container image"},{"location":"docs/cli-reference/#scan-command","title":"Scan Command","text":"<p>The <code>scan</code> command is the primary command for running security scans. If no command is specified, ASH defaults to the <code>scan</code> command.</p> <pre><code>ash [options]\n</code></pre>"},{"location":"docs/cli-reference/#scan-options","title":"Scan Options","text":"Option Description Default Environment Variable <code>--source-dir</code> Path to the directory containing code to scan Current directory <code>ASH_SOURCE_DIR</code> <code>--output-dir</code> Path to store scan results <code>.ash/ash_output</code> <code>ASH_OUTPUT_DIR</code> <code>--mode</code> Execution mode: <code>local</code>, <code>container</code>, or <code>precommit</code> <code>local</code> <code>ASH_MODE</code> <code>--config</code>, <code>-c</code> Path to ASH configuration file <code>.ash/.ash.yaml</code> <code>ASH_CONFIG</code> <code>--config-overrides</code> Override configuration values <code>--ash-plugin-modules</code> List of Python modules to import containing ASH plugins <code>ASH_PLUGIN_MODULES</code> <code>--scanners</code> Specific scanner names to run All enabled scanners <code>ASH_SCANNERS</code> <code>--exclude-scanners</code> Specific scanner names to exclude None <code>ASH_EXCLUDED_SCANNERS</code> <code>--output-formats</code>, <code>-f</code> Output formats (comma-separated). Available: text, flat-json, yaml, csv, html, dict, junitxml, markdown, sarif, asff, ocsf, cyclonedx, spdx, custom Default formats <code>--strategy</code> Whether to run scanners in parallel or sequential <code>parallel</code> <code>--log-level</code> Set the log level <code>INFO</code> <code>--fail-on-findings</code> Exit with non-zero code if findings are found From config <code>--ignore-suppressions</code> Ignore all suppression rules and report all findings <code>False</code> <code>--offline</code> Run in offline mode (container mode only) <code>False</code> <code>--offline-semgrep-rulesets</code> Semgrep rulesets for offline mode <code>p/ci</code> <code>--build/--no-build</code>, <code>-b/-B</code> Whether to build the ASH container image <code>True</code> <code>--run/--no-run</code>, <code>-r/-R</code> Whether to run the ASH container image <code>True</code> <code>--build-target</code> Container build target: <code>non-root</code> or <code>ci</code> <code>non-root</code> <code>--oci-runner</code>, <code>-o</code> OCI runner to use <code>docker</code> <code>ASH_OCI_RUNNER</code> <code>--python-only/--full</code> Use only Python-based plugins <code>False</code> <code>--cleanup</code> Clean up temporary files after scan <code>False</code> <code>--use-existing</code> Use existing results file <code>False</code> <code>--phases</code> Phases to run: <code>convert</code>, <code>scan</code>, <code>report</code>, <code>inspect</code> <code>convert,scan,report</code> <code>--inspect</code> Enable inspection of SARIF fields <code>False</code>"},{"location":"docs/cli-reference/#examples","title":"Examples","text":"<pre><code># Basic scan in local mode (default)\nash\n\n# Scan with container mode\nash --mode container\n\n# Scan with specific source and output directories\nash --source-dir ./my-project --output-dir ./scan-results\n\n# Scan with configuration overrides\nash --config-overrides 'scanners.bandit.enabled=true' --config-overrides 'global_settings.severity_threshold=MEDIUM'\n\n# Scan with specific output formats\nash --output-formats flat-json,sarif,html,markdown\n\n# Scan in precommit mode (faster)\nash --mode precommit\n\n# Scan with custom plugins\nash --ash-plugin-modules my_custom_plugin_module\n</code></pre>"},{"location":"docs/cli-reference/#config-command","title":"Config Command","text":"<p>The <code>config</code> command allows you to manage ASH configuration.</p> <pre><code>ash config [subcommand] [options]\n</code></pre>"},{"location":"docs/cli-reference/#config-subcommands","title":"Config Subcommands","text":"Subcommand Description <code>init</code> Initialize a new configuration file <code>get</code> Display current configuration <code>update</code> Update configuration values <code>validate</code> Validate configuration file"},{"location":"docs/cli-reference/#config-options","title":"Config Options","text":"Option Description Default Environment Variable <code>--config</code>, <code>-c</code> Path to configuration file <code>.ash/.ash.yaml</code> <code>ASH_CONFIG</code> <code>--config-overrides</code> Override configuration values <code>--set</code> Set configuration values (with <code>update</code>) <code>--dry-run</code> Preview changes without writing (with <code>update</code>) <code>False</code> <code>--force</code> Overwrite existing config file (with <code>init</code>) <code>False</code> <code>--debug</code>, <code>-d</code> Enable debug logging <code>False</code> <code>ASH_DEBUG</code> <code>--verbose</code>, <code>-v</code> Enable verbose logging <code>False</code> <code>ASH_VERBOSE</code> <code>--no-color</code> Disable colored output <code>False</code> <code>ASH_NO_COLOR</code>"},{"location":"docs/cli-reference/#examples_1","title":"Examples","text":"<pre><code># Initialize a new configuration file\nash config init\n\n# Display current configuration\nash config get\n\n# Update configuration\nash config update --set 'scanners.bandit.enabled=true'\n\n# Validate configuration\nash config validate\n</code></pre>"},{"location":"docs/cli-reference/#plugin-command","title":"Plugin Command","text":"<p>The <code>plugin</code> command allows you to manage ASH plugins.</p> <pre><code>ash plugin [subcommand] [options]\n</code></pre>"},{"location":"docs/cli-reference/#plugin-subcommands","title":"Plugin Subcommands","text":"Subcommand Description <code>list</code> List available plugins"},{"location":"docs/cli-reference/#plugin-options","title":"Plugin Options","text":"Option Description Default Environment Variable <code>--include-plugin-config</code> Include plugin configuration in output <code>False</code> <code>--ash-plugin-modules</code> Additional plugin modules to load <code>ASH_PLUGIN_MODULES</code> <code>--config</code>, <code>-c</code> Path to configuration file <code>.ash/.ash.yaml</code> <code>ASH_CONFIG</code> <code>--config-overrides</code> Override configuration values <code>--debug</code>, <code>-d</code> Enable debug logging <code>False</code> <code>ASH_DEBUG</code> <code>--verbose</code>, <code>-v</code> Enable verbose logging <code>False</code> <code>ASH_VERBOSE</code> <code>--no-color</code> Disable colored output <code>False</code> <code>ASH_NO_COLOR</code>"},{"location":"docs/cli-reference/#examples_2","title":"Examples","text":"<pre><code># List all available plugins\nash plugin list\n\n# List plugins with their configuration\nash plugin list --include-plugin-config\n\n# List plugins including custom modules\nash plugin list --ash-plugin-modules my_custom_plugin_module\n</code></pre>"},{"location":"docs/cli-reference/#report-command","title":"Report Command","text":"<p>The <code>report</code> command generates reports from scan results.</p> <pre><code>ash report [options]\n</code></pre>"},{"location":"docs/cli-reference/#report-options","title":"Report Options","text":"Option Description Default Environment Variable <code>--format</code> Report format to generate <code>markdown</code> <code>--output-dir</code> Directory containing scan results <code>.ash/ash_output</code> <code>ASH_OUTPUT_DIR</code> <code>--config</code>, <code>-c</code> Path to configuration file <code>.ash/.ash.yaml</code> <code>ASH_CONFIG</code> <code>--config-overrides</code> Override configuration values <code>--log-level</code> Set the log level <code>INFO</code> <code>--debug</code>, <code>-d</code> Enable debug logging <code>False</code> <code>ASH_DEBUG</code> <code>--verbose</code>, <code>-v</code> Enable verbose logging <code>False</code> <code>ASH_VERBOSE</code> <code>--no-color</code> Disable colored output <code>False</code> <code>ASH_NO_COLOR</code>"},{"location":"docs/cli-reference/#examples_3","title":"Examples","text":"<pre><code># Generate a markdown report\nash report --format markdown\n\n# Generate a JSON report\nash report --format json\n\n# Generate a report from specific results\nash report --output-dir ./my-scan-results --format html\n</code></pre>"},{"location":"docs/cli-reference/#dependencies-command","title":"Dependencies Command","text":"<p>The <code>dependencies</code> command installs dependencies for ASH plugins.</p> <pre><code>ash dependencies install [options]\n</code></pre>"},{"location":"docs/cli-reference/#dependencies-options","title":"Dependencies Options","text":"Option Description Default Environment Variable <code>--bin-path</code>, <code>-b</code> Path to install binaries <code>~/.ash/bin</code> <code>ASH_BIN_PATH</code> <code>--plugin-type</code>, <code>-t</code> Plugin types to install dependencies for <code>converter,scanner,reporter</code> <code>--config</code>, <code>-c</code> Path to configuration file <code>.ash/.ash.yaml</code> <code>ASH_CONFIG</code> <code>--config-overrides</code> Override configuration values <code>--debug</code>, <code>-d</code> Enable debug logging <code>False</code> <code>ASH_DEBUG</code> <code>--verbose</code>, <code>-v</code> Enable verbose logging <code>False</code> <code>ASH_VERBOSE</code> <code>--no-color</code> Disable colored output <code>False</code> <code>ASH_NO_COLOR</code>"},{"location":"docs/cli-reference/#examples_4","title":"Examples","text":"<pre><code># Install dependencies for all plugin types\nash dependencies install\n\n# Install dependencies for scanners only\nash dependencies install --plugin-type scanner\n\n# Install dependencies to a custom directory\nash dependencies install --bin-path ~/tools/ash-bin\n</code></pre>"},{"location":"docs/cli-reference/#inspect-command","title":"Inspect Command","text":"<p>The <code>inspect</code> command allows you to analyze ASH outputs and reports.</p> <pre><code>ash inspect [subcommand] [options]\n</code></pre>"},{"location":"docs/cli-reference/#inspect-subcommands","title":"Inspect Subcommands","text":"Subcommand Description <code>sarif-fields</code> Analyze SARIF fields across different scanners <code>findings</code> Interactive TUI to explore findings"},{"location":"docs/cli-reference/#inspect-options","title":"Inspect Options","text":"Option Description Default Environment Variable <code>--output-dir</code> Directory containing scan results <code>.ash/ash_output</code> <code>ASH_OUTPUT_DIR</code> <code>--config</code>, <code>-c</code> Path to configuration file <code>.ash/.ash.yaml</code> <code>ASH_CONFIG</code> <code>--debug</code>, <code>-d</code> Enable debug logging <code>False</code> <code>ASH_DEBUG</code> <code>--verbose</code>, <code>-v</code> Enable verbose logging <code>False</code> <code>ASH_VERBOSE</code> <code>--no-color</code> Disable colored output <code>False</code> <code>ASH_NO_COLOR</code>"},{"location":"docs/cli-reference/#examples_5","title":"Examples","text":"<pre><code># Analyze SARIF fields\nash inspect sarif-fields\n\n# Explore findings interactively\nash inspect findings\n</code></pre>"},{"location":"docs/cli-reference/#build-image-command","title":"Build-Image Command","text":"<p>The <code>build-image</code> command builds the ASH container image.</p> <pre><code>ash build-image [options]\n</code></pre>"},{"location":"docs/cli-reference/#build-image-options","title":"Build-Image Options","text":"Option Description Default Environment Variable <code>--build-target</code> Container build target: <code>non-root</code> or <code>ci</code> <code>non-root</code> <code>--offline</code> Build for offline use <code>False</code> <code>--offline-semgrep-rulesets</code> Semgrep rulesets for offline mode <code>p/ci</code> <code>--oci-runner</code>, <code>-o</code> OCI runner to use <code>docker</code> <code>ASH_OCI_RUNNER</code> <code>--debug</code>, <code>-d</code> Enable debug logging <code>False</code> <code>ASH_DEBUG</code> <code>--verbose</code>, <code>-v</code> Enable verbose logging <code>False</code> <code>ASH_VERBOSE</code> <code>--no-color</code> Disable colored output <code>False</code> <code>ASH_NO_COLOR</code>"},{"location":"docs/cli-reference/#examples_6","title":"Examples","text":"<pre><code># Build the default image\nash build-image\n\n# Build for CI environments\nash build-image --build-target ci\n\n# Build for offline use\nash build-image --offline --offline-semgrep-rulesets p/ci\n\n# Build using a specific OCI runner\nash build-image --oci-runner podman\n</code></pre>"},{"location":"docs/cli-reference/#additional-environment-variables","title":"Additional Environment Variables","text":"<p>ASH supports additional environment variables that don't directly map to command-line parameters:</p> Variable Description Default <code>ASH_IMAGE_NAME</code> Name of ASH container image <code>automated-security-helper:latest</code> <code>ASH_CONTAINER_WORK_DIR</code> Working directory inside the container <code>/work</code> <code>ASH_CONTAINER_SOURCE_DIR</code> Source directory inside the container <code>/src</code> <code>ASH_CONTAINER_OUTPUT_DIR</code> Output directory inside the container <code>/out</code>"},{"location":"docs/cli-reference/#exit-codes","title":"Exit Codes","text":"<p>ASH returns the following exit codes:</p> Code Description 0 Success - No issues found 1 Scan execution error 2 Issues found with severity at or above threshold"},{"location":"docs/config-overrides/","title":"Configuration Overrides","text":"<p>ASH supports runtime configuration overrides through the <code>--config-overrides</code> CLI parameter. This allows you to modify configuration values without editing the configuration file.</p>"},{"location":"docs/config-overrides/#basic-usage","title":"Basic Usage","text":"<pre><code>ash --config-overrides 'reporters.markdown.options.include_detailed_findings=true'\n</code></pre> <p>You can specify multiple overrides by using the parameter multiple times:</p> <pre><code>ash \\\n  --config-overrides 'reporters.cloudwatch-logs.options.aws_region=us-west-2' \\\n  --config-overrides 'global_settings.severity_threshold=LOW'\n</code></pre>"},{"location":"docs/config-overrides/#supported-value-types","title":"Supported Value Types","text":"<p>The configuration override system automatically converts values to appropriate types:</p> <ul> <li>Strings: <code>'key=value'</code></li> <li>Numbers: <code>'key=123'</code> or <code>'key=3.14'</code></li> <li>Booleans: <code>'key=true'</code> or <code>'key=false'</code></li> <li>Null: <code>'key=null'</code> or <code>'key=none'</code></li> <li>Lists: <code>'key=[item1, item2, item3]'</code> or <code>'key=[\"item1\", \"item2\", \"item3\"]'</code></li> <li>Dictionaries: <code>'key={\"subkey1\": \"value1\", \"subkey2\": \"value2\"}'</code></li> </ul>"},{"location":"docs/config-overrides/#advanced-features","title":"Advanced Features","text":""},{"location":"docs/config-overrides/#list-append-mode","title":"List Append Mode","text":"<p>You can append to existing lists by adding a <code>+</code> at the end of the key path:</p> <pre><code># Add a new plugin module without replacing existing ones\nash --config-overrides 'ash_plugin_modules+=[\"my_custom_plugin_module\"]'\n</code></pre>"},{"location":"docs/config-overrides/#complex-structures","title":"Complex Structures","text":"<p>For complex structures, you can use JSON syntax:</p> <pre><code># Add a new ignore path\nash --config-overrides 'global_settings.ignore_paths+=[{\"path\": \"build/\", \"reason\": \"Generated files\"}]'\n</code></pre>"},{"location":"docs/config-overrides/#examples","title":"Examples","text":"<ol> <li> <p>Change severity threshold:    <pre><code>ash --config-overrides 'global_settings.severity_threshold=LOW'\n</code></pre></p> </li> <li> <p>Enable a specific scanner:    <pre><code>ash --config-overrides 'scanners.bandit.enabled=true'\n</code></pre></p> </li> <li> <p>Configure AWS region for CloudWatch Logs reporter:    <pre><code>ash --config-overrides 'reporters.cloudwatch-logs.options.aws_region=us-west-2'\n</code></pre></p> </li> <li> <p>Replace the list of plugin modules:    <pre><code>ash --config-overrides 'ash_plugin_modules=[\"automated_security_helper.plugin_modules.ash_aws_plugins\"]'\n</code></pre></p> </li> <li> <p>Add a plugin module to the existing list:    <pre><code>ash --config-overrides 'ash_plugin_modules+=[\"automated_security_helper.plugin_modules.custom_plugin\"]'\n</code></pre></p> </li> <li> <p>Configure multiple scanner options:    <pre><code>ash \\\n  --config-overrides 'scanners.bandit.options.confidence_level=HIGH' \\\n  --config-overrides 'scanners.bandit.options.ignore_nosec=true'\n</code></pre></p> </li> </ol>"},{"location":"docs/config-overrides/#configuration-management","title":"Configuration Management","text":"<p>ASH provides several ways to manage your configuration:</p>"},{"location":"docs/config-overrides/#configuration-file","title":"Configuration File","text":"<p>ASH uses YAML configuration files by default. The standard location is <code>.ash/.ash.yaml</code> in your project directory. You can also use JSON format with <code>.ash/.ash.json</code>.</p> <p>A basic configuration file looks like this:</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/awslabs/automated-security-helper/refs/heads/main/automated_security_helper/schemas/AshConfig.json\nproject_name: my-project\nglobal_settings:\n  severity_threshold: MEDIUM\n  ignore_paths:\n    - path: 'tests/test_data'\n      reason: 'Test data only'\nscanners:\n  bandit:\n    enabled: true\n    options:\n      confidence_level: HIGH\nreporters:\n  markdown:\n    enabled: true\n    options:\n      include_detailed_findings: true\n</code></pre>"},{"location":"docs/config-overrides/#json-schema-support","title":"JSON Schema Support","text":"<p>ASH provides a JSON schema for configuration files, which enables validation and auto-completion in compatible editors. Add this line at the top of your YAML file:</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/awslabs/automated-security-helper/refs/heads/main/automated_security_helper/schemas/AshConfig.json\n</code></pre>"},{"location":"docs/config-overrides/#configuration-commands","title":"Configuration Commands","text":"<p>ASH provides several commands to manage your configuration:</p>"},{"location":"docs/config-overrides/#initialize-a-configuration","title":"Initialize a Configuration","text":"<p>Create a new configuration file:</p> <pre><code>ash config init\n</code></pre> <p>This creates a default configuration file at <code>.ash/.ash.yaml</code>.</p>"},{"location":"docs/config-overrides/#view-current-configuration","title":"View Current Configuration","text":"<p>View the current configuration:</p> <pre><code>ash config get\n</code></pre> <p>You can also apply overrides when viewing the configuration:</p> <pre><code>ash config get --config-overrides 'scanners.bandit.enabled=false'\n</code></pre>"},{"location":"docs/config-overrides/#update-configuration","title":"Update Configuration","text":"<p>Update an existing configuration file:</p> <pre><code>ash config update --set 'scanners.bandit.enabled=false' --set 'global_settings.severity_threshold=LOW'\n</code></pre> <p>You can use the same syntax as <code>--config-overrides</code>, including list operations:</p> <pre><code># Add a new ignore path\nash config update --set 'global_settings.ignore_paths+=[{\"path\": \"build/\", \"reason\": \"Generated files\"}]'\n\n# Preview changes without writing to file\nash config update --set 'scanners.semgrep.enabled=false' --dry-run\n</code></pre>"},{"location":"docs/config-overrides/#validate-configuration","title":"Validate Configuration","text":"<p>Validate your configuration file:</p> <pre><code>ash config validate\n</code></pre> <p>You can also validate with overrides:</p> <pre><code>ash config validate --config-overrides 'scanners.bandit.options.confidence_level=HIGH'\n</code></pre>"},{"location":"docs/config-overrides/#custom-plugins","title":"Custom Plugins","text":"<p>You can add custom plugins to ASH by specifying them in the <code>ash_plugin_modules</code> list:</p> <pre><code>ash_plugin_modules:\n  - my_custom_plugin_module\n</code></pre> <p>Or using the override:</p> <pre><code>ash --config-overrides 'ash_plugin_modules+=[\"my_custom_plugin_module\"]'\n</code></pre>"},{"location":"docs/config-overrides/#notes","title":"Notes","text":"<ul> <li>Configuration overrides are applied after loading the configuration file</li> <li>Overrides work with both default and explicit configurations</li> <li>If validation fails after applying overrides, the original configuration will be used</li> <li>For complex values, use valid JSON syntax</li> <li>Environment variables can be referenced in YAML configuration files using <code>!ENV ${VAR_NAME:default_value}</code> syntax</li> </ul>"},{"location":"docs/configuration-guide/","title":"Configuration Guide","text":"<p>ASH v3 uses a YAML configuration file to control its behavior. This guide explains how to configure ASH for your project.</p>"},{"location":"docs/configuration-guide/#configuration-file-location","title":"Configuration File Location","text":"<p>By default, ASH looks for a configuration file in the following locations (in order):</p> <ol> <li><code>.ash/.ash.yaml</code></li> <li><code>.ash/.ash.yml</code></li> <li><code>.ash.yaml</code></li> <li><code>.ash.yml</code></li> </ol> <p>You can also specify a custom configuration file path using the <code>--config</code> option:</p> <pre><code>ash --config /path/to/my-config.yaml\n</code></pre>"},{"location":"docs/configuration-guide/#creating-a-configuration-file","title":"Creating a Configuration File","text":"<p>The easiest way to create a configuration file is to use the <code>config init</code> command:</p> <pre><code>ash config init\n</code></pre> <p>This creates a default configuration file at <code>.ash/.ash.yaml</code> with recommended settings.</p>"},{"location":"docs/configuration-guide/#configuration-structure","title":"Configuration Structure","text":"<p>The ASH configuration file has the following main sections:</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/awslabs/automated-security-helper/refs/heads/main/automated_security_helper/schemas/AshConfig.json\nproject_name: my-project\nglobal_settings:\n  severity_threshold: MEDIUM\n  ignore_paths: []\nconverters:\n  # Converter plugins configuration\nscanners:\n  # Scanner plugins configuration\nreporters:\n  # Reporter plugins configuration\nash_plugin_modules: []\n</code></pre>"},{"location":"docs/configuration-guide/#global-settings","title":"Global Settings","text":"<p>The <code>global_settings</code> section controls general behavior:</p> <pre><code>global_settings:\n  # Minimum severity level to consider findings actionable\n  # Options: CRITICAL, HIGH, MEDIUM, LOW, INFO\n  severity_threshold: MEDIUM\n\n  # Paths to ignore during scanning\n  ignore_paths:\n    - path: 'tests/test_data'\n      reason: 'Test data only'\n    - path: 'node_modules/'\n      reason: 'Third-party dependencies'\n\n  # Findings to suppress based on rule ID, file path, and line numbers\n  suppressions:\n    - rule_id: 'RULE-123'  # Scanner-specific rule ID\n      file_path: 'src/example.py'  # File path (supports glob patterns)\n      line_start: 10  # Optional starting line number\n      line_end: 15  # Optional ending line number\n      reason: 'False positive due to test mock'  # Reason for suppression\n      expiration: '2025-12-31'  # Optional expiration date (YYYY-MM-DD)\n    - rule_id: 'RULE-456'\n      file_path: 'src/*.js'  # Glob pattern matching all JS files in src/\n      reason: 'Known issue, planned for fix in v2.0'\n\n  # Whether to fail with non-zero exit code if actionable findings are found\n  fail_on_findings: true\n</code></pre>"},{"location":"docs/configuration-guide/#converters-configuration","title":"Converters Configuration","text":"<p>The <code>converters</code> section configures file converters that transform files before scanning:</p> <pre><code>converters:\n  jupyter:\n    enabled: true\n    options:\n      # Converter-specific options\n  archive:\n    enabled: true\n    options:\n      # Converter-specific options\n</code></pre>"},{"location":"docs/configuration-guide/#scanners-configuration","title":"Scanners Configuration","text":"<p>The <code>scanners</code> section configures security scanners:</p> <pre><code>scanners:\n  bandit:\n    enabled: true\n    options:\n      confidence_level: high\n      severity_level: medium\n\n  semgrep:\n    enabled: true\n    options:\n      rules: ['p/ci']\n      tool_version: null  # Version constraint (e.g., '&gt;=1.125.0')\n      install_timeout: 300  # Timeout in seconds for tool installation\n\n  detect-secrets:\n    enabled: true\n    options:\n      exclude_lines: []\n\n  checkov:\n    enabled: true\n    options:\n      framework: ['all']\n      tool_version: null  # Version constraint (e.g., '&gt;=3.2.0,&lt;4.0.0')\n      install_timeout: 300  # Timeout in seconds for tool installation\n\n  cfn-nag:\n    enabled: true\n    options:\n      profile_path: null\n\n  cdk-nag:\n    enabled: true\n    options:\n      nag_packs: ['AWS_SOLUTIONS']\n\n  npm-audit:\n    enabled: true\n    options:\n      audit_level: moderate\n\n  grype:\n    enabled: true\n    options:\n      severity: medium\n\n  syft:\n    enabled: true\n    options:\n      scope: squashed\n</code></pre>"},{"location":"docs/configuration-guide/#reporters-configuration","title":"Reporters Configuration","text":"<p>The <code>reporters</code> section configures output report formats:</p> <pre><code>reporters:\n  markdown:\n    enabled: true\n    options:\n      include_detailed_findings: true\n\n  html:\n    enabled: true\n    options:\n      include_detailed_findings: true\n\n  json:\n    enabled: true\n    options:\n      pretty_print: true\n\n  csv:\n    enabled: true\n    options:\n      include_all_fields: false\n\n  sarif:\n    enabled: true\n    options:\n      include_help_uri: true\n</code></pre>"},{"location":"docs/configuration-guide/#custom-plugin-modules","title":"Custom Plugin Modules","text":"<p>The <code>ash_plugin_modules</code> section allows you to specify custom Python modules containing ASH plugins:</p> <pre><code>ash_plugin_modules:\n  - my_custom_ash_plugins\n  - another_plugin_module\n</code></pre>"},{"location":"docs/configuration-guide/#validating-configuration","title":"Validating Configuration","text":"<p>To validate your configuration file:</p> <pre><code>ash config validate\n</code></pre>"},{"location":"docs/configuration-guide/#viewing-current-configuration","title":"Viewing Current Configuration","text":"<p>To view the current configuration:</p> <pre><code>ash config get\n</code></pre>"},{"location":"docs/configuration-guide/#updating-configuration","title":"Updating Configuration","text":"<p>To update configuration values:</p> <pre><code>ash config update --set 'scanners.bandit.enabled=true'\nash config update --set 'global_settings.severity_threshold=LOW'\n</code></pre>"},{"location":"docs/configuration-guide/#configuration-overrides","title":"Configuration Overrides","text":"<p>You can override configuration values at runtime using the <code>--config-overrides</code> option:</p> <pre><code># Enable a specific scanner\nash --config-overrides 'scanners.bandit.enabled=true'\n\n# Change severity threshold\nash --config-overrides 'global_settings.severity_threshold=LOW'\n\n# Append to a list\nash --config-overrides 'ash_plugin_modules+=[\"my_custom_plugin\"]'\n\n# Add a complex value\nash --config-overrides 'global_settings.ignore_paths+=[{\"path\": \"build/\", \"reason\": \"Generated files\"}]'\n</code></pre>"},{"location":"docs/configuration-guide/#scanner-specific-configuration","title":"Scanner-Specific Configuration","text":"<p>Each scanner has its own configuration options. Here are some examples:</p>"},{"location":"docs/configuration-guide/#bandit","title":"Bandit","text":"<pre><code>scanners:\n  bandit:\n    enabled: true\n    options:\n      confidence_level: HIGH  # Options: LOW, MEDIUM, HIGH\n      severity_level: medium  # Options: low, medium, high\n      skip_tests: []  # List of test IDs to skip\n      include_tests: []  # List of test IDs to include\n      # Note: Bandit is automatically installed via UV tool management\n      # with version constraint &gt;=1.7.0 for enhanced SARIF support\n</code></pre> <p>If you have been using Bandit separately and have an existing configuration file you would like to use with ASH, ASH can automatically discover and use it. ASH will automatically search your current directory and the <code>.ash</code> directory for a file named <code>.bandit</code>, <code>.bandit.toml</code>, or <code>.bandit.yaml</code>, and will use the settings found in the file if it is detected. For more details on using a Bandit configuration file, refer to the Bandit documentation.</p>"},{"location":"docs/configuration-guide/#semgrep","title":"Semgrep","text":"<pre><code>scanners:\n  semgrep:\n    enabled: true\n    options:\n      rules: ['p/ci']  # Rulesets to use\n      timeout: 300  # Timeout in seconds\n      max_memory: 0  # Max memory in MB (0 = no limit)\n      exclude_rules: []  # Rules to exclude\n      tool_version: null  # Version constraint (e.g., '&gt;=1.125.0')\n      install_timeout: 300  # Timeout in seconds for tool installation\n</code></pre>"},{"location":"docs/configuration-guide/#detect-secrets","title":"Detect-Secrets","text":"<pre><code>scanners:\n  detect-secrets:\n    enabled: true\n    options:\n      exclude_lines: []  # Lines to exclude\n      exclude_files: []  # Files to exclude\n      custom_plugins: []  # Custom plugins to use\n</code></pre> <p>If you have been using detect-secrets separately and have an existing baseline file you would like to use with ASH, ASH can automatically use it. ASH automatically searches your current directory and the <code>.ash</code> directory for a <code>.secrets.baseline</code> file. For more details on baseline files, refer to the detect-secrets documentation.</p>"},{"location":"docs/configuration-guide/#checkov","title":"Checkov","text":"<pre><code>scanners:\n  checkov:\n    enabled: true\n    options:\n      framework: ['all']  # Frameworks to scan\n      skip_frameworks: []  # Frameworks to exclude\n      offline: false  # Run in offline mode\n      additional_formats: ['cyclonedx_json']  # Additional output formats\n      tool_version: null  # Version constraint (e.g., '&gt;=3.2.0,&lt;4.0.0')\n      install_timeout: 300  # Timeout in seconds for tool installation\n      # Note: Checkov is automatically downloaded and run via UV tool management\n      # with version constraint &gt;=3.2.0,&lt;4.0.0 for enhanced stability\n</code></pre> <p>If you have been using Checkov separately and have an existing configuration file you would like to use with ASH, ASH can automatically discover and use it. ASH will automatically search your current directory and the <code>.ash</code> directory for a file named <code>.checkov.yml</code> or <code>.checkov.yaml</code>, and will use the settings found in the file if it is detected. For more details on using a bandit configuration file, refer to the Checkov documentation.</p>"},{"location":"docs/configuration-guide/#grype","title":"Grype","text":"<pre><code>scanners:\n  grype:\n    enabled: true\n    options:\n      config_file: .grype.yaml # Specific path to grype configuration file\n      severity_threshold: MEDIUM # Options: ALL, LOW, MEDIUM, HIGH, CRITICAL\n      offline: false # Run in offline mode\n</code></pre> <p>If you have been using Grype separately and have an existing configuration file you would like to use with ASH, ASH can automatically discover and use it. ASH will automatically search your current directory, the <code>.ash</code> directory, and the <code>.grype</code> directory for a file named <code>.grype.yaml</code>. The current directory will also be searched for a <code>grype.yaml</code> file. If any of these files are found, ASH will use the settings found in the file. For more details on using a Grype configuration file, refer to the Grype documentation.</p>"},{"location":"docs/configuration-guide/#syft","title":"Syft","text":"<pre><code>scanners:\n  syft:\n    enabled: true\n    options:\n      config_file: .grype.yaml # Specific path to grype configuration file\n      exclude: ['tests'] # List of files and directories to exclude from scans\n      additional_outputs: [\"syft-json\"] # List of additional output formats for Syft. Options: \n      # \"cyclonedx-json\", \"cyclonedx-xml\",\"github-json\", \"spdx-json\", \n      # \"spdx-tag-value\", \"syft-json\", \"syft-table\", \"syft-text\"\n</code></pre> <p>If you have been using Syft separately and have an existing configuration file you would like to use with ASH, ASH can automatically discover and use it. ASH will automatically search your current directory for a file named <code>.syft.yaml</code> or <code>.syft.yml</code>. If either of these files are found, ASH will use the settings found in the file. For more details on using a Syft configuration file, refer to the Syft documentation.</p>"},{"location":"docs/configuration-guide/#uv-tool-management","title":"UV Tool Management","text":"<p>ASH v3 uses UV's tool isolation system to automatically manage scanner dependencies. This provides several benefits:</p> <ul> <li>Automatic Installation: Tools like Bandit, Checkov, and Semgrep are automatically installed when needed</li> <li>Version Constraints: ASH ensures compatible tool versions with sensible defaults:</li> <li>Bandit: <code>&gt;=1.7.0</code> (enhanced SARIF support and security fixes)</li> <li>Checkov: <code>&gt;=3.2.0,&lt;4.0.0</code> (improved stability, avoiding potential breaking changes)</li> <li>Semgrep: <code>&gt;=1.125.0</code> (comprehensive rule support and performance improvements)</li> <li>Isolation: Tools run in isolated environments without affecting your project dependencies</li> <li>Retry Logic: Automatic retry with exponential backoff for network issues</li> <li>Comprehensive Logging: Detailed installation and execution logging for troubleshooting</li> <li>Fallback Support: If UV tool installation fails, ASH falls back to system-installed tools when available</li> </ul>"},{"location":"docs/configuration-guide/#uv-tool-configuration-options","title":"UV Tool Configuration Options","text":"<p>Each UV-managed scanner supports these configuration options:</p> <pre><code>scanners:\n  checkov:  # or bandit, semgrep\n    enabled: true\n    options:\n      tool_version: \"&gt;=3.2.0,&lt;4.0.0\"  # Override default version constraint\n      install_timeout: 300             # Installation timeout in seconds (default: 300)\n</code></pre>"},{"location":"docs/configuration-guide/#environment-variables","title":"Environment Variables","text":"<p>Control UV tool behavior globally:</p> <pre><code># Disable automatic tool installation (use pre-installed tools)\nexport ASH_OFFLINE=true\n\n# Custom UV executable path (if needed)\nexport UV_EXECUTABLE=/custom/path/to/uv\n</code></pre>"},{"location":"docs/configuration-guide/#troubleshooting-uv-tool-issues","title":"Troubleshooting UV Tool Issues","text":"<p>If you encounter UV tool installation issues:</p> <ol> <li>Check UV availability: <code>uv --version</code></li> <li>Enable verbose logging: <code>ash --verbose</code> for detailed installation logs</li> <li>Use offline mode: <code>ASH_OFFLINE=true</code> to skip installations</li> <li>Pre-install tools manually:    <pre><code>uv tool install bandit&gt;=1.7.0\nuv tool install checkov&gt;=3.2.0,&lt;4.0.0\nuv tool install semgrep&gt;=1.125.0\n</code></pre></li> <li>Increase timeout for slow networks:    <pre><code>scanners:\n  checkov:\n    options:\n      install_timeout: 600  # 10 minutes\n</code></pre></li> </ol> <p>For more detailed information about UV tool management, see the UV Tool Management Developer Guide. - Flexible Version Management: Scanners can optionally specify version constraints, with sensible defaults provided</p>"},{"location":"docs/configuration-guide/#uv-tool-behavior","title":"UV Tool Behavior","text":"<ul> <li>Bandit: Automatically installed via <code>uv tool install bandit&gt;=1.7.0</code> (default version constraint)</li> <li>Checkov: Automatically installed via <code>uv tool install checkov&gt;=3.2.0,&lt;4.0.0</code> (default version constraint) with fallback to <code>uv tool run</code></li> <li>Semgrep: Automatically installed via <code>uv tool install semgrep&gt;=1.125.0</code> (default version constraint) with fallback to <code>uv tool run</code></li> </ul>"},{"location":"docs/configuration-guide/#version-constraint-configuration","title":"Version Constraint Configuration","text":"<p>Each UV-managed scanner can specify version constraints in two ways:</p> <ol> <li>Default Constraints: Built-in version constraints ensure compatibility and stability</li> <li>Custom Constraints: Override defaults via configuration options (where supported)</li> </ol> <p>For scanners that support custom version constraints (like Semgrep and Checkov), you can specify them in your configuration:</p> <pre><code>scanners:\n  semgrep:\n    options:\n      tool_version: \"&gt;=1.130.0,&lt;2.0.0\"  # Custom version constraint\n  checkov:\n    options:\n      tool_version: \"&gt;=3.3.0\"  # Custom version constraint\n</code></pre>"},{"location":"docs/configuration-guide/#troubleshooting-uv-tool-issues_1","title":"Troubleshooting UV Tool Issues","text":"<p>If you encounter issues with UV tool management:</p> <ol> <li>Check UV Installation: Ensure UV is installed and available in your PATH</li> <li>Network Connectivity: UV tool installation requires internet access</li> <li>Offline Mode: Use <code>ASH_OFFLINE=true</code> to skip tool downloads and rely on pre-installed tools</li> <li>Manual Installation: You can pre-install tools manually if needed:    <pre><code>uv tool install bandit&gt;=1.7.0\nuv tool install checkov&gt;=3.2.0,&lt;4.0.0\nuv tool install semgrep&gt;=1.125.0\n</code></pre></li> </ol>"},{"location":"docs/configuration-guide/#advanced-configuration","title":"Advanced Configuration","text":"<p>For advanced configuration options, refer to the JSON Schema that defines all available configuration options.</p> <p>You can add this schema reference to your configuration file for editor autocompletion:</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/awslabs/automated-security-helper/refs/heads/main/automated_security_helper/schemas/AshConfig.json\n</code></pre>"},{"location":"docs/installation-guide/","title":"Installation Guide","text":"<p>ASH v3 offers multiple installation methods to fit your workflow. Choose the option that works best for your environment.</p>"},{"location":"docs/installation-guide/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/installation-guide/#for-local-mode","title":"For Local Mode","text":"<ul> <li>Python 3.10 or later</li> <li>UV package manager (automatically installed with ASH)</li> </ul> <p>ASH v3 uses UV's tool isolation system to automatically manage most scanner dependencies (Bandit, Checkov, Semgrep). For full scanner coverage in local mode, the following additional non-Python tools are recommended: - Ruby with cfn-nag (<code>gem install cfn-nag</code>) - Node.js/npm (for npm audit support) - Grype and Syft (for SBOM and vulnerability scanning)</p> <p>Note: Tools like Bandit, Checkov, and Semgrep are automatically installed via UV tool management when needed, so you don't need to install them manually. ASH uses sensible default version constraints with the flexibility to override through configuration.</p>"},{"location":"docs/installation-guide/#for-container-mode","title":"For Container Mode","text":"<ul> <li>Any OCI-compatible container runtime (Docker, Podman, Finch, etc.)</li> <li>On Windows: WSL2 is typically required for running Linux containers</li> </ul>"},{"location":"docs/installation-guide/#installation-options","title":"Installation Options","text":""},{"location":"docs/installation-guide/#standard-installation","title":"Standard Installation","text":""},{"location":"docs/installation-guide/#1-using-uvx-recommended","title":"1. Using <code>uvx</code> (Recommended)","text":"<p><code>uvx</code> is a fast Python package installer and resolver that allows you to run packages directly without installing them permanently.</p>"},{"location":"docs/installation-guide/#linuxmacos","title":"Linux/macOS","text":"<pre><code># Install uv if you don't have it\ncurl -sSf https://astral.sh/uv/install.sh | sh\n\n# Create an alias for ASH\nalias ash=\"uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\"\n\n# Use as normal\nash --help\n</code></pre>"},{"location":"docs/installation-guide/#windows","title":"Windows","text":"<pre><code># Install uv if you don't have it\nirm https://astral.sh/uv/install.ps1 | iex\n\n# Create a function for ASH\nfunction ash { uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2 $args }\n\n# Use as normal\nash --help\n</code></pre>"},{"location":"docs/installation-guide/#2-using-pipx","title":"2. Using <code>pipx</code>","text":"<p><code>pipx</code> installs packages in isolated environments and makes their entry points available globally.</p> <pre><code># Works on Windows, macOS, and Linux\npipx install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n\n# Use as normal\nash --help\n</code></pre>"},{"location":"docs/installation-guide/#3-using-pip","title":"3. Using <code>pip</code>","text":"<p>Standard Python package installation:</p> <pre><code># Works on Windows, macOS, and Linux\npip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n\n# Use as normal\nash --help\n</code></pre>"},{"location":"docs/installation-guide/#4-clone-the-repository","title":"4. Clone the Repository","text":"<p>For development or if you want to modify ASH:</p> <pre><code># Works on Windows, macOS, and Linux\ngit clone https://github.com/awslabs/automated-security-helper.git --branch v3.2.2\ncd automated-security-helper\npip install .\n\n# Use as normal\nash --help\n</code></pre>"},{"location":"docs/installation-guide/#mcp-support","title":"MCP Support","text":"<p>ASH v3 includes built-in Model Context Protocol (MCP) support for AI integration. No additional installation steps are required - MCP dependencies are included as core dependencies.</p> <p>After installing ASH with any of the methods above, you can immediately use MCP features:</p> <pre><code># Start the MCP server\nash mcp\n\n# Verify MCP support\nash mcp --help\n</code></pre>"},{"location":"docs/installation-guide/#windows-specific-installation-notes","title":"Windows-Specific Installation Notes","text":"<p>ASH v3 provides the same experience on Windows as on other platforms:</p> <ul> <li>For local mode, ASH runs natively on Windows with Python 3.10+</li> <li>For container mode, you'll need:</li> <li>Windows Subsystem for Linux (WSL2) installed</li> <li>A container runtime like Docker Desktop, Rancher Desktop, or Podman Desktop with WSL2 integration enabled</li> </ul>"},{"location":"docs/installation-guide/#verifying-your-installation","title":"Verifying Your Installation","text":"<p>After installation, verify that ASH is working correctly:</p> <pre><code># Check the version\nash --version\n\n# Run a simple scan in local mode\nash --mode local\n</code></pre>"},{"location":"docs/installation-guide/#upgrading-ash","title":"Upgrading ASH","text":"<p>To upgrade ASH to the latest version:</p>"},{"location":"docs/installation-guide/#if-installed-with-uvx","title":"If installed with <code>uvx</code>","text":"<pre><code># Your alias will use the latest version when specified\nalias ash=\"uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\"\n</code></pre>"},{"location":"docs/installation-guide/#if-installed-with-pipx","title":"If installed with <code>pipx</code>","text":"<pre><code>pipx upgrade automated-security-helper\n</code></pre>"},{"location":"docs/installation-guide/#if-installed-with-pip","title":"If installed with <code>pip</code>","text":"<pre><code>pip install --upgrade git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n</code></pre>"},{"location":"docs/installation-guide/#if-installed-from-repository","title":"If installed from repository","text":"<pre><code>cd automated-security-helper\ngit pull\npip install .\n</code></pre>"},{"location":"docs/installation-guide/#next-steps","title":"Next Steps","text":"<p>After installation, you can:</p> <ol> <li>Configure ASH for your project</li> <li>Run your first scan</li> <li>Learn about ASH's CLI options</li> </ol>"},{"location":"docs/mcp-performance-scalability/","title":"MCP Server Performance and Scalability","text":"<p>This guide covers performance optimization and scalability considerations for the ASH MCP server.</p>"},{"location":"docs/mcp-performance-scalability/#overview","title":"Overview","text":"<p>The ASH MCP server in v3 includes comprehensive resource management designed to handle concurrent operations efficiently while preventing memory leaks and resource exhaustion. This guide helps you optimize performance for your specific use case.</p>"},{"location":"docs/mcp-performance-scalability/#performance-features","title":"Performance Features","text":""},{"location":"docs/mcp-performance-scalability/#resource-management","title":"Resource Management","text":"<ul> <li>Concurrent Operation Control: Configurable limits on simultaneous scans and tasks</li> <li>Memory Leak Prevention: Automatic tracking and cleanup of async tasks and event handlers</li> <li>Shared Resource Pool: Efficient thread pool management across operations</li> <li>Graceful Degradation: Automatic protection against resource exhaustion</li> </ul>"},{"location":"docs/mcp-performance-scalability/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>Real-time Monitoring: Built-in health checks and resource usage tracking</li> <li>Performance Metrics: Task counts, memory usage, and operation timings</li> <li>Alerting Thresholds: Configurable warnings for resource usage</li> <li>Detailed Logging: Optional verbose logging for performance analysis</li> </ul>"},{"location":"docs/mcp-performance-scalability/#configuration-for-performance","title":"Configuration for Performance","text":""},{"location":"docs/mcp-performance-scalability/#basic-performance-configuration","title":"Basic Performance Configuration","text":"<pre><code># .ash/ash.yaml - Optimized for performance\nmcp-resource-management:\n  # Concurrent operations - adjust based on system resources\n  max_concurrent_scans: 5              # Higher for powerful systems\n  max_concurrent_tasks: 30             # Increase for complex scans\n  thread_pool_max_workers: 8           # Match CPU core count\n\n  # Timeouts - balance speed vs completeness\n  scan_timeout_seconds: 2400           # 40 minutes for large projects\n  operation_timeout_seconds: 300       # 5 minutes for operations\n\n  # Resource monitoring\n  enable_health_checks: true\n  health_check_interval_seconds: 30    # Frequent monitoring\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#high-performance-configuration","title":"High-Performance Configuration","text":"<p>For powerful systems with ample resources:</p> <pre><code># .ash/ash.yaml - High-performance setup\nmcp-resource-management:\n  # Maximize concurrent operations\n  max_concurrent_scans: 8\n  max_concurrent_tasks: 50\n  thread_pool_max_workers: 12\n\n  # Extended timeouts for large projects\n  scan_timeout_seconds: 3600           # 1 hour\n  operation_timeout_seconds: 600       # 10 minutes\n\n  # Higher resource thresholds\n  memory_warning_threshold_mb: 4096    # 4GB warning\n  memory_critical_threshold_mb: 8192   # 8GB critical\n  task_count_warning_threshold: 40\n\n  # Larger message limits\n  max_message_size_bytes: 52428800     # 50MB\n  max_directory_size_mb: 5120          # 5GB\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#resource-constrained-configuration","title":"Resource-Constrained Configuration","text":"<p>For systems with limited resources:</p> <pre><code># .ash/ash.yaml - Resource-constrained setup\nmcp-resource-management:\n  # Conservative limits\n  max_concurrent_scans: 2\n  max_concurrent_tasks: 10\n  thread_pool_max_workers: 2\n\n  # Shorter timeouts\n  scan_timeout_seconds: 1200           # 20 minutes\n  operation_timeout_seconds: 180       # 3 minutes\n\n  # Lower resource thresholds\n  memory_warning_threshold_mb: 512     # 512MB warning\n  memory_critical_threshold_mb: 1024   # 1GB critical\n  task_count_warning_threshold: 8\n\n  # Smaller limits\n  max_directory_size_mb: 500           # 500MB\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#scalability-patterns","title":"Scalability Patterns","text":""},{"location":"docs/mcp-performance-scalability/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>For organizations with multiple teams or projects:</p> <ol> <li>Multiple MCP Server Instances</li> <li>Run separate MCP servers for different teams</li> <li>Use different configuration files for different use cases</li> <li> <p>Isolate resource usage per team/project</p> </li> <li> <p>Load Distribution</p> </li> <li>Distribute large scans across multiple instances</li> <li>Use different severity thresholds for different environments</li> <li>Implement scan scheduling to avoid peak times</li> </ol>"},{"location":"docs/mcp-performance-scalability/#vertical-scaling","title":"Vertical Scaling","text":"<p>For single instances handling high load:</p> <ol> <li> <p>Resource Optimization <pre><code># Scale up resources\nmcp-resource-management:\n  max_concurrent_scans: 10\n  thread_pool_max_workers: 16      # Match available CPU cores\n  memory_warning_threshold_mb: 8192 # Scale with available RAM\n</code></pre></p> </li> <li> <p>Performance Tuning</p> </li> <li>Enable health checks for monitoring</li> <li>Use detailed logging to identify bottlenecks</li> <li>Optimize scanner selection for speed vs coverage</li> </ol>"},{"location":"docs/mcp-performance-scalability/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"docs/mcp-performance-scalability/#built-in-monitoring","title":"Built-in Monitoring","text":"<p>The MCP server provides several monitoring capabilities:</p> <pre><code># Enable comprehensive monitoring\nmcp-resource-management:\n  enable_health_checks: true\n  enable_resource_logging: true\n  log_resource_operations: true        # Enable for troubleshooting only\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#monitoring-queries","title":"Monitoring Queries","text":"<p>Ask your AI assistant for performance information:</p> <pre><code>\"Show me the current resource usage of the MCP server\"\n\"How many scans are currently running and what's their status?\"\n\"What's the memory usage trend over the last hour?\"\n\"Are there any performance bottlenecks or warnings?\"\n\"Show me the task count and thread pool utilization\"\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#performance-metrics","title":"Performance Metrics","text":"<p>The MCP server tracks these key metrics:</p> <ul> <li>Active Scan Count: Number of currently running scans</li> <li>Task Count: Number of active async tasks</li> <li>Memory Usage: Current memory consumption</li> <li>Thread Pool Utilization: Active vs available threads</li> <li>Operation Timings: Average scan and operation durations</li> <li>Error Rates: Failed operations and their causes</li> </ul>"},{"location":"docs/mcp-performance-scalability/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"docs/mcp-performance-scalability/#1-scanner-selection","title":"1. Scanner Selection","text":"<p>Optimize which scanners run based on your needs:</p> <pre><code># Fast scanning for development\nscanners:\n  bandit:\n    enabled: true      # Fast Python scanner\n  detect-secrets:\n    enabled: true      # Fast secrets detection\n  semgrep:\n    enabled: false     # Disable slower scanners for speed\n  checkov:\n    enabled: false\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#2-directory-optimization","title":"2. Directory Optimization","text":"<p>Exclude unnecessary directories to improve performance:</p> <pre><code>global_settings:\n  ignore_paths:\n    - path: \"node_modules/**\"\n      reason: \"Large dependency directory\"\n    - path: \"build/**\"\n      reason: \"Build artifacts\"\n    - path: \"dist/**\"\n      reason: \"Distribution files\"\n    - path: \"*.log\"\n      reason: \"Log files\"\n    - path: \"test_data/**\"\n      reason: \"Test data files\"\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#3-severity-filtering","title":"3. Severity Filtering","text":"<p>Use appropriate severity thresholds:</p> <pre><code># Development environment - catch everything\nglobal_settings:\n  severity_threshold: \"LOW\"\n\n# Production environment - focus on critical issues\nglobal_settings:\n  severity_threshold: \"HIGH\"\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#4-concurrent-operation-tuning","title":"4. Concurrent Operation Tuning","text":"<p>Balance concurrency with system resources:</p> <pre><code># Check system resources\nnproc                    # Number of CPU cores\nfree -h                  # Available memory\ndf -h                    # Disk space\n\n# Configure based on resources\n# Rule of thumb: max_concurrent_scans = CPU cores / 2\n# thread_pool_max_workers = CPU cores\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"docs/mcp-performance-scalability/#typical-performance-characteristics","title":"Typical Performance Characteristics","text":"Project Size Scan Time Memory Usage Recommended Config Small (&lt; 100 files) 1-3 minutes 200-500 MB Default settings Medium (100-1000 files) 3-10 minutes 500 MB - 1 GB Increase timeouts Large (1000-10000 files) 10-30 minutes 1-2 GB Increase all limits Very Large (&gt; 10000 files) 30+ minutes 2+ GB High-performance config"},{"location":"docs/mcp-performance-scalability/#performance-testing","title":"Performance Testing","text":"<p>Test your configuration with representative projects:</p> <pre><code># Test with debug logging\nuvx --from=git+https://github.com/awslabs/automated-security-helper@v3.0.0 ash mcp --debug\n\n# Monitor system resources during scans\ntop -p $(pgrep -f \"ash mcp\")\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#troubleshooting-performance-issues","title":"Troubleshooting Performance Issues","text":""},{"location":"docs/mcp-performance-scalability/#common-performance-problems","title":"Common Performance Problems","text":"<ol> <li>Slow Scan Performance</li> <li>Check concurrent scan limits</li> <li>Verify thread pool size matches CPU cores</li> <li>Review ignored paths configuration</li> <li> <p>Consider scanner selection optimization</p> </li> <li> <p>High Memory Usage</p> </li> <li>Reduce concurrent operations</li> <li>Check for memory leaks (should be resolved in v3)</li> <li>Increase memory thresholds if system has capacity</li> <li> <p>Monitor for stuck scans</p> </li> <li> <p>Resource Exhaustion</p> </li> <li>Review and adjust resource limits</li> <li>Enable health checks for early warning</li> <li>Implement scan scheduling</li> <li>Consider horizontal scaling</li> </ol>"},{"location":"docs/mcp-performance-scalability/#performance-debugging","title":"Performance Debugging","text":"<p>Enable detailed logging for performance analysis:</p> <pre><code>mcp-resource-management:\n  enable_resource_logging: true\n  log_resource_operations: true\n</code></pre> <p>This will log: - Resource allocation and deallocation - Task creation and completion times - Memory usage patterns - Thread pool utilization - Operation timings</p>"},{"location":"docs/mcp-performance-scalability/#best-practices","title":"Best Practices","text":""},{"location":"docs/mcp-performance-scalability/#1-configuration-management","title":"1. Configuration Management","text":"<ul> <li>Start with default settings and adjust based on usage</li> <li>Monitor performance metrics regularly</li> <li>Document configuration changes and their impact</li> <li>Test configuration changes in non-production environments</li> </ul>"},{"location":"docs/mcp-performance-scalability/#2-resource-planning","title":"2. Resource Planning","text":"<ul> <li>Plan resource allocation based on expected usage patterns</li> <li>Consider peak usage times and concurrent users</li> <li>Monitor trends to predict scaling needs</li> <li>Implement alerting for resource thresholds</li> </ul>"},{"location":"docs/mcp-performance-scalability/#3-operational-excellence","title":"3. Operational Excellence","text":"<ul> <li>Implement regular health checks</li> <li>Monitor performance metrics over time</li> <li>Plan for capacity growth</li> <li>Document performance baselines and targets</li> </ul>"},{"location":"docs/mcp-performance-scalability/#4-security-considerations","title":"4. Security Considerations","text":"<ul> <li>Balance performance with security coverage</li> <li>Don't disable critical scanners for performance</li> <li>Consider using different configurations for different environments</li> <li>Regularly review and update scanner configurations</li> </ul>"},{"location":"docs/mcp-performance-scalability/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"docs/mcp-performance-scalability/#custom-resource-limits","title":"Custom Resource Limits","text":"<p>For specialized environments:</p> <pre><code>mcp-resource-management:\n  # Custom limits based on specific requirements\n  max_concurrent_scans: 12             # High-throughput environment\n  max_concurrent_tasks: 100            # Complex scanning workflows\n  thread_pool_max_workers: 20          # High-core-count systems\n\n  # Extended timeouts for specialized scans\n  scan_timeout_seconds: 7200           # 2 hours for very large projects\n  operation_timeout_seconds: 1200      # 20 minutes for complex operations\n\n  # Specialized monitoring\n  health_check_interval_seconds: 15    # Frequent health checks\n  memory_warning_threshold_mb: 16384   # 16GB systems\n  memory_critical_threshold_mb: 32768  # 32GB systems\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#environment-specific-configurations","title":"Environment-Specific Configurations","text":"<pre><code># Development environment\nmcp-resource-management:\n  max_concurrent_scans: 3\n  scan_timeout_seconds: 1200           # Shorter timeouts for faster feedback\n  enable_resource_logging: true        # Detailed logging for debugging\n\n# Production environment\nmcp-resource-management:\n  max_concurrent_scans: 8\n  scan_timeout_seconds: 3600           # Longer timeouts for completeness\n  enable_health_checks: true           # Health monitoring\n  enable_resource_logging: false       # Reduce log volume\n</code></pre>"},{"location":"docs/mcp-performance-scalability/#related-documentation","title":"Related Documentation","text":"<ul> <li>MCP Tutorial</li> <li>Configuration Guide</li> <li>ASH CLI Reference</li> </ul>"},{"location":"docs/mcp-server-guide/","title":"ASH MCP Server Guide","text":"<p>This guide provides comprehensive documentation for the ASH MCP server implementation, which uses a file-based approach to track scan progress and completion.</p>"},{"location":"docs/mcp-server-guide/#overview","title":"Overview","text":"<p>The Automated Security Helper (ASH) MCP server provides a reliable interface for AI assistants to perform security scans on your codebase. The server implements the Model Context Protocol (MCP) to enable seamless integration with AI assistants.</p> <p>The MCP server uses a file-based approach to track scan progress and completion, making it more reliable than event-based tracking. This approach ensures that scan progress and results are accurately tracked even in complex threading scenarios.</p>"},{"location":"docs/mcp-server-guide/#architecture","title":"Architecture","text":"<p>The ASH MCP server architecture consists of the following components:</p> <ol> <li>MCP Server: The main server component that registers and exposes MCP tools</li> <li>Scan Registry: A thread-safe registry that tracks active scans</li> <li>File-Based Tracking: A system that uses file existence to track scan progress</li> <li>Error Handling: Comprehensive error handling for various scenarios</li> </ol>"},{"location":"docs/mcp-server-guide/#file-based-tracking-approach","title":"File-Based Tracking Approach","text":"<p>The file-based tracking approach works as follows:</p> <ol> <li>When a scan is started, a unique scan ID is generated and registered in the scan registry</li> <li>The scan process is started asynchronously</li> <li>The scan progress is tracked by checking for the existence of result files:</li> <li><code>ash_aggregated_results.json</code>: Indicates the scan has completed</li> <li>Individual scanner result files: Indicate which scanners have completed</li> <li>The scan results are retrieved by parsing these files</li> </ol> <p>This approach is more reliable than event-based tracking because it doesn't depend on in-memory state that could be lost due to threading issues.</p>"},{"location":"docs/mcp-server-guide/#file-structure","title":"File Structure","text":"<p>The file structure used for tracking is as follows:</p> <pre><code>.ash/ash_output/\n\u251c\u2500\u2500 ash_aggregated_results.json       # Complete scan results\n\u251c\u2500\u2500 scanners/\n\u2502   \u251c\u2500\u2500 bandit/\n\u2502   \u2502   \u251c\u2500\u2500 source/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ASH.ScanResults.json  # Bandit scanner results\n\u2502   \u251c\u2500\u2500 detect-secrets/\n\u2502   \u2502   \u251c\u2500\u2500 source/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ASH.ScanResults.json  # Detect-secrets scanner results\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 reports/\n    \u251c\u2500\u2500 ash_report.csv                # CSV report\n    \u251c\u2500\u2500 ash_report.html               # HTML report\n    \u2514\u2500\u2500 ash_report.md                 # Markdown report\n</code></pre>"},{"location":"docs/mcp-server-guide/#mcp-tools","title":"MCP Tools","text":"<p>The ASH MCP server provides the following tools:</p>"},{"location":"docs/mcp-server-guide/#scan_directory","title":"scan_directory","text":"<p>Starts a security scan asynchronously and returns a scan ID for tracking.</p> <p>Parameters: - <code>directory_path</code> (required): Path to the directory to scan - <code>severity_threshold</code> (optional): Minimum severity threshold (LOW, MEDIUM, HIGH, CRITICAL) - <code>config_path</code> (optional): Path to ASH configuration file</p> <p>Returns: A dictionary with scan ID and status information.</p> <p>Example: <pre><code>result = await mcp_scan_directory(\n    directory_path=\"/path/to/your/code\",\n    severity_threshold=\"MEDIUM\",\n    config_path=None\n)\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"success\": true,\n  \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"pending\",\n  \"directory_path\": \"/path/to/your/code\",\n  \"output_directory\": \"/path/to/your/code/.ash/ash_output\",\n  \"severity_threshold\": \"MEDIUM\",\n  \"config_path\": null,\n  \"start_time\": \"2025-07-16T12:34:56.789012\",\n  \"message\": \"Scan started successfully. Use get_scan_progress to track progress.\"\n}\n</code></pre></p>"},{"location":"docs/mcp-server-guide/#get_scan_progress","title":"get_scan_progress","text":"<p>Gets the current progress of a running scan using file-based tracking.</p> <p>Parameters: - <code>scan_id</code> (required): The scan ID returned from scan_directory</p> <p>Returns: A dictionary with scan progress information.</p> <p>Example: <pre><code>progress = await mcp_get_scan_progress(\n    scan_id=\"550e8400-e29b-41d4-a716-446655440000\"\n)\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"success\": true,\n  \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"running\",\n  \"completed_scanners\": 2,\n  \"total_scanners\": 5,\n  \"scanners\": {\n    \"bandit\": {\n      \"status\": \"completed\",\n      \"target_type\": \"source\",\n      \"finding_count\": 3,\n      \"severity_counts\": {\n        \"LOW\": 1,\n        \"MEDIUM\": 2,\n        \"HIGH\": 0,\n        \"CRITICAL\": 0\n      }\n    },\n    \"detect-secrets\": {\n      \"status\": \"completed\",\n      \"target_type\": \"source\",\n      \"finding_count\": 0,\n      \"severity_counts\": {}\n    },\n    \"semgrep\": {\n      \"status\": \"running\",\n      \"target_type\": \"source\"\n    },\n    \"checkov\": {\n      \"status\": \"pending\",\n      \"target_type\": \"source\"\n    },\n    \"cdk-nag\": {\n      \"status\": \"pending\",\n      \"target_type\": \"source\"\n    }\n  },\n  \"start_time\": \"2025-07-16T12:34:56.789012\",\n  \"current_time\": \"2025-07-16T12:36:23.456789\",\n  \"duration\": 86.667777,\n  \"timestamp\": \"2025-07-16T12:36:23.456789\"\n}\n</code></pre></p>"},{"location":"docs/mcp-server-guide/#get_scan_results","title":"get_scan_results","text":"<p>Gets the final results of a completed scan using file-based tracking.</p> <p>Parameters: - <code>scan_id</code> (required): The scan ID returned from scan_directory</p> <p>Returns: A dictionary with scan results information.</p> <p>Example: <pre><code>results = await mcp_get_scan_results(\n    scan_id=\"550e8400-e29b-41d4-a716-446655440000\"\n)\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"success\": true,\n  \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"completed\",\n  \"finding_count\": 5,\n  \"severity_counts\": {\n    \"LOW\": 1,\n    \"MEDIUM\": 3,\n    \"HIGH\": 1,\n    \"CRITICAL\": 0\n  },\n  \"findings\": [\n    {\n      \"id\": \"finding-1\",\n      \"title\": \"Hardcoded password\",\n      \"severity\": \"HIGH\",\n      \"scanner\": \"detect-secrets\",\n      \"file_path\": \"config.py\",\n      \"line_number\": 42,\n      \"description\": \"Hardcoded password found in configuration file\"\n    },\n    // Additional findings...\n  ],\n  \"scan_duration\": 124.567890,\n  \"start_time\": \"2025-07-16T12:34:56.789012\",\n  \"end_time\": \"2025-07-16T12:37:01.356902\",\n  \"timestamp\": \"2025-07-16T12:37:01.356902\"\n}\n</code></pre></p>"},{"location":"docs/mcp-server-guide/#list_active_scans","title":"list_active_scans","text":"<p>Lists all active and recent scans with their current status.</p> <p>Parameters: None</p> <p>Returns: A dictionary with information about all scans in the registry.</p> <p>Example: <pre><code>scans = await mcp_list_active_scans()\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"success\": true,\n  \"active_scans\": [\n    {\n      \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"status\": \"running\",\n      \"directory_path\": \"/path/to/your/code\",\n      \"start_time\": \"2025-07-16T12:34:56.789012\"\n    }\n  ],\n  \"all_scans\": [\n    {\n      \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"status\": \"running\",\n      \"directory_path\": \"/path/to/your/code\",\n      \"start_time\": \"2025-07-16T12:34:56.789012\"\n    },\n    {\n      \"scan_id\": \"660e8400-e29b-41d4-a716-446655440000\",\n      \"status\": \"completed\",\n      \"directory_path\": \"/path/to/another/code\",\n      \"start_time\": \"2025-07-16T11:22:33.444555\",\n      \"end_time\": \"2025-07-16T11:25:12.345678\"\n    }\n  ],\n  \"stats\": {\n    \"total_scans\": 2,\n    \"active_scans\": 1,\n    \"status_counts\": {\n      \"running\": 1,\n      \"completed\": 1,\n      \"failed\": 0,\n      \"cancelled\": 0\n    }\n  },\n  \"timestamp\": \"2025-07-16T12:36:23.456789\"\n}\n</code></pre></p>"},{"location":"docs/mcp-server-guide/#cancel_scan","title":"cancel_scan","text":"<p>Cancels a running scan and cleans up its resources.</p> <p>Parameters: - <code>scan_id</code> (required): The scan ID to cancel</p> <p>Returns: A dictionary with cancellation result information.</p> <p>Example: <pre><code>result = await mcp_cancel_scan(\n    scan_id=\"550e8400-e29b-41d4-a716-446655440000\"\n)\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"success\": true,\n  \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"cancelled\",\n  \"message\": \"Scan cancelled successfully\",\n  \"timestamp\": \"2025-07-16T12:36:45.123456\"\n}\n</code></pre></p>"},{"location":"docs/mcp-server-guide/#check_installation","title":"check_installation","text":"<p>Checks if ASH is properly installed and ready to use.</p> <p>Parameters: None</p> <p>Returns: A dictionary with installation status information.</p> <p>Example: <pre><code>result = await mcp_check_installation()\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"success\": true,\n  \"installed\": true,\n  \"version\": \"3.0.0\",\n  \"ash_command_available\": true,\n  \"ash_command_output\": \"ASH v3.0.0\",\n  \"ash_dir_exists\": true,\n  \"timestamp\": \"2025-07-16T12:37:12.345678\"\n}\n</code></pre></p>"},{"location":"docs/mcp-server-guide/#error-handling","title":"Error Handling","text":"<p>The ASH MCP server provides comprehensive error handling for various scenarios. Each error response includes:</p> <ul> <li><code>success</code>: Always <code>false</code> for error responses</li> <li><code>operation</code>: The operation that was being performed</li> <li><code>error</code>: Error message</li> <li><code>error_type</code>: Type of error</li> <li><code>error_category</code>: Category of error</li> <li><code>context</code>: Additional context about the error</li> <li><code>suggestions</code>: List of suggestions for resolving the error</li> </ul>"},{"location":"docs/mcp-server-guide/#error-categories","title":"Error Categories","text":"<p>The following error categories are used:</p> <ul> <li><code>file_not_found</code>: A required file or directory was not found</li> <li><code>permission_denied</code>: Permission issues accessing files or directories</li> <li><code>invalid_format</code>: File format or structure is invalid</li> <li><code>invalid_parameter</code>: Invalid parameter values provided</li> <li><code>invalid_path</code>: Path is invalid or improperly formatted</li> <li><code>resource_exhausted</code>: System resources are exhausted</li> <li><code>operation_timeout</code>: Operation took too long and timed out</li> <li><code>scan_not_found</code>: Requested scan ID was not found in the registry</li> <li><code>scan_incomplete</code>: Scan has not completed yet</li> <li><code>unexpected_error</code>: An unexpected error occurred</li> </ul>"},{"location":"docs/mcp-server-guide/#example-error-response","title":"Example Error Response","text":"<pre><code>{\n  \"success\": false,\n  \"operation\": \"get_scan_results\",\n  \"error\": \"Scan 550e8400-e29b-41d4-a716-446655440000 not found\",\n  \"error_type\": \"MCPResourceError\",\n  \"error_category\": \"scan_not_found\",\n  \"context\": {\n    \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n    \"error_category\": \"scan_not_found\"\n  },\n  \"suggestions\": [\n    \"Check that the scan ID is correct\",\n    \"Verify that the scan exists in the registry\",\n    \"The scan may have been cleaned up if it was completed a long time ago\"\n  ],\n  \"timestamp\": \"2025-07-16T12:37:12.345678\"\n}\n</code></pre>"},{"location":"docs/mcp-server-guide/#configuration","title":"Configuration","text":"<p>The ASH MCP server can be configured using the <code>mcp-resource-management</code> section in the ASH configuration file:</p> <pre><code># .ash/ash.yaml\nmcp-resource-management:\n  # Concurrent operations\n  max_concurrent_scans: 5\n  max_concurrent_tasks: 30\n  thread_pool_max_workers: 8\n\n  # Timeouts\n  scan_timeout_seconds: 2400\n  operation_timeout_seconds: 300\n\n  # Resource monitoring\n  enable_health_checks: true\n  health_check_interval_seconds: 30\n  memory_warning_threshold_mb: 1024\n  memory_critical_threshold_mb: 2048\n  task_count_warning_threshold: 20\n\n  # Message limits\n  max_message_size_bytes: 10485760  # 10MB\n  max_directory_size_mb: 1024       # 1GB\n</code></pre>"},{"location":"docs/mcp-server-guide/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>max_concurrent_scans</code>: Maximum number of concurrent scans</li> <li><code>max_concurrent_tasks</code>: Maximum number of concurrent tasks</li> <li><code>thread_pool_max_workers</code>: Maximum number of worker threads</li> <li><code>scan_timeout_seconds</code>: Timeout for scans in seconds</li> <li><code>operation_timeout_seconds</code>: Timeout for operations in seconds</li> <li><code>enable_health_checks</code>: Whether to enable health checks</li> <li><code>health_check_interval_seconds</code>: Interval between health checks in seconds</li> <li><code>memory_warning_threshold_mb</code>: Memory usage warning threshold in MB</li> <li><code>memory_critical_threshold_mb</code>: Memory usage critical threshold in MB</li> <li><code>task_count_warning_threshold</code>: Task count warning threshold</li> <li><code>max_message_size_bytes</code>: Maximum message size in bytes</li> <li><code>max_directory_size_mb</code>: Maximum directory size in MB</li> </ul>"},{"location":"docs/mcp-server-guide/#best-practices","title":"Best Practices","text":"<p>When using the ASH MCP server, follow these best practices:</p> <ol> <li>Store scan IDs: Always store the scan ID returned by <code>scan_directory</code> for later use</li> <li>Check scan progress: Periodically check scan progress using <code>get_scan_progress</code></li> <li>Handle errors gracefully: Check the <code>success</code> field in responses and handle errors appropriately</li> <li>Clean up resources: Cancel scans that are no longer needed using <code>cancel_scan</code></li> <li>Validate parameters: Ensure all parameters are valid before calling MCP tools</li> <li>Check installation: Use <code>check_installation</code> to verify ASH is properly installed</li> <li>Configure appropriately: Adjust configuration options based on your system resources and usage patterns</li> <li>Monitor resource usage: Keep an eye on system resources during scans</li> <li>Implement timeouts: Set appropriate timeouts for scans and operations</li> <li>Handle concurrent scans: Limit the number of concurrent scans based on your system resources</li> </ol>"},{"location":"docs/mcp-server-guide/#examples","title":"Examples","text":""},{"location":"docs/mcp-server-guide/#basic-scan-workflow","title":"Basic Scan Workflow","text":"<pre><code># Start a scan\nresult = await mcp_scan_directory(\n    directory_path=\"/path/to/your/code\",\n    severity_threshold=\"MEDIUM\"\n)\n\n# Get the scan ID\nscan_id = result[\"scan_id\"]\n\n# Check scan progress periodically\nwhile True:\n    progress = await mcp_get_scan_progress(scan_id=scan_id)\n\n    if progress[\"status\"] in [\"completed\", \"failed\", \"cancelled\"]:\n        break\n\n    # Wait before checking again\n    await asyncio.sleep(5)\n\n# Get scan results\nif progress[\"status\"] == \"completed\":\n    results = await mcp_get_scan_results(scan_id=scan_id)\n    # Process results\nelse:\n    # Handle failure or cancellation\n    print(f\"Scan {scan_id} {progress['status']}\")\n</code></pre>"},{"location":"docs/mcp-server-guide/#handling-multiple-scans","title":"Handling Multiple Scans","text":"<pre><code># Start multiple scans\nscan_ids = []\nfor directory in directories:\n    result = await mcp_scan_directory(\n        directory_path=directory,\n        severity_threshold=\"MEDIUM\"\n    )\n    scan_ids.append(result[\"scan_id\"])\n\n# Check progress of all scans\ncompleted_scans = set()\nwhile len(completed_scans) &lt; len(scan_ids):\n    for scan_id in scan_ids:\n        if scan_id in completed_scans:\n            continue\n\n        progress = await mcp_get_scan_progress(scan_id=scan_id)\n\n        if progress[\"status\"] in [\"completed\", \"failed\", \"cancelled\"]:\n            completed_scans.add(scan_id)\n\n    # Wait before checking again\n    await asyncio.sleep(5)\n\n# Get results for all completed scans\nfor scan_id in scan_ids:\n    progress = await mcp_get_scan_progress(scan_id=scan_id)\n\n    if progress[\"status\"] == \"completed\":\n        results = await mcp_get_scan_results(scan_id=scan_id)\n        # Process results\n    else:\n        # Handle failure or cancellation\n        print(f\"Scan {scan_id} {progress['status']}\")\n</code></pre>"},{"location":"docs/mcp-server-guide/#error-handling-example","title":"Error Handling Example","text":"<pre><code># Start a scan with error handling\ntry:\n    result = await mcp_scan_directory(\n        directory_path=\"/path/to/your/code\",\n        severity_threshold=\"MEDIUM\"\n    )\n\n    if not result[\"success\"]:\n        # Handle error\n        print(f\"Error: {result['error']}\")\n        print(f\"Suggestions: {result['suggestions']}\")\n        return\n\n    scan_id = result[\"scan_id\"]\n\n    # Check scan progress\n    while True:\n        progress = await mcp_get_scan_progress(scan_id=scan_id)\n\n        if not progress[\"success\"]:\n            # Handle error\n            print(f\"Error: {progress['error']}\")\n            print(f\"Suggestions: {progress['suggestions']}\")\n            return\n\n        if progress[\"status\"] in [\"completed\", \"failed\", \"cancelled\"]:\n            break\n\n        # Wait before checking again\n        await asyncio.sleep(5)\n\n    # Get scan results\n    if progress[\"status\"] == \"completed\":\n        results = await mcp_get_scan_results(scan_id=scan_id)\n\n        if not results[\"success\"]:\n            # Handle error\n            print(f\"Error: {results['error']}\")\n            print(f\"Suggestions: {results['suggestions']}\")\n            return\n\n        # Process results\n    else:\n        # Handle failure or cancellation\n        print(f\"Scan {scan_id} {progress['status']}\")\n\nexcept Exception as e:\n    # Handle unexpected errors\n    print(f\"Unexpected error: {str(e)}\")\n</code></pre>"},{"location":"docs/mcp-server-guide/#related-documentation","title":"Related Documentation","text":"<ul> <li>MCP Tutorial</li> <li>MCP Performance and Scalability</li> <li>Configuration Guide</li> <li>ASH CLI Reference</li> </ul>"},{"location":"docs/migration-guide/","title":"Migration Guide: ASH v2 to v3","text":"<p>This guide helps users migrate from ASH v2 to ASH v3.</p>"},{"location":"docs/migration-guide/#migration-steps","title":"Migration Steps","text":"<ol> <li>Install ASH v3 using one of the installation methods from the Installation Guide</li> <li>Initialize Configuration:    <pre><code>ash config init\n</code></pre></li> <li>Update Scripts:</li> <li>Add <code>--mode container</code> to your ASH commands if you need to run ASH in a container still<ul> <li>NOTE: This is only required if you are using the ASH CLI to manage the lifecycle of the container. If you are already running inside a container, such as running inside a CI pipeline using a pre-built ASH image, then you do not have to adjust your scripts.</li> </ul> </li> <li>Update output directory handling:<ul> <li>If you are explicitly passing the <code>--output-dir</code> to ASH, then ASH will continue to output to the same directory.</li> <li>If you are not explicitly passing the <code>--output-dir</code> to ASH, then you will need to update output directory references to <code>.ash/ash_output</code> OR start including <code>--output-dir ash_output</code> in your scripts to retain the existing output directory.</li> </ul> </li> <li>Replace any collection and/or parsing of <code>aggregated_results.txt</code> with collecting/parsing the reports found in the new <code>reports</code> directory of the <code>output-dir</code> OR with JSON parsing of the new <code>ash_aggregated_results.json</code> (public JSON schema in GitHub)</li> <li>Recommendation: Add <code>ash report</code> to your script after <code>ash</code> has completed to pretty-print the summary report in the terminal or job stdout.</li> <li>Update Pre-commit Configuration:</li> <li>Change hook ID from <code>ash</code> to <code>ash-simple-scan</code></li> <li>Update the revision to <code>v3.0.0</code> or later</li> <li>Test Your Migration:    <pre><code>ash --mode local\n</code></pre></li> </ol>"},{"location":"docs/migration-guide/#key-changes-in-ash-v3","title":"Key Changes in ASH v3","text":"<ol> <li>Python-based CLI: ASH now has a Python-based CLI entrypoint while maintaining backward compatibility with the shell script entrypoint</li> <li>Multiple Execution Modes: Run ASH in <code>local</code>, <code>container</code>, or <code>precommit</code> mode depending on your needs</li> <li>Enhanced Configuration: Support for YAML/JSON configuration files with overrides via CLI parameters</li> <li>Improved Reporting: Multiple report formats including JSON, Markdown, HTML, and CSV</li> <li>UV Tool Management: Automatic installation and isolation of scanner tools (Bandit, Checkov, Semgrep) via UV</li> <li>Customizable: Extend ASH with custom plugins, scanners, and reporters</li> </ol>"},{"location":"docs/migration-guide/#installation-changes","title":"Installation Changes","text":""},{"location":"docs/migration-guide/#ash-v2","title":"ASH v2","text":"<pre><code># Clone the repository\ngit clone https://github.com/awslabs/automated-security-helper.git\nexport PATH=\"${PATH}:/path/to/automated-security-helper\"\n</code></pre>"},{"location":"docs/migration-guide/#ash-v3","title":"ASH v3","text":"<pre><code># Option 1: Using uvx (recommended) -- add to shell profile\nalias ash=\"uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\"\n\n# Option 2: Using pipx\npipx install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n\n# Option 3: Using pip\npip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n</code></pre>"},{"location":"docs/migration-guide/#tool-management-changes","title":"Tool Management Changes","text":""},{"location":"docs/migration-guide/#ash-v2_1","title":"ASH v2","text":"<ul> <li>Required manual installation of all scanner tools</li> <li>Tools needed to be available in system PATH</li> <li>Version compatibility issues could occur</li> </ul>"},{"location":"docs/migration-guide/#ash-v3_1","title":"ASH v3","text":"<ul> <li>Automatic Tool Management: Tools like Bandit, Checkov, and Semgrep are automatically installed via UV tool isolation</li> <li>Flexible Version Constraints: ASH uses sensible default version constraints (e.g., Bandit &gt;=1.7.0 for enhanced SARIF support) with the ability to override through configuration</li> <li>Isolated Environments: Tools run in isolated environments without affecting your project dependencies</li> <li>Fallback Support: If UV tool installation fails, ASH falls back to system-installed tools when available</li> </ul>"},{"location":"docs/migration-guide/#migration-impact","title":"Migration Impact","text":"<ul> <li>No Action Required: Most users don't need to change anything - tools are installed automatically</li> <li>Offline Environments: Set <code>ASH_OFFLINE=true</code> to skip automatic installations and use pre-installed tools</li> <li>Custom Tool Versions: Pre-install specific versions using <code>uv tool install &lt;tool&gt;==&lt;version&gt;</code> if needed</li> </ul>"},{"location":"docs/migration-guide/#command-line-changes","title":"Command Line Changes","text":""},{"location":"docs/migration-guide/#basic-usage","title":"Basic Usage","text":""},{"location":"docs/migration-guide/#ash-v2_2","title":"ASH v2","text":"<pre><code># Only runs in a container\nash --source-dir /path/to/code --output-dir /path/to/output\n</code></pre>"},{"location":"docs/migration-guide/#ash-v3_2","title":"ASH v3","text":"<pre><code># Runs in Local mode by default with scanners found locally in $PATH\nash --source-dir /path/to/code --output-dir /path/to/output\n\n# Explicitly run in container mode (ensures all default scanners are available)\nash --mode container --source-dir /path/to/code --output-dir /path/to/output\n</code></pre>"},{"location":"docs/migration-guide/#common-parameters","title":"Common Parameters","text":"ASH v2 Parameter ASH v3 Parameter Notes <code>--source-dir</code> <code>--source-dir</code> Same behavior <code>--output-dir</code> <code>--output-dir</code> Default changed to <code>.ash/ash_output</code> <code>--ext</code> Not directly supported Use configuration file instead <code>--force</code> <code>--force</code> Same behavior <code>--no-cleanup</code> <code>--cleanup false</code> Inverted logic <code>--debug</code> <code>--debug</code> Same behavior <code>--quiet</code> <code>--quiet</code> Same behavior <code>--no-color</code> <code>--color false</code> Inverted logic <code>--single-process</code> <code>--strategy sequential</code> Renamed <code>--oci-runner</code> <code>--oci-runner</code> Same behavior"},{"location":"docs/migration-guide/#output-directory-changes","title":"Output Directory Changes","text":""},{"location":"docs/migration-guide/#ash-v2_3","title":"ASH v2","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 aggregated_results.txt\n\u2514\u2500\u2500 ash_cf2cdk_output/\n</code></pre>"},{"location":"docs/migration-guide/#ash-v3_3","title":"ASH v3","text":"<pre><code>.ash/ash_output/\n\u251c\u2500\u2500 ash_aggregated_results.json\n\u251c\u2500\u2500 ash-ignore-report.txt\n\u251c\u2500\u2500 ash-scan-set-files-list.txt\n\u251c\u2500\u2500 converted\n\u2502   \u2514\u2500\u2500 jupyter\n\u251c\u2500\u2500 reports\n\u2502   \u251c\u2500\u2500 ash.cdx.json\n\u2502   \u251c\u2500\u2500 ash.csv\n\u2502   \u251c\u2500\u2500 ash.flat.json\n\u2502   \u251c\u2500\u2500 ash.gl-sast-report.json\n\u2502   \u251c\u2500\u2500 ash.html\n\u2502   \u251c\u2500\u2500 ash.junit.xml\n\u2502   \u251c\u2500\u2500 ash.ocsf.json\n\u2502   \u251c\u2500\u2500 ash.sarif\n\u2502   \u251c\u2500\u2500 ash.summary.md\n\u2502   \u2514\u2500\u2500 ash.summary.txt\n\u2514\u2500\u2500 scanners\n    \u251c\u2500\u2500 bandit\n    \u2502   \u2514\u2500\u2500 source\n    \u2502       \u251c\u2500\u2500 bandit.sarif\n    \u2502       \u251c\u2500\u2500 BanditScanner.stderr.log\n    \u2502       \u2514\u2500\u2500 BanditScanner.stdout.log\n    \u251c\u2500\u2500 cdk-nag\n    \u2502   \u2514\u2500\u2500 source\n    \u2502       \u251c\u2500\u2500 ash-cdk-nag.sarif\n    \u2502       \u251c\u2500\u2500 tests--test_data--scanners--cdk--insecure-s3-template--yaml\n    \u2502       \u2502   \u251c\u2500\u2500 ASHCDKNagScanner.assets.json\n    \u2502       \u2502   \u251c\u2500\u2500 ASHCDKNagScanner.template.json\n    \u2502       \u2502   \u251c\u2500\u2500 AwsSolutions-ASHCDKNagScanner-NagReport.json\n    \u2502       \u2502   \u251c\u2500\u2500 cdk.out\n    \u2502       \u2502   \u251c\u2500\u2500 HIPAA.Security-ASHCDKNagScanner-NagReport.json\n    \u2502       \u2502   \u251c\u2500\u2500 manifest.json\n    \u2502       \u2502   \u251c\u2500\u2500 NIST.800.53.R4-ASHCDKNagScanner-NagReport.json\n    \u2502       \u2502   \u251c\u2500\u2500 NIST.800.53.R5-ASHCDKNagScanner-NagReport.json\n    \u2502       \u2502   \u251c\u2500\u2500 PCI.DSS.321-ASHCDKNagScanner-NagReport.json\n    \u2502       \u2502   \u2514\u2500\u2500 tree.json\n    \u251c\u2500\u2500 cfn-nag\n    \u2502   \u2514\u2500\u2500 source\n    \u2502       \u251c\u2500\u2500 _ash__ash_output_precommit__scanners__cdk-nag__source__tests-test_data-scanners-cdk-insecure-s3-template-yaml__ASHCDKNagScanner__template__json\n    \u2502       \u2502   \u2514\u2500\u2500 CfnNagScanner.stdout.log\n    \u2502       \u251c\u2500\u2500 _ash__ash_output_precommit_local__scanners__cdk-nag__source__tests-test_data-scanners-cdk-insecure-s3-template-yaml__ASHCDKNagScanner__template__json\n    \u2502       \u2502   \u2514\u2500\u2500 CfnNagScanner.stdout.log\n    \u2502       \u2514\u2500\u2500 cfn_nag.sarif\n    \u251c\u2500\u2500 checkov\n    \u2502   \u2514\u2500\u2500 source\n    \u2502       \u251c\u2500\u2500 CheckovScanner.stderr.log\n    \u2502       \u251c\u2500\u2500 CheckovScanner.stdout.log\n    \u2502       \u2514\u2500\u2500 results_sarif.sarif\n    \u251c\u2500\u2500 detect-secrets\n    \u2502   \u2514\u2500\u2500 source\n    \u2502       \u2514\u2500\u2500 results_sarif.sarif\n    \u251c\u2500\u2500 grype\n    \u2502   \u2514\u2500\u2500 source\n    \u2502       \u251c\u2500\u2500 GrypeScanner.stderr.log\n    \u2502       \u2514\u2500\u2500 results_sarif.sarif\n    \u251c\u2500\u2500 opengrep\n    \u2502   \u2514\u2500\u2500 source\n    \u2502       \u251c\u2500\u2500 OpengrepScanner.stderr.log\n    \u2502       \u251c\u2500\u2500 OpengrepScanner.stdout.log\n    \u2502       \u2514\u2500\u2500 results_sarif.sarif\n    \u251c\u2500\u2500 semgrep\n    \u2502   \u2514\u2500\u2500 source\n    \u2502       \u251c\u2500\u2500 results_sarif.sarif\n    \u2502       \u251c\u2500\u2500 SemgrepScanner.stderr.log\n    \u2502       \u2514\u2500\u2500 SemgrepScanner.stdout.log\n    \u2514\u2500\u2500 syft\n        \u2514\u2500\u2500 source\n            \u251c\u2500\u2500 syft.cdx.json\n            \u251c\u2500\u2500 syft.cdx.json.syft-table.txt\n            \u2514\u2500\u2500 SyftScanner.stderr.log\n</code></pre>"},{"location":"docs/migration-guide/#configuration-changes","title":"Configuration Changes","text":""},{"location":"docs/migration-guide/#ash-v2_4","title":"ASH v2","text":"<p>No formal configuration file. Settings controlled via command line parameters.</p>"},{"location":"docs/migration-guide/#ash-v3_4","title":"ASH v3","text":"<p>YAML configuration file (<code>.ash/.ash.yaml</code>):</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/awslabs/automated-security-helper/refs/heads/main/automated_security_helper/schemas/AshConfig.json\nproject_name: my-project\nglobal_settings:\n  severity_threshold: MEDIUM\n  ignore_paths:\n    - path: 'tests/test_data'\n      reason: 'Test data only'\nscanners:\n  bandit:\n    enabled: true\n    options:\n      confidence_level: HIGH\nreporters:\n  markdown:\n    enabled: true\n    options:\n      include_detailed_findings: true\n</code></pre>"},{"location":"docs/migration-guide/#pre-commit-integration-changes","title":"Pre-commit Integration Changes","text":""},{"location":"docs/migration-guide/#ash-v2_5","title":"ASH v2","text":"<p>V2 pre-commit hook runs in a container</p> <pre><code>repos:\n  - repo: https://github.com/awslabs/automated-security-helper\n    rev: v1.3.3\n    hooks:\n      - id: ash\n</code></pre>"},{"location":"docs/migration-guide/#ash-v3_5","title":"ASH v3","text":"<p>V3 pre-commit hook runs 100% locally, no container involved</p> <pre><code>repos:\n  - repo: https://github.com/awslabs/automated-security-helper\n    rev: v3.0.0\n    hooks:\n      - id: ash-simple-scan\n</code></pre>"},{"location":"docs/migration-guide/#scanner-changes","title":"Scanner Changes","text":""},{"location":"docs/migration-guide/#added-scanners","title":"Added Scanners","text":"<ul> <li>detect-secrets: Replaced git-secrets for more comprehensive secret scanning</li> <li>Expanded Checkov Coverage: Now scans all supported frameworks</li> </ul>"},{"location":"docs/migration-guide/#improved-scanners","title":"Improved Scanners","text":"<ul> <li>Enhanced Semgrep Integration: Utilizes Semgrep's full language support</li> <li>Better SCA and SBOM Generation: Full integration of Grype and Syft</li> </ul>"},{"location":"docs/migration-guide/#windows-support-changes","title":"Windows Support Changes","text":""},{"location":"docs/migration-guide/#ash-v2_6","title":"ASH v2","text":"<ul> <li>Required a container runtime for all operations, which required WSL2.</li> </ul>"},{"location":"docs/migration-guide/#ash-v3_6","title":"ASH v3","text":"<ul> <li>Local Mode: Runs natively on Windows with Python 3.10+</li> <li>Container Mode: Still requires WSL2 and a container runtime</li> </ul>"},{"location":"docs/migration-guide/#programmatic-usage-new-in-v3","title":"Programmatic Usage (New in v3)","text":"<p>ASH v3 can be used programmatically in Python:</p> <pre><code>from automated_security_helper.interactions.run_ash_scan import run_ash_scan\nfrom automated_security_helper.core.enums import RunMode, Strategy\n\n# Run a scan\nresults = run_ash_scan(\n    source_dir=\"/path/to/code\",\n    output_dir=\"/path/to/output\",\n    mode=RunMode.local,\n    strategy=Strategy.parallel,\n    scanners=[\"bandit\", \"semgrep\"]\n)\n</code></pre>"},{"location":"docs/migration-guide/#troubleshooting-common-migration-issues","title":"Troubleshooting Common Migration Issues","text":""},{"location":"docs/migration-guide/#issue-ash-command-not-found","title":"Issue: ASH command not found","text":"<p>Solution: Ensure you've installed ASH v3 using one of the installation methods and that it's in your PATH.</p>"},{"location":"docs/migration-guide/#issue-missing-scanners-in-local-mode","title":"Issue: Missing scanners in local mode","text":"<p>Solution: Use <code>--mode container</code> to access all scanners or install the required dependencies locally.</p>"},{"location":"docs/migration-guide/#issue-configuration-file-not-found","title":"Issue: Configuration file not found","text":"<p>Solution: Run <code>ash config init</code> to create a default configuration file.</p>"},{"location":"docs/migration-guide/#issue-different-findings-compared-to-v2","title":"Issue: Different findings compared to v2","text":"<p>Solution: ASH v3 uses updated versions of scanners and may have different detection capabilities. Review the findings and adjust your configuration as needed.</p>"},{"location":"docs/migration-guide/#issue-scripts-parsing-aggregated_resultstxt-no-longer-work","title":"Issue: Scripts parsing aggregated_results.txt no longer work","text":"<p>Solution: Update your scripts to parse the new JSON output format in <code>ash_aggregated_results.json</code>.</p>"},{"location":"docs/migration-guide/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during migration:</p> <ol> <li>Check the ASH Documentation</li> <li>Create an issue on GitHub</li> <li>Run <code>ash --help</code> for command-line help</li> </ol>"},{"location":"docs/quick-start-guide/","title":"Quick Start","text":"<p>This guide will help you get started with ASH v3 quickly. For more detailed information, refer to the other documentation pages.</p>"},{"location":"docs/quick-start-guide/#overview","title":"Overview","text":"<p>ASH v3 has been entirely rewritten in Python with significant improvements:</p> <ol> <li>Python-based CLI: New Python entrypoint with backward compatibility for shell scripts</li> <li>Multiple Execution Modes: Run in <code>local</code>, <code>container</code>, or <code>precommit</code> mode</li> <li>New Output Structure: Results stored in <code>.ash/ash_output/</code> with multiple report formats</li> <li>Enhanced Configuration: YAML-based configuration with CLI overrides</li> </ol>"},{"location":"docs/quick-start-guide/#installation","title":"Installation","text":"<p>Choose one of these methods to install ASH:</p>"},{"location":"docs/quick-start-guide/#option-1-using-uvx-recommended","title":"Option 1: Using uvx (recommended)","text":"<p>Prerequisites: Python 3.10+, uv</p>"},{"location":"docs/quick-start-guide/#linuxmacos","title":"Linux/macOS","text":"<pre><code># Install uv if you don't have it\ncurl -sSf https://astral.sh/uv/install.sh | sh\n\n# Create an alias for ASH\nalias ash=\"uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\"\n</code></pre>"},{"location":"docs/quick-start-guide/#windows-powershell","title":"Windows PowerShell","text":"<pre><code># Install uv if you don't have it\nirm https://astral.sh/uv/install.ps1 | iex\n\n# Create a function for ASH\nfunction ash { uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2 $args }\n</code></pre>"},{"location":"docs/quick-start-guide/#option-2-using-pipx","title":"Option 2: Using pipx","text":"<p>Prerequisites: Python 3.10+, pipx</p> <pre><code>pipx install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n</code></pre>"},{"location":"docs/quick-start-guide/#option-3-using-pip","title":"Option 3: Using pip","text":"<p>Prerequisites: Python 3.10+</p> <pre><code>pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n</code></pre>"},{"location":"docs/quick-start-guide/#basic-usage","title":"Basic Usage","text":""},{"location":"docs/quick-start-guide/#running-your-first-scan","title":"Running Your First Scan","text":"<p>Navigate to your project directory and run:</p> <pre><code># Run a scan in local mode (Python-based scanners only)\nash --mode local\n\n# Run a scan in container mode (all scanners)\nash --mode container\n\n# Run a scan in precommit mode (fast subset of scanners)\nash --mode precommit\n</code></pre> <p>The <code>precommit</code> mode runs a subset of fast scanners: - bandit - detect-secrets - checkov - cdk-nag - npm-audit (if available)</p>"},{"location":"docs/quick-start-guide/#specifying-source-and-output-directories","title":"Specifying Source and Output Directories","text":"<pre><code>ash --source-dir /path/to/code --output-dir /path/to/output\n</code></pre>"},{"location":"docs/quick-start-guide/#viewing-results","title":"Viewing Results","text":"<p>After running a scan, check the output directory (default: <code>.ash/ash_output/</code>):</p> <pre><code># View the summary report\ncat .ash/ash_output/reports/ash.summary.txt\n\n# Open the HTML report in your browser\nopen .ash/ash_output/reports/ash.html  # On macOS\nxdg-open .ash/ash_output/reports/ash.html  # On Linux\nstart .ash/ash_output/reports/ash.html  # On Windows\n</code></pre> <p>Available report formats: - <code>ash.summary.txt</code>: Human-readable text summary - <code>ash.summary.md</code>: Markdown summary for GitHub PRs and other platforms - <code>ash.html</code>: Interactive HTML report - <code>ash.csv</code>: CSV report for filtering and sorting findings - <code>ash_aggregated_results.json</code>: Complete machine-readable results</p>"},{"location":"docs/quick-start-guide/#configuration","title":"Configuration","text":"<p>ASH uses a YAML configuration file. Create a basic configuration:</p> <pre><code># Initialize a new configuration file\nash config init\n</code></pre> <p>This creates <code>.ash/.ash.yaml</code> with default settings. Edit this file to customize your scan:</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/awslabs/automated-security-helper/refs/heads/main/automated_security_helper/schemas/AshConfig.json\nproject_name: my-project\nglobal_settings:\n  severity_threshold: MEDIUM\n  ignore_paths:\n    - path: 'tests/test_data'\n      reason: 'Test data only'\n  suppressions:\n    - rule_id: 'RULE-123'\n      file_path: 'src/example.py'\n      line_start: 10\n      line_end: 15\n      reason: 'False positive due to test mock'\n      expiration: '2025-12-31'\n    - rule_id: 'RULE-456'\n      file_path: 'src/*.js'\n      reason: 'Known issue, planned for fix in v2.0'\nscanners:\n  bandit:\n    enabled: true\n  semgrep:\n    enabled: true\nreporters:\n  markdown:\n    enabled: true\n  html:\n    enabled: true\n</code></pre>"},{"location":"docs/quick-start-guide/#common-tasks","title":"Common Tasks","text":""},{"location":"docs/quick-start-guide/#overriding-configuration-options","title":"Overriding Configuration Options","text":"<pre><code># Enable a specific scanner\nash --config-overrides 'scanners.bandit.enabled=true'\n\n# Change severity threshold\nash --config-overrides 'global_settings.severity_threshold=LOW'\n\n# Ignore a path\nash --config-overrides 'global_settings.ignore_paths+=[{\"path\": \"build/\", \"reason\": \"Generated files\"}]'\n</code></pre>"},{"location":"docs/quick-start-guide/#running-specific-scanners","title":"Running Specific Scanners","text":"<pre><code># Run only specific scanners\nash --scanners bandit,semgrep\n\n# Exclude specific scanners\nash --exclude-scanners cfn-nag,cdk-nag\n</code></pre>"},{"location":"docs/quick-start-guide/#generating-specific-reports","title":"Generating Specific Reports","text":"<pre><code># Generate specific report formats\nash --output-formats markdown,html,json\n\n# Generate a report from existing results\nash report --format html --output-dir ./my-scan-results\n</code></pre>"},{"location":"docs/quick-start-guide/#using-ash-with-pre-commit","title":"Using ASH with pre-commit","text":"<p>Add this to your <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n  - repo: https://github.com/awslabs/automated-security-helper\n    rev: v3.0.0\n    hooks:\n      - id: ash-simple-scan\n</code></pre> <p>Run with:</p> <pre><code>pre-commit run ash-simple-scan --all-files\n</code></pre>"},{"location":"docs/quick-start-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Configure ASH for your project</li> <li>Learn about ASH's CLI options</li> <li>Set up ASH in CI/CD pipelines</li> <li>Explore advanced features</li> </ul>"},{"location":"docs/support/","title":"Support Matrix","text":"<p>ASH itself should support running in any environment that can support running <code>linux/amd64</code> container images.</p>"},{"location":"docs/support/#local-execution","title":"Local Execution","text":"<p>The table below provides a matrix of tested runtime environments for ASH.</p> OCI Container Tool Host Full Support Partial Support No Support Untested Finch macOS w/ Intel Finch macOS w/ Apple Silicon Docker Desktop macOS w/ Intel Docker Desktop macOS w/ Apple Silicon Rancher Desktop w/ docker+moby macOS w/ Intel Rancher Desktop w/ docker+moby macOS w/ Apple Silicon Rancher Desktop w/ nerdctl+containerd macOS w/ Intel Rancher Desktop w/ nerdctl+containerd macOS w/ Apple Silicon"},{"location":"docs/support/#continuous-integration","title":"Continuous Integration","text":"<p>The table below provides a matrix of tested CI execution environment for ASH.</p> <p>For more information, please see Running ASH in CI</p> CI Platform Execution Method Full Support Partial Support No Support Untested GitLab CI Container Job GitLab CI <code>docker run</code> GitHub Actions (hosted  Ubuntu agents) Container Job GitHub Actions (hosted  Ubuntu agents) <code>docker run</code> Azure Pipelines (hosted Ubuntu agents) Container Job Azure Pipelines (hosted Ubuntu agents) <code>docker run</code> Jenkins <code>docker run</code>"},{"location":"docs/suppressions/","title":"Global Suppressions","text":"<p>ASH v3 supports global suppressions, allowing you to suppress specific security findings across your project. This feature helps reduce noise from known issues that have been reviewed and accepted, allowing teams to focus on new and relevant security findings.</p>"},{"location":"docs/suppressions/#understanding-suppressions-vs-ignore-paths","title":"Understanding Suppressions vs. Ignore Paths","text":"<p>ASH provides two mechanisms for excluding findings:</p> <ol> <li>Ignore Paths: Files matching these patterns are completely excluded from scanning and do not appear in final results. Use this when you want to completely skip scanning certain files or directories (like test data, third-party code, or generated files).</li> <li>Suppressions: Findings matching these rules are still scanned but marked as suppressed in the final report, making them visible but not counted toward failure thresholds. Use this for specific known issues that have been reviewed and accepted.</li> </ol> <p>Key differences:</p> Feature Ignore Paths Suppressions Scope Entire files/directories Specific findings Visibility Files not scanned at all Findings still visible but marked as suppressed Granularity File-level only Rule ID, file path, and line number Tracking No tracking of ignored files Suppressed findings are tracked and reported Expiration No expiration mechanism Can set expiration dates"},{"location":"docs/suppressions/#configuring-suppressions","title":"Configuring Suppressions","text":"<p>Suppressions are defined in the <code>.ash.yaml</code> configuration file under the <code>global_settings</code> section:</p> <pre><code>global_settings:\n  suppressions:\n    - rule_id: 'RULE-123'\n      path: 'src/example.py'\n      line_start: 10\n      line_end: 15\n      reason: 'False positive due to test mock'\n      expiration: '2025-12-31'\n    - rule_id: 'RULE-456'\n      path: 'src/*.js'\n      reason: 'Known issue, planned for fix in v2.0'\n</code></pre>"},{"location":"docs/suppressions/#suppression-properties","title":"Suppression Properties","text":"<p>Each suppression rule can include the following properties:</p> Property Required Description <code>path</code> Yes File path or glob pattern to match <code>reason</code> Yes Justification for the suppression <code>rule_id</code> Yes The scanner-specific rule ID to suppress <code>line_start</code> No Starting line number for the suppression <code>line_end</code> No Ending line number for the suppression <code>expiration</code> No Date when the suppression expires (YYYY-MM-DD)"},{"location":"docs/suppressions/#matching-rules","title":"Matching Rules","text":"<ul> <li>Rule ID: Must match exactly the rule ID reported by the scanner</li> <li>File Path: Supports glob patterns (e.g., <code>src/*.js</code>, <code>**/*.py</code>)</li> <li>Line Range: If specified, only findings within this line range will be suppressed</li> </ul>"},{"location":"docs/suppressions/#examples","title":"Examples","text":""},{"location":"docs/suppressions/#suppress-a-specific-rule-in-a-file","title":"Suppress a Specific Rule in a File","text":"<pre><code>suppressions:\n  - rule_id: 'B605'  # Bandit rule for os.system\n    path: 'src/utils.py'\n    reason: 'Command is properly sanitized'\n</code></pre>"},{"location":"docs/suppressions/#suppress-a-rule-in-multiple-files","title":"Suppress a Rule in Multiple Files","text":"<pre><code>suppressions:\n  - rule_id: 'CKV_AWS_123'\n    path: 'terraform/*.tf'\n    reason: 'Approved exception per security review'\n</code></pre>"},{"location":"docs/suppressions/#suppress-a-rule-for-specific-lines","title":"Suppress a Rule for Specific Lines","text":"<pre><code>suppressions:\n  - rule_id: 'detect-secrets'\n    path: 'config/settings.py'\n    line_start: 45\n    line_end: 47\n    reason: 'Test credentials used in CI only'\n</code></pre>"},{"location":"docs/suppressions/#suppress-with-expiration-date","title":"Suppress with Expiration Date","text":"<pre><code>suppressions:\n  - rule_id: 'RULE-789'\n    path: 'src/legacy.py'\n    reason: 'Will be fixed in next sprint'\n    expiration: '2025-06-30'\n</code></pre>"},{"location":"docs/suppressions/#temporarily-disabling-suppressions","title":"Temporarily Disabling Suppressions","text":"<p>To temporarily ignore all suppressions and see all findings, use the <code>--ignore-suppressions</code> flag:</p> <pre><code>ash --ignore-suppressions\n</code></pre> <p>This is useful when you want to:</p> <ul> <li>Verify if previously suppressed issues have been fixed</li> <li>Get a complete view of all security findings in your codebase</li> <li>Perform a comprehensive security review</li> </ul> <p>When this flag is used, ASH will process all findings as if no suppressions were defined, but will still respect the <code>ignore_paths</code> settings.</p>"},{"location":"docs/suppressions/#expiring-suppressions","title":"Expiring Suppressions","text":"<p>When a suppression has an expiration date:</p> <ol> <li>The suppression will only be applied until that date</li> <li>When the date is reached, the suppression will no longer be applied</li> <li>ASH will warn you when suppressions are about to expire within 30 days</li> </ol> <p>This helps ensure that temporary exceptions don't become permanent security gaps.</p>"},{"location":"docs/suppressions/#best-practices","title":"Best Practices","text":"<ol> <li>Always provide a reason: Document why the finding is being suppressed</li> <li>Use expiration dates: Set an expiration date for temporary suppressions</li> <li>Be specific: Use line numbers when possible to limit the scope of suppressions</li> <li>Regular review: Periodically review suppressions to ensure they're still valid</li> <li>Document approvals: Include reference to security review or approval in the reason</li> </ol>"},{"location":"docs/troubleshooting-uv-installation/","title":"Troubleshooting UV Tool Installation","text":"<p>This guide helps resolve common issues with UV tool installation in ASH (Automated Security Helper).</p>"},{"location":"docs/troubleshooting-uv-installation/#quick-diagnosis","title":"Quick Diagnosis","text":""},{"location":"docs/troubleshooting-uv-installation/#check-installation-status","title":"Check Installation Status","text":"<pre><code># Check if UV is available\nuv --version\n\n# List installed UV tools\nuv tool list\n\n# Check specific tool installation\nuv tool run bandit --version\nuv tool run checkov --version\nuv tool run semgrep --version\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>export ASH_LOG_LEVEL=DEBUG\nash --mode local\n</code></pre> <p>Look for log messages with these tags: - <code>[INSTALLATION_START]</code> - Installation beginning - <code>[INSTALLATION_PROGRESS]</code> - Installation progress - <code>[INSTALLATION_SUCCESS]</code> - Successful installation - <code>[INSTALLATION_FAILED]</code> - Installation failure - <code>[INSTALLATION_ERROR]</code> - Installation error - <code>[INSTALLATION_SKIP]</code> - Installation skipped</p>"},{"location":"docs/troubleshooting-uv-installation/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"docs/troubleshooting-uv-installation/#1-uv-not-available","title":"1. UV Not Available","text":""},{"location":"docs/troubleshooting-uv-installation/#symptoms","title":"Symptoms","text":"<pre><code>UV tool validation failed - UV is not available but required\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#diagnosis","title":"Diagnosis","text":"<pre><code>which uv\nuv --version\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#solutions","title":"Solutions","text":"<p>Option A: Install UV via pip <pre><code>pip install uv\n</code></pre></p> <p>Option B: Install UV via official installer <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\nsource ~/.bashrc  # or restart terminal\n</code></pre></p> <p>Option C: Install UV via package manager <pre><code># macOS with Homebrew\nbrew install uv\n\n# Ubuntu/Debian\nsudo apt update &amp;&amp; sudo apt install uv\n\n# Arch Linux\npacman -S uv\n</code></pre></p> <p>Option D: Use pre-installed tools <pre><code># Install tools system-wide as fallback\npip install bandit checkov semgrep\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#2-installation-timeout","title":"2. Installation Timeout","text":""},{"location":"docs/troubleshooting-uv-installation/#symptoms_1","title":"Symptoms","text":"<pre><code>Tool installation timed out after 300 seconds for bandit\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#diagnosis_1","title":"Diagnosis","text":"<ul> <li>Check network connectivity</li> <li>Check if PyPI is accessible</li> <li>Monitor installation progress</li> </ul>"},{"location":"docs/troubleshooting-uv-installation/#solutions_1","title":"Solutions","text":"<p>Option A: Increase timeout <pre><code># .ash/ash.yaml\nscanners:\n  bandit:\n    options:\n      install_timeout: 600  # 10 minutes\n</code></pre></p> <p>Option B: Check network connectivity <pre><code># Test PyPI connectivity\ncurl -I https://pypi.org/\n\n# Test specific package availability\ncurl -I https://pypi.org/project/bandit/\n</code></pre></p> <p>Option C: Use offline mode <pre><code>export ASH_OFFLINE=true\nash --mode local\n</code></pre></p> <p>Option D: Pre-install tools <pre><code># Pre-install before running ASH\nuv tool install bandit\nuv tool install checkov\nuv tool install semgrep\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#3-version-constraint-issues","title":"3. Version Constraint Issues","text":""},{"location":"docs/troubleshooting-uv-installation/#symptoms_2","title":"Symptoms","text":"<pre><code>Tool installation failed for bandit with exit code 1\nNo matching distribution found for bandit&gt;=2.0.0\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#diagnosis_2","title":"Diagnosis","text":"<pre><code># Check available versions\npip index versions bandit\n\n# Test version constraint manually\nuv tool install \"bandit&gt;=1.7.0,&lt;2.0.0\"\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#solutions_2","title":"Solutions","text":"<p>Option A: Fix version constraint syntax <pre><code># Correct syntax\ntool_version: \"&gt;=1.7.0,&lt;2.0.0\"\n\n# Common mistakes to avoid\ntool_version: \"&gt;= 1.7.0, &lt; 2.0.0\"  # Extra spaces\ntool_version: \"&gt;=1.7.0 &lt;2.0.0\"     # Missing comma\n</code></pre></p> <p>Option B: Use broader version ranges <pre><code># Instead of exact version\ntool_version: \"==1.7.5\"\n\n# Use minimum version\ntool_version: \"&gt;=1.7.0\"\n</code></pre></p> <p>Option C: Check version availability <pre><code># List available versions\npip index versions bandit\npip index versions checkov\npip index versions semgrep\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#4-offline-mode-issues","title":"4. Offline Mode Issues","text":""},{"location":"docs/troubleshooting-uv-installation/#symptoms_3","title":"Symptoms","text":"<pre><code>Offline mode is enabled (ASH_OFFLINE=true) but tool 'bandit' is not available\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#diagnosis_3","title":"Diagnosis","text":"<pre><code># Check if tools are pre-installed\nwhich bandit\nwhich checkov\nwhich semgrep\n\n# Check UV tool installation\nuv tool list\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#solutions_3","title":"Solutions","text":"<p>Option A: Pre-install tools with UV <pre><code># Install tools before enabling offline mode\nuv tool install bandit\nuv tool install checkov\nuv tool install semgrep\n\n# Then enable offline mode\nexport ASH_OFFLINE=true\n</code></pre></p> <p>Option B: Install tools system-wide <pre><code>pip install bandit checkov semgrep\n</code></pre></p> <p>Option C: Use UV cache in offline mode <pre><code># Set UV offline mode\nexport UV_OFFLINE=1\nexport ASH_OFFLINE=true\n</code></pre></p> <p>Option D: Disable offline mode temporarily <pre><code>unset ASH_OFFLINE\nash --mode local\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#5-permission-issues","title":"5. Permission Issues","text":""},{"location":"docs/troubleshooting-uv-installation/#symptoms_4","title":"Symptoms","text":"<pre><code>Permission denied: '/usr/local/bin/uv'\n[Errno 13] Permission denied: '/home/user/.local/share/uv'\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#diagnosis_4","title":"Diagnosis","text":"<pre><code># Check UV installation location\nwhich uv\nls -la $(which uv)\n\n# Check UV cache directory permissions\nuv cache dir\nls -la $(uv cache dir)\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#solutions_4","title":"Solutions","text":"<p>Option A: Fix UV permissions <pre><code># If UV is installed system-wide\nsudo chown $(whoami) $(which uv)\n\n# If cache directory has permission issues\nsudo chown -R $(whoami) $(uv cache dir)\n</code></pre></p> <p>Option B: Install UV in user directory <pre><code># Reinstall UV for current user\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p> <p>Option C: Use virtual environment <pre><code>python -m venv venv\nsource venv/bin/activate\npip install uv\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#6-network-and-proxy-issues","title":"6. Network and Proxy Issues","text":""},{"location":"docs/troubleshooting-uv-installation/#symptoms_5","title":"Symptoms","text":"<pre><code>Failed to download package: Connection timeout\nSSL certificate verification failed\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#diagnosis_5","title":"Diagnosis","text":"<pre><code># Test direct connectivity\ncurl -I https://pypi.org/\n\n# Check proxy settings\necho $HTTP_PROXY\necho $HTTPS_PROXY\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#solutions_5","title":"Solutions","text":"<p>Option A: Configure proxy for UV <pre><code>export HTTP_PROXY=http://proxy.company.com:8080\nexport HTTPS_PROXY=http://proxy.company.com:8080\nexport NO_PROXY=localhost,127.0.0.1\n</code></pre></p> <p>Option B: Use trusted hosts <pre><code># For pip fallback\npip install --trusted-host pypi.org --trusted-host pypi.python.org bandit\n</code></pre></p> <p>Option C: Use offline mode with pre-downloaded packages <pre><code># Download packages on a machine with internet\nuv tool install bandit\nuv tool install checkov\nuv tool install semgrep\n\n# Copy UV cache to offline machine\ntar -czf uv-cache.tar.gz $(uv cache dir)\n# Transfer and extract on offline machine\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#7-disk-space-issues","title":"7. Disk Space Issues","text":""},{"location":"docs/troubleshooting-uv-installation/#symptoms_6","title":"Symptoms","text":"<pre><code>No space left on device\nOSError: [Errno 28] No space left on device\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#diagnosis_6","title":"Diagnosis","text":"<pre><code># Check disk space\ndf -h\n\n# Check UV cache size\ndu -sh $(uv cache dir)\n\n# Check specific tool sizes\nuv tool list\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#solutions_6","title":"Solutions","text":"<p>Option A: Clean UV cache <pre><code>uv cache clean\n</code></pre></p> <p>Option B: Move UV cache to larger disk <pre><code># Set custom cache directory\nexport UV_CACHE_DIR=/path/to/larger/disk/uv-cache\n</code></pre></p> <p>Option C: Free up disk space <pre><code># Remove unused packages\npip uninstall &lt;unused-packages&gt;\n\n# Clean system package cache\nsudo apt clean  # Ubuntu/Debian\nbrew cleanup    # macOS\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#8-concurrent-installation-issues","title":"8. Concurrent Installation Issues","text":""},{"location":"docs/troubleshooting-uv-installation/#symptoms_7","title":"Symptoms","text":"<pre><code>Another uv process is already running\nLock file conflict during installation\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#diagnosis_7","title":"Diagnosis","text":"<pre><code># Check for running UV processes\nps aux | grep uv\n\n# Check for lock files\nfind /tmp -name \"*uv*lock*\" 2&gt;/dev/null\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#solutions_7","title":"Solutions","text":"<p>Option A: Wait for other processes <pre><code># Wait for other UV processes to complete\nwait\n</code></pre></p> <p>Option B: Kill stuck processes <pre><code># Find and kill stuck UV processes\npkill -f \"uv tool\"\n</code></pre></p> <p>Option C: Remove lock files <pre><code># Remove stale lock files (use with caution)\nrm -f /tmp/*uv*lock*\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#9-tool-specific-issues","title":"9. Tool-Specific Issues","text":""},{"location":"docs/troubleshooting-uv-installation/#bandit-issues","title":"Bandit Issues","text":"<p>Symptoms: <pre><code>ModuleNotFoundError: No module named 'bandit.formatters.sarif'\n</code></pre></p> <p>Solution: <pre><code># Ensure SARIF extra is installed\nscanners:\n  bandit:\n    options:\n      tool_version: \"&gt;=1.7.0\"  # SARIF support requires 1.7.0+\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#checkov-issues","title":"Checkov Issues","text":"<p>Symptoms: <pre><code>checkov: command not found after installation\n</code></pre></p> <p>Solution: <pre><code># Verify installation\nuv tool run checkov --version\n\n# Check PATH\necho $PATH | grep -o '[^:]*uv[^:]*'\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#semgrep-issues","title":"Semgrep Issues","text":"<p>Symptoms: <pre><code>Semgrep rules download failed in offline mode\n</code></pre></p> <p>Solution: <pre><code># Pre-download rules for offline use\nexport SEMGREP_RULES_CACHE_DIR=/path/to/rules/cache\nsemgrep --config=auto --download-rules\n</code></pre></p>"},{"location":"docs/troubleshooting-uv-installation/#advanced-troubleshooting","title":"Advanced Troubleshooting","text":""},{"location":"docs/troubleshooting-uv-installation/#debug-installation-process","title":"Debug Installation Process","text":"<pre><code># Enable maximum verbosity\nexport ASH_LOG_LEVEL=TRACE\nexport UV_VERBOSE=1\n\n# Run with debug output\nash --mode local 2&gt;&amp;1 | tee ash-debug.log\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#manual-installation-testing","title":"Manual Installation Testing","text":"<pre><code># Test manual UV tool installation\nuv tool install --verbose bandit\n\n# Test tool execution\nuv tool run bandit --help\n\n# Test with specific version\nuv tool install \"bandit&gt;=1.7.0,&lt;2.0.0\"\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#environment-validation","title":"Environment Validation","text":"<pre><code># Check Python environment\npython --version\nwhich python\n\n# Check UV environment\nuv --version\nuv tool dir\nuv cache dir\n\n# Check system resources\ndf -h\nfree -h\nulimit -a\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#configuration-validation","title":"Configuration Validation","text":"<pre><code># Validate ASH configuration\nash --validate-config\n\n# Check scanner configuration\nash --list-scanners\n\n# Test specific scanner\nash --scanner bandit --dry-run\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#prevention-strategies","title":"Prevention Strategies","text":""},{"location":"docs/troubleshooting-uv-installation/#1-pre-installation-in-cicd","title":"1. Pre-Installation in CI/CD","text":"<pre><code># GitHub Actions example\n- name: Setup UV and Tools\n  run: |\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    uv tool install bandit\n    uv tool install checkov\n    uv tool install semgrep\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#2-health-checks","title":"2. Health Checks","text":"<pre><code>#!/bin/bash\n# health-check.sh\n\necho \"Checking UV installation...\"\nuv --version || exit 1\n\necho \"Checking tool installations...\"\nuv tool run bandit --version || exit 1\nuv tool run checkov --version || exit 1\nuv tool run semgrep --version || exit 1\n\necho \"All tools are ready!\"\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#3-monitoring-and-alerting","title":"3. Monitoring and Alerting","text":"<pre><code># Monitor installation success rates\ngrep \"INSTALLATION_SUCCESS\\|INSTALLATION_FAILED\" ash.log | \\\n  awk '{print $1}' | sort | uniq -c\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#4-backup-strategies","title":"4. Backup Strategies","text":"<pre><code># Backup UV cache for offline use\ntar -czf uv-cache-backup.tar.gz $(uv cache dir)\n\n# Backup tool installations\nuv tool list &gt; installed-tools.txt\n</code></pre>"},{"location":"docs/troubleshooting-uv-installation/#getting-help","title":"Getting Help","text":"<p>If you continue to experience issues:</p> <ol> <li>Check ASH Documentation: Review the main documentation for configuration examples</li> <li>Enable Debug Logging: Use <code>ASH_LOG_LEVEL=DEBUG</code> for detailed error information</li> <li>Check UV Documentation: Visit UV documentation for UV-specific issues</li> <li>Report Issues: Create an issue with:</li> <li>ASH version</li> <li>UV version</li> <li>Operating system</li> <li>Complete error logs</li> <li>Configuration files (sanitized)</li> </ol>"},{"location":"docs/troubleshooting-uv-installation/#useful-commands-for-issue-reports","title":"Useful Commands for Issue Reports","text":"<pre><code># System information\nuname -a\npython --version\nuv --version\n\n# ASH information\nash --version\nash --list-scanners\n\n# Configuration\ncat .ash/ash.yaml\n\n# Recent logs (sanitize sensitive information)\ntail -100 ash.log\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/","title":"UV Tool Installation Guide","text":"<p>This guide covers the enhanced UV tool installation functionality in ASH (Automated Security Helper), which provides explicit tool installation for Python-based security scanners.</p>"},{"location":"docs/uv-tool-installation-guide/#overview","title":"Overview","text":"<p>ASH now supports explicit UV tool installation for Python-based security scanners including:</p> <ul> <li>Bandit - Python source code security analyzer</li> <li>Checkov - Infrastructure as Code (IaC) security scanner</li> <li>Semgrep - Static analysis tool for finding bugs and security issues</li> </ul> <p>The enhanced installation system provides:</p> <ul> <li>Explicit Installation: Tools are installed upfront rather than on-demand</li> <li>Version Control: Specify exact version constraints for reproducible scans</li> <li>Offline Mode Support: Works with cached tools and pre-installed dependencies</li> <li>Enhanced Error Handling: Comprehensive logging and fallback mechanisms</li> <li>Performance Optimization: Leverages UV's caching for faster subsequent installations</li> </ul>"},{"location":"docs/uv-tool-installation-guide/#configuration-options","title":"Configuration Options","text":""},{"location":"docs/uv-tool-installation-guide/#bandit-scanner-configuration","title":"Bandit Scanner Configuration","text":"<pre><code>scanners:\n  bandit:\n    enabled: true\n    options:\n      tool_version: \"&gt;=1.7.0,&lt;2.0.0\"  # Version constraint for installation\n      install_timeout: 300             # Installation timeout in seconds\n      confidence_level: \"all\"\n      ignore_nosec: false\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#checkov-scanner-configuration","title":"Checkov Scanner Configuration","text":"<pre><code>scanners:\n  checkov:\n    enabled: true\n    options:\n      tool_version: \"&gt;=3.2.0,&lt;4.0.0\"  # Version constraint for installation\n      install_timeout: 300             # Installation timeout in seconds\n      frameworks: [\"all\"]\n      offline: false\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#semgrep-scanner-configuration","title":"Semgrep Scanner Configuration","text":"<pre><code>scanners:\n  semgrep:\n    enabled: true\n    options:\n      tool_version: \"&gt;=1.125.0\"        # Version constraint for installation\n      install_timeout: 300             # Installation timeout in seconds\n      config: \"auto\"\n      metrics: \"auto\"\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#installation-workflow","title":"Installation Workflow","text":""},{"location":"docs/uv-tool-installation-guide/#1-explicit-installation-process","title":"1. Explicit Installation Process","text":"<p>When a scanner is initialized, the following workflow occurs:</p> <ol> <li>UV Availability Check: Verify that UV is available on the system</li> <li>Tool Status Check: Check if the tool is already installed via UV</li> <li>Installation Attempt: If not installed, attempt explicit installation with version constraints</li> <li>Validation: Validate that the installed tool is functional</li> <li>Fallback: If installation fails, fall back to pre-installed tools or existing validation</li> </ol>"},{"location":"docs/uv-tool-installation-guide/#2-installation-commands","title":"2. Installation Commands","text":"<p>The system generates UV tool installation commands automatically:</p> <pre><code># Basic installation\nuv tool install bandit\n\n# With version constraint\nuv tool install \"bandit&gt;=1.7.0,&lt;2.0.0\"\n\n# With package extras (for enhanced functionality)\nuv tool install \"bandit[sarif,toml]&gt;=1.7.0,&lt;2.0.0\"\n\n# In offline mode\nuv tool install --offline \"bandit&gt;=1.7.0,&lt;2.0.0\"\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#version-constraints","title":"Version Constraints","text":""},{"location":"docs/uv-tool-installation-guide/#supported-version-constraint-formats","title":"Supported Version Constraint Formats","text":"<pre><code># Exact version\ntool_version: \"==1.7.5\"\n\n# Minimum version\ntool_version: \"&gt;=1.7.0\"\n\n# Version range\ntool_version: \"&gt;=1.7.0,&lt;2.0.0\"\n\n# Compatible release\ntool_version: \"~=1.7.0\"\n\n# Latest version (default)\ntool_version: null\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#default-version-constraints","title":"Default Version Constraints","text":"<p>Each scanner has sensible default version constraints:</p> <ul> <li>Bandit: <code>&gt;=1.7.0,&lt;2.0.0</code> (SARIF and TOML support)</li> <li>Checkov: <code>&gt;=3.2.0,&lt;4.0.0</code> (Improved stability)</li> <li>Semgrep: <code>&gt;=1.125.0</code> (Enhanced performance)</li> </ul>"},{"location":"docs/uv-tool-installation-guide/#offline-mode-support","title":"Offline Mode Support","text":""},{"location":"docs/uv-tool-installation-guide/#enabling-offline-mode","title":"Enabling Offline Mode","text":"<p>Set the <code>ASH_OFFLINE</code> environment variable:</p> <pre><code>export ASH_OFFLINE=true\nash --mode local\n</code></pre> <p>Or configure in scanner options:</p> <pre><code>scanners:\n  checkov:\n    options:\n      offline: true\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#offline-mode-behavior","title":"Offline Mode Behavior","text":"<p>In offline mode:</p> <ol> <li>Installation Skipping: UV tool installation attempts are skipped</li> <li>Pre-installed Detection: System searches for pre-installed tools</li> <li>Cache Usage: Leverages UV's local cache when available</li> <li>Clear Error Messages: Provides helpful guidance for offline limitations</li> </ol>"},{"location":"docs/uv-tool-installation-guide/#offline-mode-environment-variables","title":"Offline Mode Environment Variables","text":"<pre><code># Enable ASH offline mode\nexport ASH_OFFLINE=true\n\n# Enable UV offline mode (for tool execution)\nexport UV_OFFLINE=1\n\n# Specify Semgrep rules cache directory\nexport SEMGREP_RULES_CACHE_DIR=/path/to/cached/rules\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#error-handling-and-troubleshooting","title":"Error Handling and Troubleshooting","text":""},{"location":"docs/uv-tool-installation-guide/#common-installation-issues","title":"Common Installation Issues","text":""},{"location":"docs/uv-tool-installation-guide/#1-uv-not-available","title":"1. UV Not Available","text":"<p>Error: <code>UV tool validation failed - UV is not available but required</code></p> <p>Solution: <pre><code># Install UV using pip\npip install uv\n\n# Or using homebrew (macOS)\nbrew install uv\n\n# Or using the official installer\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p>"},{"location":"docs/uv-tool-installation-guide/#2-installation-timeout","title":"2. Installation Timeout","text":"<p>Error: <code>Tool installation timed out after 300 seconds</code></p> <p>Solutions: - Increase timeout in configuration:   <pre><code>scanners:\n  bandit:\n    options:\n      install_timeout: 600  # 10 minutes\n</code></pre> - Check network connectivity - Use offline mode with pre-installed tools</p>"},{"location":"docs/uv-tool-installation-guide/#3-version-conflicts","title":"3. Version Conflicts","text":"<p>Error: <code>Tool installation failed with exit code 1</code></p> <p>Solutions: - Check version constraint syntax - Verify tool version availability - Use broader version ranges:   <pre><code>tool_version: \"&gt;=1.7.0\"  # Instead of exact version\n</code></pre></p>"},{"location":"docs/uv-tool-installation-guide/#4-offline-mode-issues","title":"4. Offline Mode Issues","text":"<p>Error: <code>Offline mode is enabled but tool 'bandit' is not available</code></p> <p>Solutions: - Pre-install tools manually:   <pre><code>uv tool install bandit\n</code></pre> - Install tools system-wide:   <pre><code>pip install bandit checkov semgrep\n</code></pre> - Disable offline mode temporarily</p>"},{"location":"docs/uv-tool-installation-guide/#logging-and-diagnostics","title":"Logging and Diagnostics","text":"<p>The installation system provides structured logging with tags:</p> <ul> <li><code>[INSTALLATION_START]</code>: Installation initiation</li> <li><code>[INSTALLATION_PROGRESS]</code>: Progress updates</li> <li><code>[INSTALLATION_SUCCESS]</code>: Successful installation</li> <li><code>[INSTALLATION_FAILED]</code>: Installation failure</li> <li><code>[INSTALLATION_ERROR]</code>: Unexpected errors</li> <li><code>[INSTALLATION_SKIP]</code>: Installation skipped (offline mode, already installed)</li> </ul> <p>Enable debug logging for detailed diagnostics:</p> <pre><code>export ASH_LOG_LEVEL=DEBUG\nash --mode local\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#performance-optimization","title":"Performance Optimization","text":""},{"location":"docs/uv-tool-installation-guide/#caching-benefits","title":"Caching Benefits","text":"<p>UV's built-in caching provides significant performance improvements:</p> <ul> <li>First Installation: Downloads and caches tool and dependencies</li> <li>Subsequent Installations: Uses cached artifacts (50-90% faster)</li> <li>Cross-Project Sharing: Cache shared across different ASH projects</li> </ul>"},{"location":"docs/uv-tool-installation-guide/#cache-management","title":"Cache Management","text":"<pre><code># View cache information\nuv cache info\n\n# Clean cache if needed\nuv cache clean\n\n# Check cache size and location\nuv cache dir\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Version Ranges: Allows cache reuse across compatible versions</li> <li>Pre-install in CI: Install tools in CI setup phase for faster builds</li> <li>Shared Cache: Use shared cache directories in containerized environments</li> <li>Concurrent Scans: Installation system handles concurrent scanner initialization</li> </ol>"},{"location":"docs/uv-tool-installation-guide/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"docs/uv-tool-installation-guide/#custom-installation-commands","title":"Custom Installation Commands","text":"<p>For advanced use cases, you can define custom installation commands:</p> <pre><code># In custom scanner implementation\ndef _get_tool_version_constraint(self) -&gt; str:\n    return \"&gt;=1.7.0,&lt;2.0.0\"\n\ndef _get_tool_package_extras(self) -&gt; List[str]:\n    return [\"sarif\", \"toml\", \"yaml\"]\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#retry-configuration","title":"Retry Configuration","text":"<p>Configure installation retry behavior:</p> <pre><code>retry_config = {\n    \"max_retries\": 3,\n    \"base_delay\": 1.0,\n    \"max_delay\": 60.0,\n}\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#progress-monitoring","title":"Progress Monitoring","text":"<p>For long-running installations, progress monitoring is automatically enabled:</p> <pre><code>def progress_callback(message: str):\n    print(f\"Installation progress: {message}\")\n\n# Progress updates every 10 seconds for installations &gt; 60 seconds\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#migration-guide","title":"Migration Guide","text":""},{"location":"docs/uv-tool-installation-guide/#from-on-demand-to-explicit-installation","title":"From On-Demand to Explicit Installation","text":"<p>If you're upgrading from a previous version of ASH:</p> <ol> <li>No Configuration Changes Required: Explicit installation is enabled by default</li> <li>Optional Version Pinning: Add <code>tool_version</code> to scanner configurations for reproducibility</li> <li>Timeout Adjustment: Increase <code>install_timeout</code> if you experience timeout issues</li> <li>Offline Mode: Configure offline mode if running in air-gapped environments</li> </ol>"},{"location":"docs/uv-tool-installation-guide/#backward-compatibility","title":"Backward Compatibility","text":"<p>The enhanced installation system maintains full backward compatibility:</p> <ul> <li>Existing Configurations: Continue to work without changes</li> <li>Fallback Mechanisms: Falls back to pre-installed tools if UV installation fails</li> <li>Legacy Behavior: On-demand installation still available as fallback</li> </ul>"},{"location":"docs/uv-tool-installation-guide/#examples","title":"Examples","text":""},{"location":"docs/uv-tool-installation-guide/#basic-configuration","title":"Basic Configuration","text":"<pre><code># .ash/ash.yaml\nscanners:\n  bandit:\n    enabled: true\n  checkov:\n    enabled: true\n  semgrep:\n    enabled: true\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#advanced-configuration-with-version-control","title":"Advanced Configuration with Version Control","text":"<pre><code># .ash/ash.yaml\nscanners:\n  bandit:\n    enabled: true\n    options:\n      tool_version: \"&gt;=1.7.5,&lt;1.8.0\"\n      install_timeout: 600\n      confidence_level: \"medium\"\n\n  checkov:\n    enabled: true\n    options:\n      tool_version: \"&gt;=3.2.5,&lt;3.3.0\"\n      install_timeout: 300\n      frameworks: [\"terraform\", \"cloudformation\"]\n\n  semgrep:\n    enabled: true\n    options:\n      tool_version: \"&gt;=1.125.0,&lt;2.0.0\"\n      install_timeout: 450\n      config: \"p/security-audit\"\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#offline-mode-configuration","title":"Offline Mode Configuration","text":"<pre><code># .ash/ash.yaml\nscanners:\n  checkov:\n    enabled: true\n    options:\n      offline: true\n      frameworks: [\"terraform\"]\n\n  semgrep:\n    enabled: true\n    options:\n      offline: true\n      config: \"p/ci\"  # Use cached rules\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># GitHub Actions example\n- name: Setup UV\n  run: |\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    echo \"$HOME/.cargo/bin\" &gt;&gt; $GITHUB_PATH\n\n- name: Pre-install Security Tools\n  run: |\n    uv tool install \"bandit&gt;=1.7.0\"\n    uv tool install \"checkov&gt;=3.2.0\"\n    uv tool install \"semgrep&gt;=1.125.0\"\n\n- name: Run ASH Security Scan\n  run: |\n    ash --mode local\n</code></pre>"},{"location":"docs/uv-tool-installation-guide/#best-practices","title":"Best Practices","text":""},{"location":"docs/uv-tool-installation-guide/#1-version-management","title":"1. Version Management","text":"<ul> <li>Pin Major Versions: Use ranges like <code>&gt;=1.7.0,&lt;2.0.0</code> to avoid breaking changes</li> <li>Regular Updates: Periodically update version constraints for security patches</li> <li>Testing: Test version updates in development before production deployment</li> </ul>"},{"location":"docs/uv-tool-installation-guide/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>Pre-installation: Install tools during environment setup rather than scan time</li> <li>Cache Sharing: Use shared UV cache directories in containerized environments</li> <li>Concurrent Scans: Leverage concurrent scanner initialization for faster startup</li> </ul>"},{"location":"docs/uv-tool-installation-guide/#3-error-handling","title":"3. Error Handling","text":"<ul> <li>Timeout Configuration: Set appropriate timeouts based on network conditions</li> <li>Fallback Planning: Ensure pre-installed tools are available as fallbacks</li> <li>Monitoring: Monitor installation success rates and adjust configurations</li> </ul>"},{"location":"docs/uv-tool-installation-guide/#4-security-considerations","title":"4. Security Considerations","text":"<ul> <li>Version Pinning: Pin tool versions for reproducible security scans</li> <li>Offline Mode: Use offline mode in secure environments with limited network access</li> <li>Tool Verification: Regularly validate that installed tools are functional</li> </ul>"},{"location":"docs/uv-tool-installation-guide/#troubleshooting-checklist","title":"Troubleshooting Checklist","text":"<p>When experiencing installation issues:</p> <ol> <li>Check UV Installation: <code>uv --version</code></li> <li>Verify Network Connectivity: Can reach PyPI and tool repositories</li> <li>Check Disk Space: Ensure sufficient space for tool installation and cache</li> <li>Review Logs: Enable debug logging for detailed error information</li> <li>Test Manual Installation: Try installing tools manually with UV</li> <li>Check Version Constraints: Verify version constraint syntax and availability</li> <li>Consider Offline Mode: Use offline mode with pre-installed tools if needed</li> <li>Update Configuration: Adjust timeouts and retry settings as needed</li> </ol>"},{"location":"docs/plugins/","title":"Plugin Development Guide","text":"<p>ASH v3 features a flexible plugin architecture that allows you to extend its functionality through custom plugins. This guide provides an overview of the plugin system and how to develop your own plugins.</p>"},{"location":"docs/plugins/#built-in-plugins","title":"Built-in Plugins","text":"<p>ASH ships with a comprehensive set of built-in plugins that provide core functionality:</p> <ul> <li>Built-in Plugins Overview: Complete guide to all built-in plugins</li> <li>Security Scanners: 10 built-in security scanners (Bandit, Semgrep, Checkov, etc.)</li> <li>Report Formats: 12 output formats (SARIF, HTML, CSV, etc.)</li> <li>File Converters: Archive extraction and Jupyter notebook processing</li> <li>Event Handlers: Scan lifecycle event handling</li> </ul>"},{"location":"docs/plugins/#plugin-types","title":"Plugin Types","text":"<p>ASH supports three types of plugins:</p> <ol> <li>Scanners: Perform security scans on files and generate findings</li> <li>Reporters: Generate reports from scan results in various formats</li> <li>Converters: Transform files before scanning (e.g., convert Jupyter notebooks to Python)</li> </ol>"},{"location":"docs/plugins/#plugin-architecture","title":"Plugin Architecture","text":"<p>ASH plugins are Python classes that inherit from base plugin classes and are registered using decorators. The plugin system is designed to be:</p> <ul> <li>Modular: Each plugin has a specific responsibility</li> <li>Configurable: Plugins can be configured via YAML configuration</li> <li>Discoverable: Plugins are automatically discovered and loaded</li> <li>Extensible: New plugin types can be added in the future</li> </ul>"},{"location":"docs/plugins/#getting-started","title":"Getting Started","text":"<p>To create a custom plugin:</p> <ol> <li>Create a Python module with your plugin implementation</li> <li>Register your plugin using the appropriate decorator</li> <li>Add your plugin module to the ASH configuration</li> </ol> <p>For detailed instructions and examples, see the specific plugin type documentation:</p> <ul> <li>Scanner Plugins</li> <li>Reporter Plugins</li> <li>Converter Plugins</li> <li>Plugin Best Practices</li> </ul>"},{"location":"docs/plugins/#plugin-module-structure","title":"Plugin Module Structure","text":"<p>A typical plugin module has the following structure:</p> <pre><code>my_ash_plugins/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 converters.py\n\u251c\u2500\u2500 scanners.py\n\u2514\u2500\u2500 reporters.py\n</code></pre> <p>The <code>__init__.py</code> file should register your plugins for discovery:</p> <pre><code># my_ash_plugins/__init__.py\nfrom my_ash_plugins.scanners import MyCustomScanner\nfrom my_ash_plugins.reporters import MyCustomReporter\nfrom my_ash_plugins.converters import MyCustomConverter\n\n# Make plugins discoverable\nASH_SCANNERS = [MyCustomScanner]\nASH_REPORTERS = [MyCustomReporter]\nASH_CONVERTERS = [MyCustomConverter]\n</code></pre>"},{"location":"docs/plugins/#using-custom-plugins","title":"Using Custom Plugins","text":"<p>Add your custom plugin module to the ASH configuration:</p> <pre><code># .ash/.ash.yaml\nash_plugin_modules:\n  - my_ash_plugins\n</code></pre> <p>Or specify it on the command line:</p> <pre><code>ash --ash-plugin-modules my_ash_plugins\n</code></pre>"},{"location":"docs/plugins/#real-world-examples","title":"Real-World Examples","text":"<p>ASH includes several built-in plugins that you can use as examples:</p> <ul> <li>Scanner Examples: Bandit, Semgrep, Checkov</li> <li>Reporter Examples: Markdown, HTML, JSON, S3, BedrockSummary</li> <li>Converter Examples: Jupyter, Archive</li> </ul> <p>You can find these plugins in the ASH source code:</p> <ul> <li>Scanners: <code>automated_security_helper/plugins/scanners/</code></li> <li>Reporters: <code>automated_security_helper/plugins/reporters/</code></li> <li>Converters: <code>automated_security_helper/plugins/converters/</code></li> </ul>"},{"location":"docs/plugins/#next-steps","title":"Next Steps","text":"<ul> <li>Review the ASH Plugin Architecture</li> <li>Learn how to create Scanner Plugins</li> <li>Learn how to create Reporter Plugins</li> <li>Learn how to create Converter Plugins</li> <li>Review Plugin Best Practices</li> </ul>"},{"location":"docs/plugins/architecture/","title":"ASH Plugin Architecture","text":"<p>This document provides visual diagrams of the ASH plugin architecture, workflows, and relationships.</p> <p>For more detailed diagrams specific to each plugin type, see: - Scanner Plugin Diagrams - Reporter Plugin Diagrams - Converter Plugin Diagrams - Event Subscriber Diagrams</p>"},{"location":"docs/plugins/architecture/#plugin-architecture-overview","title":"Plugin Architecture Overview","text":"<p>The following diagram shows the high-level architecture of the ASH plugin system:</p> <pre><code>graph TD\n    A[ASH Core] --&gt; B[Plugin Manager]\n    B --&gt; C[Scanner Plugins]\n    B --&gt; D[Reporter Plugins]\n    B --&gt; E[Converter Plugins]\n    B --&gt; F[Event Subscribers]\n\n    C --&gt; G[Scan Results]\n    E --&gt; H[Converted Files]\n    G --&gt; D\n    H --&gt; C\n\n    subgraph \"Plugin Types\"\n        C\n        D\n        E\n        F\n    end\n\n    subgraph \"Data Flow\"\n        G\n        H\n    end\n\n</code></pre>"},{"location":"docs/plugins/architecture/#plugin-lifecycle","title":"Plugin Lifecycle","text":"<p>The following diagram shows the lifecycle of plugins during an ASH scan:</p> <pre><code>sequenceDiagram\n    participant User\n    participant ASH as ASH Core\n    participant PM as Plugin Manager\n    participant CP as Converter Plugins\n    participant SP as Scanner Plugins\n    participant ES as Event Subscribers\n    participant RP as Reporter Plugins\n\n    User-&gt;&gt;ASH: Start Scan\n    ASH-&gt;&gt;PM: Load Plugins\n    PM-&gt;&gt;CP: Initialize\n    PM-&gt;&gt;SP: Initialize\n    PM-&gt;&gt;ES: Initialize\n    PM-&gt;&gt;RP: Initialize\n\n    ASH-&gt;&gt;ES: Emit ScanStarted Event\n    ES--&gt;&gt;ASH: Handle Event\n\n    ASH-&gt;&gt;CP: Convert Files\n    CP--&gt;&gt;ASH: Converted Files\n\n    ASH-&gt;&gt;ES: Emit ConversionCompleted Event\n    ES--&gt;&gt;ASH: Handle Event\n\n    ASH-&gt;&gt;SP: Scan Files\n    SP--&gt;&gt;ASH: Scan Results\n\n    ASH-&gt;&gt;ES: Emit ScanCompleted Event\n    ES--&gt;&gt;ASH: Handle Event\n\n    ASH-&gt;&gt;RP: Generate Reports\n    RP--&gt;&gt;ASH: Reports\n\n    ASH-&gt;&gt;ES: Emit ReportingCompleted Event\n    ES--&gt;&gt;ASH: Handle Event\n\n    ASH--&gt;&gt;User: Scan Complete\n\n</code></pre>"},{"location":"docs/plugins/architecture/#data-flow-between-plugins","title":"Data Flow Between Plugins","text":"<p>The following diagram shows the data flow between different types of plugins:</p> <pre><code>flowchart LR\n    A[Source Files] --&gt; B[Converter Plugins]\n    B --&gt; C[Converted Files]\n    C --&gt; D[Scanner Plugins]\n    A --&gt; D\n    D --&gt; E[Scan Results]\n    E --&gt; F[Reporter Plugins]\n    F --&gt; G[Reports]\n\n    H[Event Subscribers] -.-&gt; B\n    H -.-&gt; D\n    H -.-&gt; F\n\n    subgraph \"Input\"\n        A\n    end\n\n    subgraph \"Processing\"\n        B\n        C\n        D\n        E\n        H\n    end\n\n    subgraph \"Output\"\n        F\n        G\n    end\n\n</code></pre>"},{"location":"docs/plugins/architecture/#plugin-registration-and-discovery","title":"Plugin Registration and Discovery","text":"<p>The following diagram shows how plugins are registered and discovered:</p> <pre><code>graph TD\n    A[Plugin Module] --&gt; B[__init__.py]\n    B --&gt; C[ASH_SCANNERS]\n    B --&gt; D[ASH_REPORTERS]\n    B --&gt; E[ASH_CONVERTERS]\n    B --&gt; F[ASH_EVENT_SUBSCRIBERS]\n\n    G[ASH Configuration] --&gt; H[ash_plugin_modules]\n    H --&gt; I[Plugin Manager]\n\n    C --&gt; I\n    D --&gt; I\n    E --&gt; I\n    F --&gt; I\n\n    I --&gt; J[Load Plugins]\n    J --&gt; K[Initialize Plugins]\n    K --&gt; L[Execute Plugins]\n\n</code></pre>"},{"location":"docs/plugins/architecture/#plugin-configuration-flow","title":"Plugin Configuration Flow","text":"<p>The following diagram shows how plugin configuration flows through the system:</p> <pre><code>graph TD\n    A[.ash/.ash.yaml] --&gt; B[Configuration Parser]\n    B --&gt; C[Global Configuration]\n    B --&gt; D[Scanner Configuration]\n    B --&gt; E[Reporter Configuration]\n    B --&gt; F[Converter Configuration]\n\n    C --&gt; G[Plugin Manager]\n    D --&gt; G\n    E --&gt; G\n    F --&gt; G\n\n    G --&gt; H[Scanner Plugins]\n    G --&gt; I[Reporter Plugins]\n    G --&gt; J[Converter Plugins]\n\n    K[CLI Overrides] --&gt; B\n\n</code></pre>"},{"location":"docs/plugins/architecture/#event-system","title":"Event System","text":"<p>The following diagram shows the event system in ASH:</p> <pre><code>graph TD\n    A[ASH Core] --&gt; B[Event Emitter]\n    B --&gt; C[ScanStarted]\n    B --&gt; D[ConversionStarted]\n    B --&gt; E[ConversionCompleted]\n    B --&gt; F[ScannerStarted]\n    B --&gt; G[ScannerCompleted]\n    B --&gt; H[ScanCompleted]\n    B --&gt; I[ReportingStarted]\n    B --&gt; J[ReportingCompleted]\n\n    C --&gt; K[Event Subscribers]\n    D --&gt; K\n    E --&gt; K\n    F --&gt; K\n    G --&gt; K\n    H --&gt; K\n    I --&gt; K\n    J --&gt; K\n\n    K --&gt; L[Custom Actions]\n\n</code></pre>"},{"location":"docs/plugins/converter-plugins-diagrams/","title":"Converter Plugin Diagrams","text":"<p>This document provides visual diagrams of the ASH converter plugin architecture using Mermaid.</p>"},{"location":"docs/plugins/converter-plugins-diagrams/#converter-plugin-lifecycle","title":"Converter Plugin Lifecycle","text":"<p>The following diagram shows the lifecycle of a converter plugin during an ASH scan:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant PM as Plugin Manager\n    participant CP as Converter Plugin\n    participant FS as File System\n    participant ES as Event System\n\n    ASH-&gt;&gt;PM: Load Converter Plugins\n    PM-&gt;&gt;CP: Initialize\n    CP--&gt;&gt;PM: Return Initialized Plugin\n\n    ASH-&gt;&gt;ES: Emit ConversionStarted Event\n\n    ASH-&gt;&gt;CP: Validate Converter\n    CP--&gt;&gt;ASH: Return Validation Status\n\n    ASH-&gt;&gt;CP: convert(target)\n    CP-&gt;&gt;FS: Read Source Files\n    FS--&gt;&gt;CP: Return File Contents\n\n    CP-&gt;&gt;CP: Process Files\n    Note over CP: Transform File Content\n\n    CP-&gt;&gt;FS: Write Converted Files\n    FS--&gt;&gt;CP: Files Written\n\n    CP--&gt;&gt;ASH: Return Converted Path\n\n    ASH-&gt;&gt;ES: Emit ConversionCompleted Event\n</code></pre>"},{"location":"docs/plugins/converter-plugins-diagrams/#converter-plugin-data-flow","title":"Converter Plugin Data Flow","text":"<p>The following diagram shows the data flow through a converter plugin:</p> <pre><code>flowchart LR\n    A[Source Files] --&gt; B[Converter Plugin]\n\n    subgraph Converter Plugin\n        C[File Reader] --&gt; D[Content Transformer]\n        D --&gt; E[File Writer]\n    end\n\n    B --&gt; F[Converted Files]\n    F --&gt; G[Scanner Plugins]\n</code></pre>"},{"location":"docs/plugins/converter-plugins-diagrams/#converter-plugin-class-hierarchy","title":"Converter Plugin Class Hierarchy","text":"<p>The following diagram shows the class hierarchy for converter plugins:</p> <pre><code>classDiagram\n    class PluginBase {\n        +context: PluginContext\n        +config: Any\n        +validate_plugin_dependencies() bool\n        +model_post_init(context)\n        #_plugin_log(message, level, target_type, append_to_stream)\n        #_run_subprocess(cmd, stdout_preference, stderr_preference)\n    }\n\n    class ConverterPluginBase {\n        +convert(target) Path\n        +converted_dir: Path\n    }\n\n    class ConverterPluginConfigBase {\n        +name: str\n        +enabled: bool\n        +options: ConverterOptionsBase\n    }\n\n    class ConverterOptionsBase {\n        +file_extensions: List[str]\n        +preserve_line_numbers: bool\n    }\n\n    class CustomConverter {\n        +convert(target) Path\n    }\n\n    PluginBase &lt;|-- ConverterPluginBase\n    ConverterPluginBase &lt;|-- CustomConverter\n    ConverterPluginConfigBase -- CustomConverter : configures\n    ConverterOptionsBase -- ConverterPluginConfigBase : contains\n</code></pre>"},{"location":"docs/plugins/converter-plugins-diagrams/#converter-plugin-configuration-flow","title":"Converter Plugin Configuration Flow","text":"<p>The following diagram shows how configuration flows through a converter plugin:</p> <pre><code>flowchart TD\n    A[.ash/.ash.yaml] --&gt; B[Configuration Parser]\n    C[CLI Arguments] --&gt; B\n    B --&gt; D[ASH Configuration]\n    D --&gt; E[Converter Configuration]\n    E --&gt; F[Converter Plugin]\n\n    subgraph Converter Plugin\n        G[Validate Config] --&gt; H[Apply Config]\n        H --&gt; I[Use in Convert Logic]\n    end\n\n    F --&gt; J[Converted Files]\n</code></pre>"},{"location":"docs/plugins/converter-plugins-diagrams/#file-transformation-process","title":"File Transformation Process","text":"<p>The following diagram shows the file transformation process in a converter plugin:</p> <pre><code>flowchart LR\n    A[Source File] --&gt; B[Converter Plugin]\n\n    subgraph Converter Plugin\n        C[Parse File] --&gt; D[Transform Content]\n        D --&gt; E[Preserve Line Numbers]\n        E --&gt; F[Generate Output]\n    end\n\n    B --&gt; G[Converted File]\n\n    H[Source Line Mapping] -.-&gt; B\n    B -.-&gt; I[Target Line Mapping]\n\n    J[Original Line Numbers] -.-&gt; H\n    I -.-&gt; K[Converted Line Numbers]\n</code></pre>"},{"location":"docs/plugins/converter-plugins-diagrams/#converter-integration-with-ash-core","title":"Converter Integration with ASH Core","text":"<p>The following diagram shows how converter plugins integrate with the ASH core:</p> <pre><code>flowchart TD\n    A[ASH CLI] --&gt; B[ASH Core]\n    B --&gt; C[Plugin Manager]\n    C --&gt; D[Converter Registry]\n    D --&gt; E[Converter Plugins]\n\n    E --&gt; F[Converted Files]\n    F --&gt; G[Scanner Plugins]\n    G --&gt; H[Scan Results]\n\n    I[Event System] -.-&gt; E\n    I -.-&gt; G\n</code></pre>"},{"location":"docs/plugins/converter-plugins/","title":"Converter Plugins","text":"<p>Converter plugins transform files before scanning to make them compatible with security scanners. For example, converting Jupyter notebooks to Python files.</p> <p>For detailed visual diagrams of converter plugin architecture and workflow, see Converter Plugin Diagrams.</p>"},{"location":"docs/plugins/converter-plugins/#converter-plugin-interface","title":"Converter Plugin Interface","text":"<p>Converter plugins must implement the <code>ConverterPluginBase</code> interface:</p> <pre><code>from automated_security_helper.base.converter_plugin import ConverterPluginBase, ConverterPluginConfigBase\nfrom automated_security_helper.plugins.decorators import ash_converter_plugin\n\n@ash_converter_plugin\nclass MyConverter(ConverterPluginBase):\n    \"\"\"My custom converter implementation\"\"\"\n\n    def convert(self, target):\n        \"\"\"Convert the target file or directory\"\"\"\n        # Your code here\n</code></pre>"},{"location":"docs/plugins/converter-plugins/#converter-plugin-configuration","title":"Converter Plugin Configuration","text":"<p>Define a configuration class for your converter:</p> <pre><code>from typing import List\nfrom pydantic import Field\n\nclass MyConverterConfig(ConverterPluginConfigBase):\n    name: str = \"my-converter\"\n    enabled: bool = True\n\n    class Options:\n        file_extensions: List[str] = Field(default=[\".ipynb\"], description=\"File extensions to convert\")\n        preserve_line_numbers: bool = Field(default=True, description=\"Preserve line numbers in converted files\")\n</code></pre>"},{"location":"docs/plugins/converter-plugins/#converter-plugin-example","title":"Converter Plugin Example","text":"<p>Here's a complete example of a custom converter plugin:</p> <pre><code>import json\nimport os\nfrom pathlib import Path\nfrom typing import List\n\nfrom pydantic import Field\n\nfrom automated_security_helper.base.converter_plugin import ConverterPluginBase, ConverterPluginConfigBase\nfrom automated_security_helper.plugins.decorators import ash_converter_plugin\n\nclass JupyterConverterConfig(ConverterPluginConfigBase):\n    \"\"\"Configuration for JupyterConverter\"\"\"\n    name: str = \"jupyter\"\n    enabled: bool = True\n\n    class Options:\n        file_extensions: List[str] = Field(default=[\".ipynb\"], description=\"File extensions to convert\")\n        preserve_line_numbers: bool = Field(default=True, description=\"Preserve line numbers in converted files\")\n        include_markdown: bool = Field(default=False, description=\"Include markdown cells in output\")\n\n@ash_converter_plugin\nclass JupyterConverter(ConverterPluginBase):\n    \"\"\"Converts Jupyter notebooks to Python files\"\"\"\n\n    def convert(self, target: Path):\n        \"\"\"Convert Jupyter notebooks to Python files\"\"\"\n        if target.is_file():\n            if target.suffix in self.config.options.file_extensions:\n                return self._convert_file(target)\n            return None\n\n        # Process directory\n        converted_dir = self.converted_dir / target.name\n        converted_dir.mkdir(parents=True, exist_ok=True)\n\n        # Find all notebook files\n        notebook_files = []\n        for ext in self.config.options.file_extensions:\n            notebook_files.extend(target.glob(f\"**/*{ext}\"))\n\n        # Convert each notebook\n        for notebook_file in notebook_files:\n            rel_path = notebook_file.relative_to(target)\n            output_path = converted_dir / rel_path.with_suffix(\".py\")\n            output_path.parent.mkdir(parents=True, exist_ok=True)\n\n            try:\n                self._convert_notebook(notebook_file, output_path)\n            except Exception as e:\n                self._plugin_log(\n                    f\"Error converting {notebook_file}: {str(e)}\",\n                    level=\"ERROR\",\n                    append_to_stream=\"stderr\",\n                )\n\n        return converted_dir\n\n    def _convert_file(self, file_path: Path):\n        \"\"\"Convert a single notebook file\"\"\"\n        output_path = self.converted_dir / file_path.with_suffix(\".py\").name\n        self.converted_dir.mkdir(parents=True, exist_ok=True)\n\n        try:\n            self._convert_notebook(file_path, output_path)\n            return output_path\n        except Exception as e:\n            self._plugin_log(\n                f\"Error converting {file_path}: {str(e)}\",\n                level=\"ERROR\",\n                append_to_stream=\"stderr\",\n            )\n            return None\n\n    def _convert_notebook(self, input_path: Path, output_path: Path):\n        \"\"\"Convert a notebook to a Python file\"\"\"\n        try:\n            with open(input_path, \"r\", encoding=\"utf-8\") as f:\n                notebook = json.load(f)\n\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(f\"# Converted from {input_path.name}\\n\\n\")\n\n                cell_count = 0\n                for cell in notebook.get(\"cells\", []):\n                    cell_type = cell.get(\"cell_type\")\n                    source = cell.get(\"source\", [])\n\n                    # Skip non-code cells if not including markdown\n                    if cell_type != \"code\" and not self.config.options.include_markdown:\n                        continue\n\n                    # Join source lines\n                    if isinstance(source, list):\n                        source = \"\".join(source)\n\n                    # Add cell marker\n                    cell_count += 1\n                    f.write(f\"# Cell {cell_count} ({cell_type})\\n\")\n\n                    # Write content\n                    if cell_type == \"code\":\n                        f.write(source)\n                    else:\n                        # Comment out markdown\n                        for line in source.split(\"\\n\"):\n                            f.write(f\"# {line}\\n\")\n\n                    f.write(\"\\n\\n\")\n\n            return output_path\n        except Exception as e:\n            raise Exception(f\"Failed to convert notebook: {str(e)}\")\n</code></pre>"},{"location":"docs/plugins/converter-plugins/#converter-plugin-best-practices","title":"Converter Plugin Best Practices","text":"<ol> <li>Preserve Line Numbers: Try to preserve line numbers for better mapping of findings</li> <li>Handle Directories: Support converting both individual files and directories</li> <li>Error Handling: Use try/except blocks to handle errors</li> <li>Logging: Use the <code>_plugin_log</code> method for logging</li> <li>Return Paths: Return the path to the converted file or directory</li> </ol>"},{"location":"docs/plugins/converter-plugins/#converter-plugin-configuration-in-ash","title":"Converter Plugin Configuration in ASH","text":"<p>Configure your converter in the ASH configuration file:</p> <pre><code># .ash/.ash.yaml\nconverters:\n  jupyter:\n    enabled: true\n    options:\n      file_extensions: [\".ipynb\"]\n      preserve_line_numbers: true\n      include_markdown: false\n</code></pre>"},{"location":"docs/plugins/converter-plugins/#testing-converter-plugins","title":"Testing Converter Plugins","text":"<p>Create unit tests for your converter:</p> <pre><code>import pytest\nfrom pathlib import Path\n\nfrom automated_security_helper.base.plugin_context import PluginContext\nfrom my_ash_plugins.converters import JupyterConverter\n\ndef test_jupyter_converter():\n    # Create a plugin context\n    context = PluginContext(\n        source_dir=Path(\"test_data\"),\n        output_dir=Path(\"test_output\"),\n        converted_dir=Path(\"test_output/converted\")\n    )\n\n    # Create converter instance\n    converter = JupyterConverter(context=context)\n\n    # Create a test notebook\n    notebook_path = Path(\"test_data/test.ipynb\")\n    with open(notebook_path, \"w\") as f:\n        f.write('{\"cells\": [{\"cell_type\": \"code\", \"source\": [\"print(\\\\\"Hello, world!\\\\\")\\\\n\"]}]}')\n\n    # Convert the notebook\n    converted_path = converter.convert(notebook_path)\n\n    # Assert conversion\n    assert converted_path is not None\n    assert converted_path.exists()\n    with open(converted_path, \"r\") as f:\n        content = f.read()\n        assert \"print(\\\"Hello, world!\\\")\" in content\n</code></pre>"},{"location":"docs/plugins/development-guide/","title":"Plugin Development Guide","text":"<p>This guide provides comprehensive information on developing custom plugins for ASH. Whether you're creating scanners, reporters, converters, or event subscribers, this document will help you understand the plugin architecture and development process.</p>"},{"location":"docs/plugins/development-guide/#plugin-architecture-overview","title":"Plugin Architecture Overview","text":"<p>ASH's plugin system is designed to be extensible and modular. Plugins are Python classes that inherit from base plugin classes and implement specific interfaces.</p>"},{"location":"docs/plugins/development-guide/#plugin-types","title":"Plugin Types","text":"<p>ASH supports four main types of plugins:</p> <ol> <li>Scanner Plugins: Analyze code and infrastructure for security issues</li> <li>Reporter Plugins: Generate reports in various formats</li> <li>Converter Plugins: Process files before scanning</li> <li>Event Subscribers: React to events during the scan lifecycle</li> </ol>"},{"location":"docs/plugins/development-guide/#plugin-lifecycle","title":"Plugin Lifecycle","text":"<p>All plugins follow a similar lifecycle:</p> <ol> <li>Registration: Plugins are registered with the plugin manager</li> <li>Configuration: Plugin settings are loaded from the ASH configuration</li> <li>Initialization: Plugins are initialized with required dependencies</li> <li>Execution: Plugins perform their specific tasks</li> <li>Cleanup: Plugins clean up resources when done</li> </ol>"},{"location":"docs/plugins/development-guide/#creating-a-custom-plugin","title":"Creating a Custom Plugin","text":""},{"location":"docs/plugins/development-guide/#basic-plugin-structure","title":"Basic Plugin Structure","text":"<p>All plugins follow a similar structure:</p> <pre><code>from automated_security_helper.base.scanner_plugin import ScannerPluginBase, ScannerPluginConfigBase\n\nclass MyCustomScannerConfig(ScannerPluginConfigBase):\n    \"\"\"Configuration for MyCustomScanner.\"\"\"\n\n    # Define configuration options\n    custom_option: str = \"default_value\"\n\nclass MyCustomScanner(ScannerPluginBase):\n    \"\"\"Custom scanner implementation.\"\"\"\n\n    def __init__(self, config: MyCustomScannerConfig):\n        super().__init__(config)\n        # Initialize scanner-specific resources\n\n    def scan(self, target_path: str) -&gt; dict:\n        \"\"\"Perform the scan operation.\"\"\"\n        # Implement scanning logic\n        return {\n            \"findings\": [],\n            \"status\": \"success\"\n        }\n\n    def cleanup(self):\n        \"\"\"Clean up resources.\"\"\"\n        # Implement cleanup logic\n</code></pre>"},{"location":"docs/plugins/development-guide/#plugin-registration","title":"Plugin Registration","text":"<p>Plugins must be registered with ASH to be discovered:</p> <pre><code>from automated_security_helper.plugins import ash_plugin_manager\n\n# Register your plugin\nash_plugin_manager.register_scanner(\n    name=\"my-custom-scanner\",\n    scanner_class=MyCustomScanner,\n    config_class=MyCustomScannerConfig\n)\n</code></pre>"},{"location":"docs/plugins/development-guide/#scanner-plugin-development","title":"Scanner Plugin Development","text":"<p>Scanner plugins analyze code and infrastructure for security issues.</p>"},{"location":"docs/plugins/development-guide/#scanner-plugin-interface","title":"Scanner Plugin Interface","text":"<pre><code>class ScannerPluginBase(ABC):\n    \"\"\"Base class for all scanner plugins.\"\"\"\n\n    @abstractmethod\n    def scan(self, target_path: str) -&gt; dict:\n        \"\"\"Scan the target path and return findings.\"\"\"\n        pass\n\n    @abstractmethod\n    def cleanup(self):\n        \"\"\"Clean up resources.\"\"\"\n        pass\n</code></pre>"},{"location":"docs/plugins/development-guide/#scanner-plugin-example","title":"Scanner Plugin Example","text":"<pre><code>from automated_security_helper.base.scanner_plugin import ScannerPluginBase, ScannerPluginConfigBase\nfrom automated_security_helper.core.enums import ScannerStatus\n\nclass CustomRegexScannerConfig(ScannerPluginConfigBase):\n    \"\"\"Configuration for CustomRegexScanner.\"\"\"\n\n    name: str = \"custom-regex\"\n    enabled: bool = True\n    patterns: List[str] = [\"password\\\\s*=\\\\s*['\\\"]([^'\\\"]+)['\\\"]\"]\n\nclass CustomRegexScanner(ScannerPluginBase):\n    \"\"\"Scanner that uses regex patterns to find security issues.\"\"\"\n\n    def __init__(self, config: CustomRegexScannerConfig):\n        super().__init__(config)\n        self.patterns = [re.compile(p) for p in config.patterns]\n\n    def scan(self, target_path: str) -&gt; dict:\n        \"\"\"Scan files for regex patterns.\"\"\"\n        findings = []\n\n        for file_path in self._get_files(target_path):\n            with open(file_path, 'r') as f:\n                content = f.read()\n\n            for i, line in enumerate(content.splitlines()):\n                for pattern in self.patterns:\n                    if match := pattern.search(line):\n                        findings.append({\n                            \"file\": file_path,\n                            \"line\": i + 1,\n                            \"pattern\": pattern.pattern,\n                            \"match\": match.group(0),\n                            \"severity\": \"HIGH\"\n                        })\n\n        return {\n            \"findings\": findings,\n            \"status\": ScannerStatus.FAILED if findings else ScannerStatus.PASSED\n        }\n\n    def cleanup(self):\n        \"\"\"Clean up resources.\"\"\"\n        self.patterns = []\n\n    def _get_files(self, path: str) -&gt; List[str]:\n        \"\"\"Get all files in the path.\"\"\"\n        if os.path.isfile(path):\n            return [path]\n\n        files = []\n        for root, _, filenames in os.walk(path):\n            for filename in filenames:\n                files.append(os.path.join(root, filename))\n\n        return files\n</code></pre>"},{"location":"docs/plugins/development-guide/#reporter-plugin-development","title":"Reporter Plugin Development","text":"<p>Reporter plugins generate reports in various formats.</p>"},{"location":"docs/plugins/development-guide/#reporter-plugin-interface","title":"Reporter Plugin Interface","text":"<pre><code>class ReporterPluginBase(ABC):\n    \"\"\"Base class for all reporter plugins.\"\"\"\n\n    @abstractmethod\n    def generate_report(self, results: AshAggregatedResults) -&gt; str:\n        \"\"\"Generate a report from the scan results.\"\"\"\n        pass\n</code></pre>"},{"location":"docs/plugins/development-guide/#reporter-plugin-example","title":"Reporter Plugin Example","text":"<pre><code>from automated_security_helper.base.reporter_plugin import ReporterPluginBase, ReporterPluginConfigBase\nfrom automated_security_helper.models.asharp_model import AshAggregatedResults\n\nclass CustomJSONReporterConfig(ReporterPluginConfigBase):\n    \"\"\"Configuration for CustomJSONReporter.\"\"\"\n\n    name: str = \"custom-json\"\n    enabled: bool = True\n    pretty_print: bool = True\n\nclass CustomJSONReporter(ReporterPluginBase):\n    \"\"\"Reporter that generates a custom JSON report.\"\"\"\n\n    def __init__(self, config: CustomJSONReporterConfig):\n        super().__init__(config)\n        self.pretty_print = config.pretty_print\n\n    def generate_report(self, results: AshAggregatedResults) -&gt; str:\n        \"\"\"Generate a custom JSON report.\"\"\"\n        report_data = {\n            \"project\": results.metadata.project_name,\n            \"timestamp\": results.metadata.generated_at,\n            \"summary\": {\n                \"total\": results.metadata.summary_stats.total,\n                \"critical\": results.metadata.summary_stats.critical,\n                \"high\": results.metadata.summary_stats.high,\n                \"medium\": results.metadata.summary_stats.medium,\n                \"low\": results.metadata.summary_stats.low,\n                \"info\": results.metadata.summary_stats.info,\n            },\n            \"findings\": []\n        }\n\n        # Extract findings from SARIF\n        if results.sarif and results.sarif.runs:\n            for run in results.sarif.runs:\n                if run.results:\n                    for result in run.results:\n                        finding = {\n                            \"rule_id\": result.ruleId,\n                            \"level\": result.level,\n                            \"message\": result.message.text if result.message else \"No message\",\n                        }\n                        report_data[\"findings\"].append(finding)\n\n        # Generate JSON\n        indent = 2 if self.pretty_print else None\n        return json.dumps(report_data, indent=indent)\n</code></pre>"},{"location":"docs/plugins/development-guide/#converter-plugin-development","title":"Converter Plugin Development","text":"<p>Converter plugins process files before scanning.</p>"},{"location":"docs/plugins/development-guide/#converter-plugin-interface","title":"Converter Plugin Interface","text":"<pre><code>class ConverterPluginBase(ABC):\n    \"\"\"Base class for all converter plugins.\"\"\"\n\n    @abstractmethod\n    def convert(self, source_path: str, target_path: str) -&gt; List[str]:\n        \"\"\"Convert files from source_path to target_path.\"\"\"\n        pass\n</code></pre>"},{"location":"docs/plugins/development-guide/#converter-plugin-example","title":"Converter Plugin Example","text":"<pre><code>from automated_security_helper.base.converter_plugin import ConverterPluginBase, ConverterPluginConfigBase\n\nclass CustomYAMLConverterConfig(ConverterPluginConfigBase):\n    \"\"\"Configuration for CustomYAMLConverter.\"\"\"\n\n    name: str = \"custom-yaml\"\n    enabled: bool = True\n    file_extensions: List[str] = [\".yaml\", \".yml\"]\n\nclass CustomYAMLConverter(ConverterPluginBase):\n    \"\"\"Converter that processes YAML files.\"\"\"\n\n    def __init__(self, config: CustomYAMLConverterConfig):\n        super().__init__(config)\n        self.file_extensions = config.file_extensions\n\n    def convert(self, source_path: str, target_path: str) -&gt; List[str]:\n        \"\"\"Convert YAML files to a format suitable for scanning.\"\"\"\n        converted_files = []\n\n        for root, _, files in os.walk(source_path):\n            for file in files:\n                if any(file.endswith(ext) for ext in self.file_extensions):\n                    source_file = os.path.join(root, file)\n                    rel_path = os.path.relpath(source_file, source_path)\n                    target_file = os.path.join(target_path, rel_path)\n\n                    # Create target directory if it doesn't exist\n                    os.makedirs(os.path.dirname(target_file), exist_ok=True)\n\n                    # Process the YAML file\n                    with open(source_file, 'r') as f:\n                        yaml_content = yaml.safe_load(f)\n\n                    # Write processed content to target file\n                    with open(target_file, 'w') as f:\n                        yaml.dump(yaml_content, f)\n\n                    converted_files.append(target_file)\n\n        return converted_files\n</code></pre>"},{"location":"docs/plugins/development-guide/#event-subscriber-development","title":"Event Subscriber Development","text":"<p>Event subscribers react to events during the scan lifecycle.</p>"},{"location":"docs/plugins/development-guide/#event-subscriber-interface","title":"Event Subscriber Interface","text":"<pre><code># Event types\nclass AshEventType(Enum):\n    SCAN_START = \"scan_start\"\n    SCAN_COMPLETE = \"scan_complete\"\n    CONVERT_START = \"convert_start\"\n    CONVERT_COMPLETE = \"convert_complete\"\n    REPORT_START = \"report_start\"\n    REPORT_COMPLETE = \"report_complete\"\n\n# Event subscriber function type\nEventSubscriberFunc = Callable[..., bool]\n</code></pre>"},{"location":"docs/plugins/development-guide/#event-subscriber-example","title":"Event Subscriber Example","text":"<pre><code>from automated_security_helper.plugins.events import AshEventType\nfrom automated_security_helper.plugins import ash_plugin_manager\n\ndef scan_complete_handler(**kwargs):\n    \"\"\"Handle scan completion events.\"\"\"\n    scanner = kwargs.get('scanner', 'unknown')\n    remaining = kwargs.get('remaining_count', 0)\n\n    print(f\"Scanner {scanner} completed. {remaining} scanners remaining.\")\n\n    # Return True to indicate successful handling\n    return True\n\n# Register the event subscriber\nash_plugin_manager.subscribe(AshEventType.SCAN_COMPLETE, scan_complete_handler)\n</code></pre>"},{"location":"docs/plugins/development-guide/#plugin-configuration","title":"Plugin Configuration","text":"<p>Plugins are configured through the ASH configuration file:</p> <pre><code># Custom scanner configuration\nscanners:\n  custom-regex:\n    enabled: true\n    patterns:\n      - \"password\\\\s*=\\\\s*['\\\"]([^'\\\"]+)['\\\"]\"\n      - \"api_key\\\\s*=\\\\s*['\\\"]([^'\\\"]+)['\\\"]\"\n\n# Custom reporter configuration\nreporters:\n  custom-json:\n    enabled: true\n    pretty_print: true\n\n# Custom converter configuration\nconverters:\n  custom-yaml:\n    enabled: true\n    file_extensions:\n      - \".yaml\"\n      - \".yml\"\n</code></pre>"},{"location":"docs/plugins/development-guide/#plugin-distribution","title":"Plugin Distribution","text":""},{"location":"docs/plugins/development-guide/#creating-a-plugin-package","title":"Creating a Plugin Package","text":"<p>To distribute your plugins, create a Python package:</p> <pre><code>my-ash-plugins/\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 my_ash_plugins/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 scanners.py\n\u2502   \u251c\u2500\u2500 reporters.py\n\u2502   \u2514\u2500\u2500 converters.py\n</code></pre>"},{"location":"docs/plugins/development-guide/#plugin-registration-in-package","title":"Plugin Registration in Package","text":"<p>Register your plugins in the <code>__init__.py</code> file:</p> <pre><code>from automated_security_helper.plugins import ash_plugin_manager\nfrom .scanners import CustomRegexScanner, CustomRegexScannerConfig\nfrom .reporters import CustomJSONReporter, CustomJSONReporterConfig\n\n# Register plugins\ndef register_plugins():\n    ash_plugin_manager.register_scanner(\n        name=\"custom-regex\",\n        scanner_class=CustomRegexScanner,\n        config_class=CustomRegexScannerConfig\n    )\n\n    ash_plugin_manager.register_reporter(\n        name=\"custom-json\",\n        reporter_class=CustomJSONReporter,\n        config_class=CustomJSONReporterConfig\n    )\n\n# Auto-register when imported\nregister_plugins()\n</code></pre>"},{"location":"docs/plugins/development-guide/#using-custom-plugin-packages","title":"Using Custom Plugin Packages","text":"<p>Configure ASH to use your custom plugin package:</p> <pre><code># ASH configuration\nash_plugin_modules:\n  - \"my_ash_plugins\"\n</code></pre> <p>Or specify via command line:</p> <pre><code>ash --plugin-modules my_ash_plugins\n</code></pre>"},{"location":"docs/plugins/development-guide/#best-practices","title":"Best Practices","text":""},{"location":"docs/plugins/development-guide/#plugin-design","title":"Plugin Design","text":"<ul> <li>Single Responsibility: Each plugin should do one thing well</li> <li>Configurability: Make plugins configurable for different use cases</li> <li>Error Handling: Handle errors gracefully and provide useful error messages</li> <li>Performance: Optimize for performance, especially for large codebases</li> <li>Documentation: Document your plugins thoroughly</li> </ul>"},{"location":"docs/plugins/development-guide/#testing-plugins","title":"Testing Plugins","text":"<p>Create tests for your plugins:</p> <pre><code>def test_custom_regex_scanner():\n    \"\"\"Test CustomRegexScanner.\"\"\"\n    # Create test files\n    with tempfile.TemporaryDirectory() as tmpdir:\n        test_file = os.path.join(tmpdir, \"test.py\")\n        with open(test_file, \"w\") as f:\n            f.write('password = \"secret123\"')\n\n        # Configure scanner\n        config = CustomRegexScannerConfig(\n            patterns=[\"password\\\\s*=\\\\s*['\\\"]([^'\\\"]+)['\\\"]\"]\n        )\n        scanner = CustomRegexScanner(config)\n\n        # Run scan\n        result = scanner.scan(tmpdir)\n\n        # Verify results\n        assert len(result[\"findings\"]) == 1\n        assert result[\"findings\"][0][\"file\"] == test_file\n        assert result[\"findings\"][0][\"line\"] == 1\n        assert result[\"findings\"][0][\"severity\"] == \"HIGH\"\n</code></pre>"},{"location":"docs/plugins/development-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/development-guide/#common-issues","title":"Common Issues","text":"<ul> <li>Plugin Not Found: Ensure your plugin module is in the Python path</li> <li>Configuration Errors: Validate your configuration against the plugin's schema</li> <li>Dependency Issues: Check that all required dependencies are installed</li> <li>Performance Problems: Profile your plugin to identify bottlenecks</li> </ul>"},{"location":"docs/plugins/development-guide/#debugging-plugins","title":"Debugging Plugins","text":"<p>Enable debug logging to troubleshoot plugin issues:</p> <pre><code>ash --debug\n</code></pre>"},{"location":"docs/plugins/development-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Scanner Plugin Guide: Detailed guide for scanner plugins</li> <li>Reporter Plugin Guide: Detailed guide for reporter plugins</li> <li>Converter Plugin Guide: Detailed guide for converter plugins</li> <li>Event Subscriber Guide: Detailed guide for event subscribers</li> <li>Plugin Best Practices: Best practices for plugin development</li> </ul>"},{"location":"docs/plugins/event-subscribers-diagrams/","title":"Event Subscriber Diagrams","text":"<p>This document provides visual diagrams of the ASH event system architecture using Mermaid.</p>"},{"location":"docs/plugins/event-subscribers-diagrams/#event-system-overview","title":"Event System Overview","text":"<p>The following diagram shows the high-level architecture of the ASH event system:</p> <pre><code>flowchart TD\n    A[ASH Core] --&gt; B[Event Emitter]\n    B --&gt; C[Event Registry]\n    C --&gt; D[Event Subscribers]\n\n    E[Scanner Plugins] -.-&gt; B\n    F[Converter Plugins] -.-&gt; B\n    G[Reporter Plugins] -.-&gt; B\n\n    D --&gt; H[Custom Actions]\n    D --&gt; I[Notifications]\n    D --&gt; J[Logging]\n    D --&gt; K[Metrics]\n    D --&gt; L[External Systems]\n</code></pre>"},{"location":"docs/plugins/event-subscribers-diagrams/#event-flow-sequence","title":"Event Flow Sequence","text":"<p>The following diagram shows the sequence of events during an ASH scan:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant EM as Event Manager\n    participant ES as Event Subscribers\n    participant CP as Converter Plugins\n    participant SP as Scanner Plugins\n    participant RP as Reporter Plugins\n\n    ASH-&gt;&gt;EM: Emit ScanStarted\n    EM-&gt;&gt;ES: Notify ScanStarted\n    ES--&gt;&gt;EM: Handle ScanStarted\n\n    ASH-&gt;&gt;EM: Emit ConversionStarted\n    EM-&gt;&gt;ES: Notify ConversionStarted\n    ES--&gt;&gt;EM: Handle ConversionStarted\n\n    ASH-&gt;&gt;CP: Convert Files\n    CP--&gt;&gt;ASH: Return Converted Files\n\n    ASH-&gt;&gt;EM: Emit ConversionCompleted\n    EM-&gt;&gt;ES: Notify ConversionCompleted\n    ES--&gt;&gt;EM: Handle ConversionCompleted\n\n    ASH-&gt;&gt;EM: Emit ScannerStarted\n    EM-&gt;&gt;ES: Notify ScannerStarted\n    ES--&gt;&gt;EM: Handle ScannerStarted\n\n    ASH-&gt;&gt;SP: Scan Files\n    SP--&gt;&gt;ASH: Return Scan Results\n\n    ASH-&gt;&gt;EM: Emit ScannerCompleted\n    EM-&gt;&gt;ES: Notify ScannerCompleted\n    ES--&gt;&gt;EM: Handle ScannerCompleted\n\n    ASH-&gt;&gt;EM: Emit ScanCompleted\n    EM-&gt;&gt;ES: Notify ScanCompleted\n    ES--&gt;&gt;EM: Handle ScanCompleted\n\n    ASH-&gt;&gt;EM: Emit ReportingStarted\n    EM-&gt;&gt;ES: Notify ReportingStarted\n    ES--&gt;&gt;EM: Handle ReportingStarted\n\n    ASH-&gt;&gt;RP: Generate Reports\n    RP--&gt;&gt;ASH: Return Reports\n\n    ASH-&gt;&gt;EM: Emit ReportingCompleted\n    EM-&gt;&gt;ES: Notify ReportingCompleted\n    ES--&gt;&gt;EM: Handle ReportingCompleted\n</code></pre>"},{"location":"docs/plugins/event-subscribers-diagrams/#event-subscriber-registration","title":"Event Subscriber Registration","text":"<p>The following diagram shows how event subscribers are registered:</p> <pre><code>flowchart TD\n    A[Plugin Module] --&gt; B[__init__.py]\n    B --&gt; C[ASH_EVENT_HANDLERS]\n\n    C --&gt; D[Event Type 1]\n    C --&gt; E[Event Type 2]\n    C --&gt; F[Event Type 3]\n\n    D --&gt; G[Handler Function 1]\n    D --&gt; H[Handler Function 2]\n    E --&gt; I[Handler Function 3]\n    F --&gt; J[Handler Function 4]\n\n    K[Plugin Manager] --&gt; L[Load Modules]\n    L --&gt; M[Discover Event Handlers]\n    M --&gt; N[Register Handlers]\n\n    C -.-&gt; N\n</code></pre>"},{"location":"docs/plugins/event-subscribers-diagrams/#event-data-flow","title":"Event Data Flow","text":"<p>The following diagram shows the data flow through the event system:</p> <pre><code>flowchart LR\n    A[ASH Core] --&gt; B[Event Data]\n    B --&gt; C[Event Emitter]\n\n    C --&gt; D[Event Type]\n    D --&gt; E[Event Handlers]\n\n    E --&gt; F[Handler 1]\n    E --&gt; G[Handler 2]\n    E --&gt; H[Handler 3]\n\n    F --&gt; I[Custom Action 1]\n    G --&gt; J[Custom Action 2]\n    H --&gt; K[Custom Action 3]\n\n    L[Event Context] -.-&gt; B\n    M[Plugin Context] -.-&gt; B\n    N[Phase Data] -.-&gt; B\n</code></pre>"},{"location":"docs/plugins/event-subscribers-diagrams/#event-handler-execution","title":"Event Handler Execution","text":"<p>The following diagram shows the execution flow of event handlers:</p> <pre><code>flowchart TD\n    A[Event Triggered] --&gt; B{Has Handlers?}\n    B --&gt;|Yes| C[Get Handler List]\n    B --&gt;|No| D[End]\n\n    C --&gt; E[Execute Handler 1]\n    E --&gt; F{Success?}\n    F --&gt;|Yes| G[Execute Handler 2]\n    F --&gt;|No| H[Log Error]\n    H --&gt; G\n\n    G --&gt; I{Success?}\n    I --&gt;|Yes| J[Execute Handler 3]\n    I --&gt;|No| K[Log Error]\n    K --&gt; J\n\n    J --&gt; L{Success?}\n    L --&gt;|Yes| M[All Handlers Complete]\n    L --&gt;|No| N[Log Error]\n    N --&gt; M\n\n    M --&gt; D\n</code></pre>"},{"location":"docs/plugins/event-subscribers-diagrams/#integration-with-external-systems","title":"Integration with External Systems","text":"<p>The following diagram shows how event subscribers can integrate with external systems:</p> <pre><code>flowchart LR\n    A[ASH Event] --&gt; B[Event Subscriber]\n\n    B --&gt; C[Slack Notifications]\n    B --&gt; D[Email Alerts]\n    B --&gt; E[Metrics Database]\n    B --&gt; F[Logging System]\n    B --&gt; G[CI/CD Pipeline]\n    B --&gt; H[Issue Tracker]\n\n    subgraph External Systems\n        C\n        D\n        E\n        F\n        G\n        H\n    end\n</code></pre>"},{"location":"docs/plugins/event-subscribers/","title":"Event Subscribers","text":"<p>ASH provides a comprehensive event system that allows plugins to react to various events during the scanning process. This enables you to create custom logging, notifications, integrations, and other reactive behaviors.</p> <p>For detailed visual diagrams of the event system architecture and workflow, see Event Subscriber Diagrams.</p>"},{"location":"docs/plugins/event-subscribers/#overview","title":"Overview","text":"<p>The event system uses a discovery-based pattern similar to how ASH discovers scanners, converters, and reporters. Event subscribers are registered using the <code>ASH_EVENT_HANDLERS</code> dictionary in your plugin module.</p>"},{"location":"docs/plugins/event-subscribers/#basic-event-subscriber","title":"Basic Event Subscriber","text":"<p>Here's a simple example of creating an event subscriber:</p> <pre><code># my_ash_plugins/__init__.py\nfrom automated_security_helper.plugins.events import AshEventType\n\ndef handle_scan_complete(**kwargs):\n    \"\"\"Handle scan complete event\"\"\"\n    scanner = kwargs.get('scanner', 'Unknown')\n    remaining_count = kwargs.get('remaining_count', 0)\n\n    print(f\"Scanner '{scanner}' completed!\")\n    if remaining_count &gt; 0:\n        print(f\"{remaining_count} scanners still running\")\n    else:\n        print(\"All scanners completed!\")\n\n    return True\n\n# Event callback registry\nASH_EVENT_HANDLERS = {\n    AshEventType.SCAN_COMPLETE: [handle_scan_complete],\n}\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#available-event-types","title":"Available Event Types","text":"<p>ASH provides the following event types:</p>"},{"location":"docs/plugins/event-subscribers/#phase-events","title":"Phase Events","text":"<ul> <li><code>AshEventType.CONVERT_START</code>: Fired when the convert phase begins</li> <li><code>AshEventType.CONVERT_COMPLETE</code>: Fired when the convert phase completes</li> <li><code>AshEventType.SCAN_START</code>: Fired when the scan phase begins</li> <li><code>AshEventType.SCAN_COMPLETE</code>: Fired when each individual scanner completes</li> <li><code>AshEventType.REPORT_START</code>: Fired when the report phase begins</li> <li><code>AshEventType.REPORT_COMPLETE</code>: Fired when the report phase completes</li> </ul>"},{"location":"docs/plugins/event-subscribers/#general-events","title":"General Events","text":"<ul> <li><code>AshEventType.ERROR</code>: Fired when errors occur</li> <li><code>AshEventType.WARNING</code>: Fired for warning conditions</li> <li><code>AshEventType.INFO</code>: Fired for informational events</li> </ul>"},{"location":"docs/plugins/event-subscribers/#event-data","title":"Event Data","text":"<p>Each event type provides specific data through keyword arguments:</p>"},{"location":"docs/plugins/event-subscribers/#scan_complete-event-data","title":"SCAN_COMPLETE Event Data","text":"<p>The <code>SCAN_COMPLETE</code> event is fired each time an individual scanner finishes and provides:</p> <ul> <li><code>scanner</code>: Name of the completed scanner</li> <li><code>completed_count</code>: Number of scanners completed so far</li> <li><code>total_count</code>: Total number of scanners</li> <li><code>remaining_count</code>: Number of scanners still running</li> <li><code>remaining_scanners</code>: List of remaining scanner names</li> <li><code>message</code>: Human-readable summary message</li> <li><code>phase</code>: The phase name (\"scan\")</li> <li><code>plugin_context</code>: The current plugin context</li> </ul>"},{"location":"docs/plugins/event-subscribers/#common-event-data","title":"Common Event Data","text":"<p>All events include:</p> <ul> <li><code>phase</code>: The name of the current phase</li> <li><code>plugin_context</code>: The current plugin context with source/output directories and configuration</li> </ul>"},{"location":"docs/plugins/event-subscribers/#multiple-event-subscribers","title":"Multiple Event Subscribers","text":"<p>You can register multiple subscribers for the same event:</p> <pre><code>def log_scan_completion(**kwargs):\n    \"\"\"Log scan completion to file\"\"\"\n    scanner = kwargs.get('scanner')\n    with open('/tmp/scan.log', 'a') as f:\n        f.write(f\"Scanner {scanner} completed at {datetime.now()}\\n\")\n    return True\n\ndef notify_scan_completion(**kwargs):\n    \"\"\"Send notification about scan completion\"\"\"\n    scanner = kwargs.get('scanner')\n    remaining = kwargs.get('remaining_count', 0)\n    # Send notification logic here\n    return True\n\nASH_EVENT_HANDLERS = {\n    AshEventType.SCAN_COMPLETE: [\n        log_scan_completion,\n        notify_scan_completion,\n    ],\n}\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#multiple-event-types","title":"Multiple Event Types","text":"<p>You can subscribe to multiple event types:</p> <pre><code>def handle_phase_start(**kwargs):\n    \"\"\"Handle any phase start\"\"\"\n    phase = kwargs.get('phase', 'Unknown')\n    print(f\"Phase '{phase}' started\")\n    return True\n\ndef handle_phase_complete(**kwargs):\n    \"\"\"Handle any phase completion\"\"\"\n    phase = kwargs.get('phase', 'Unknown')\n    print(f\"Phase '{phase}' completed\")\n    return True\n\nASH_EVENT_HANDLERS = {\n    AshEventType.SCAN_START: [handle_phase_start],\n    AshEventType.SCAN_COMPLETE: [handle_scan_completion],\n    AshEventType.CONVERT_START: [handle_phase_start],\n    AshEventType.CONVERT_COMPLETE: [handle_phase_complete],\n    AshEventType.REPORT_START: [handle_phase_start],\n    AshEventType.REPORT_COMPLETE: [handle_phase_complete],\n}\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#error-handling","title":"Error Handling","text":"<p>Event subscribers should handle errors gracefully to avoid disrupting the scan process:</p> <pre><code>def robust_event_handler(**kwargs):\n    \"\"\"Event handler with proper error handling\"\"\"\n    try:\n        scanner = kwargs.get('scanner', 'Unknown')\n        # Your event handling logic here\n        print(f\"Processing completion of {scanner}\")\n        return True\n    except Exception as e:\n        # Log the error but don't re-raise to avoid disrupting the scan\n        print(f\"Error in event handler: {e}\")\n        return False\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#real-world-examples","title":"Real-World Examples","text":""},{"location":"docs/plugins/event-subscribers/#slack-notifications","title":"Slack Notifications","text":"<pre><code>import requests\n\ndef notify_slack_on_completion(**kwargs):\n    \"\"\"Send Slack notification when all scanners complete\"\"\"\n    remaining_count = kwargs.get('remaining_count', 0)\n\n    if remaining_count == 0:  # All scanners completed\n        webhook_url = os.environ.get(\"SLACK_WEBHOOK\", None)\n        if webhook_url is None:\n            ASH_LOGGER.error(\"SLACK_WEBHOOK variable is unset! Unable to send webhook.\")\n            return False\n        message = {\n            \"text\": \"\ud83c\udf89 ASH security scan completed successfully!\",\n            \"channel\": \"#security-alerts\"\n        }\n        try:\n            requests.post(webhook_url, json=message)\n        except Exception as e:\n            print(f\"Failed to send Slack notification: {e}\")\n\n    return True\n\nASH_EVENT_HANDLERS = {\n    AshEventType.EXECUTION_COMPLETE: [notify_slack_on_completion],\n}\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#custom-metrics-collection","title":"Custom Metrics Collection","text":"<pre><code>import time\n\n# Global state for tracking metrics\nscan_metrics = {}\n\ndef track_scan_metrics(**kwargs):\n    \"\"\"Track scan performance metrics\"\"\"\n    scanner = kwargs.get('scanner')\n    completed_count = kwargs.get('completed_count', 0)\n    total_count = kwargs.get('total_count', 0)\n\n    # Record completion time\n    scan_metrics[scanner]['completed_at'] = time.time()\n\n    # Calculate progress\n    progress = (completed_count / total_count) * 100 if total_count &gt; 0 else 0\n    print(f\"Scan progress: {progress:.1f}% ({completed_count}/{total_count})\")\n\n    return True\n\nASH_EVENT_HANDLERS = {\n    AshEventType.SCAN_COMPLETE: [track_scan_metrics],\n}\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#integration-with-external-systems","title":"Integration with External Systems","text":"<pre><code>import json\nimport requests\nfrom datetime import datetime, timezone\n\ndef send_to_monitoring_system(**kwargs):\n    \"\"\"Send scan completion data to external monitoring system\"\"\"\n    try:\n        scanner = kwargs.get('scanner')\n        completed_count = kwargs.get('completed_count', 0)\n        total_count = kwargs.get('total_count', 0)\n        remaining_count = kwargs.get('remaining_count', 0)\n\n        # Prepare monitoring data\n        monitoring_data = {\n            'timestamp': datetime.now(timezone.utc).isoformat(),\n            'event_type': 'scanner_completed',\n            'scanner_name': scanner,\n            'progress': {\n                'completed': completed_count,\n                'total': total_count,\n                'remaining': remaining_count,\n                'percentage': (completed_count / total_count * 100) if total_count &gt; 0 else 0\n            }\n        }\n\n        # Send to monitoring endpoint\n        response = requests.post(\n            'https://monitoring.example.com/api/events',\n            json=monitoring_data,\n            headers={'Content-Type': 'application/json'},\n            timeout=5\n        )\n\n        if response.status_code == 200:\n            print(f\"Successfully sent monitoring data for {scanner}\")\n        else:\n            print(f\"Failed to send monitoring data: {response.status_code}\")\n\n    except Exception as e:\n        print(f\"Error sending monitoring data: {e}\")\n\n    return True\n\nASH_EVENT_HANDLERS = {\n    AshEventType.SCAN_COMPLETE: [send_to_monitoring_system],\n}\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#database-logging","title":"Database Logging","text":"<pre><code>import sqlite3\nfrom datetime import datetime\n\ndef log_to_database(**kwargs):\n    \"\"\"Log scan events to SQLite database\"\"\"\n    try:\n        scanner = kwargs.get('scanner')\n        completed_count = kwargs.get('completed_count', 0)\n        total_count = kwargs.get('total_count', 0)\n        phase = kwargs.get('phase', 'unknown')\n\n        # Connect to database\n        conn = sqlite3.connect('/tmp/ash_scan_log.db')\n        cursor = conn.cursor()\n\n        # Create table if it doesn't exist\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS scan_events (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                timestamp TEXT,\n                phase TEXT,\n                scanner TEXT,\n                completed_count INTEGER,\n                total_count INTEGER,\n                progress_percentage REAL\n            )\n        ''')\n\n        # Insert event data\n        progress = (completed_count / total_count * 100) if total_count &gt; 0 else 0\n        cursor.execute('''\n            INSERT INTO scan_events\n            (timestamp, phase, scanner, completed_count, total_count, progress_percentage)\n            VALUES (?, ?, ?, ?, ?, ?)\n        ''', (\n            datetime.now(timezone.utc).isoformat(),\n            phase,\n            scanner,\n            completed_count,\n            total_count,\n            progress\n        ))\n\n        conn.commit()\n        conn.close()\n\n        print(f\"Logged scan event for {scanner} to database\")\n\n    except Exception as e:\n        print(f\"Error logging to database: {e}\")\n\n    return True\n\nASH_EVENT_HANDLERS = {\n    AshEventType.SCAN_COMPLETE: [log_to_database],\n}\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#plugin-discovery","title":"Plugin Discovery","text":"<p>ASH automatically discovers event subscribers by:</p> <ol> <li>Loading modules specified in the <code>internal_modules</code> list (for built-in plugins)</li> <li>Loading additional modules specified in configuration via <code>ash_plugin_modules</code></li> <li>Scanning for <code>ASH_EVENT_HANDLERS</code> constants in loaded modules</li> <li>Registering discovered event subscribers with the plugin manager</li> </ol> <p>The event subscribers are called in the order they appear in the callback list for each event type.</p>"},{"location":"docs/plugins/event-subscribers/#best-practices","title":"Best Practices","text":"<ol> <li>Return Values: Always return <code>True</code> for successful handling or <code>False</code> for errors</li> <li>Error Handling: Use try-catch blocks to prevent event handler errors from disrupting scans</li> <li>Performance: Keep event handlers lightweight to avoid slowing down the scan process</li> <li>Logging: Use appropriate log levels and avoid excessive output</li> <li>State Management: Be careful with global state in multi-threaded environments</li> <li>Resource Cleanup: Clean up any resources (files, connections) in your event handlers</li> <li>Timeouts: Use timeouts for external API calls to prevent hanging</li> <li>Graceful Degradation: Design handlers to fail gracefully without affecting the main scan process</li> </ol>"},{"location":"docs/plugins/event-subscribers/#advanced-usage","title":"Advanced Usage","text":""},{"location":"docs/plugins/event-subscribers/#conditional-event-handling","title":"Conditional Event Handling","text":"<pre><code>def conditional_handler(**kwargs):\n    \"\"\"Only handle events under certain conditions\"\"\"\n    scanner = kwargs.get('scanner')\n    remaining_count = kwargs.get('remaining_count', 0)\n\n    # Only notify for critical scanners or when all complete\n    critical_scanners = ['bandit', 'semgrep', 'checkov']\n\n    if scanner in critical_scanners or remaining_count == 0:\n        print(f\"Important: {scanner} completed!\")\n        # Send notification logic here\n\n    return True\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#event-filtering","title":"Event Filtering","text":"<pre><code>def filtered_handler(**kwargs):\n    \"\"\"Filter events based on context\"\"\"\n    plugin_context = kwargs.get('plugin_context')\n\n    # Only handle events for certain source directories\n    if plugin_context and 'production' in str(plugin_context.source_dir):\n        scanner = kwargs.get('scanner')\n        print(f\"Production scan: {scanner} completed\")\n        # Handle production-specific logic\n\n    return True\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#stateful-event-handling","title":"Stateful Event Handling","text":"<pre><code>class ScanProgressTracker:\n    def __init__(self):\n        self.start_time = None\n        self.completed_scanners = []\n\n    def handle_scan_start(self, **kwargs):\n        \"\"\"Track scan start time\"\"\"\n        self.start_time = time.time()\n        self.completed_scanners = []\n        print(\"Scan progress tracking started\")\n        return True\n\n    def handle_scan_complete(self, **kwargs):\n        \"\"\"Track individual scanner completion\"\"\"\n        scanner = kwargs.get('scanner')\n        remaining_count = kwargs.get('remaining_count', 0)\n\n        self.completed_scanners.append(scanner)\n\n        if self.start_time:\n            elapsed = time.time() - self.start_time\n            print(f\"Scanner {scanner} completed after {elapsed:.1f}s\")\n\n        if remaining_count == 0:\n            total_time = time.time() - self.start_time if self.start_time else 0\n            print(f\"All scanners completed in {total_time:.1f}s\")\n            print(f\"Completion order: {', '.join(self.completed_scanners)}\")\n\n        return True\n\n# Create tracker instance\ntracker = ScanProgressTracker()\n\nASH_EVENT_HANDLERS = {\n    AshEventType.SCAN_START: [tracker.handle_scan_start],\n    AshEventType.SCAN_COMPLETE: [tracker.handle_scan_complete],\n}\n</code></pre>"},{"location":"docs/plugins/event-subscribers/#integration-with-built-in-events","title":"Integration with Built-in Events","text":"<p>ASH includes built-in event subscribers for core functionality like scan completion logging. Your custom event subscribers will run alongside these built-in handlers, allowing you to extend ASH's behavior without replacing core functionality.</p> <p>The built-in scan completion logger provides enhanced logging that shows remaining scanners:</p> <pre><code>INFO: Completed scanner: bandit\nINFO: Remaining scanners (2): semgrep, checkov\n</code></pre> <p>Your custom event subscribers will receive the same event data and can provide additional functionality like notifications, metrics collection, or integration with external systems.</p>"},{"location":"docs/plugins/plugin-best-practices/","title":"Plugin Best Practices","text":"<p>This guide provides best practices for developing ASH plugins.</p>"},{"location":"docs/plugins/plugin-best-practices/#general-best-practices","title":"General Best Practices","text":"<ol> <li>Follow the Plugin Interface: Implement all required methods for your plugin type</li> <li>Use Pydantic Models: For configuration and data validation</li> <li>Handle Errors Gracefully: Use try/except blocks and provide meaningful error messages</li> <li>Document Your Plugin: Add docstrings and comments to explain your plugin's functionality</li> <li>Test Thoroughly: Create unit tests for your plugins</li> <li>Version Your Plugins: Use semantic versioning for your plugins</li> <li>Respect the Plugin Context: Use the provided directories for outputs</li> <li>Clean Up After Yourself: Remove temporary files when done</li> </ol>"},{"location":"docs/plugins/plugin-best-practices/#scanner-plugin-best-practices","title":"Scanner Plugin Best Practices","text":"<ol> <li>Generate SARIF Reports: SARIF is the standard format for security findings</li> <li>Handle Ignore Paths: Skip files that are in the global ignore paths</li> <li>Use Subprocess Utilities: Use the provided <code>_run_subprocess</code> method for running external commands</li> <li>Add Metadata: Add useful metadata to the results container</li> <li>Support Both File and Directory Scanning: Handle both individual files and directories</li> </ol>"},{"location":"docs/plugins/plugin-best-practices/#reporter-plugin-best-practices","title":"Reporter Plugin Best Practices","text":"<ol> <li>Validate Dependencies: Implement the <code>validate</code> method to check dependencies</li> <li>Output to Files: Write reports to the <code>reports</code> directory</li> <li>Return Content: Return the report content as a string</li> <li>Use Model Methods: Use the model's helper methods like <code>to_simple_dict()</code> and <code>to_flat_vulnerabilities()</code></li> <li>Handle Large Reports: Be mindful of memory usage when generating large reports</li> </ol>"},{"location":"docs/plugins/plugin-best-practices/#converter-plugin-best-practices","title":"Converter Plugin Best Practices","text":"<ol> <li>Preserve Line Numbers: Try to preserve line numbers for better mapping of findings back to original files</li> <li>Handle Directories: Support converting both individual files and directories</li> <li>Return Paths: Return the path to the converted file or directory</li> <li>Skip Unsupported Files: Only convert files with supported extensions</li> <li>Maintain File Structure: Preserve the directory structure when converting directories</li> </ol>"},{"location":"docs/plugins/plugin-best-practices/#plugin-dependencies","title":"Plugin Dependencies","text":"<p>You can specify dependencies for your plugins:</p> <pre><code>from automated_security_helper.base.plugin_dependency import PluginDependency, CustomCommand\nfrom automated_security_helper.plugins.decorators import ash_scanner_plugin\nfrom automated_security_helper.base.scanner_plugin import ScannerPluginBase\n\n@ash_scanner_plugin\nclass MyCustomScanner(ScannerPluginBase):\n    name = \"my-custom-scanner\"\n    description = \"My custom security scanner\"\n    version = \"1.0.0\"\n    dependencies = [\n        PluginDependency(\n            name=\"my-scanner-tool\",\n            commands=[\n                CustomCommand(\n                    platform=\"linux\",\n                    arch=\"amd64\",\n                    command=[\"pip\", \"install\", \"my-scanner-tool\"]\n                ),\n                CustomCommand(\n                    platform=\"darwin\",\n                    arch=\"amd64\",\n                    command=[\"pip\", \"install\", \"my-scanner-tool\"]\n                )\n            ]\n        )\n    ]\n</code></pre>"},{"location":"docs/plugins/plugin-best-practices/#plugin-event-subscribers","title":"Plugin Event Subscribers","text":"<p>ASH supports event subscribers for reacting to events during the scan process. Event subscribers are registered using the <code>ASH_EVENT_HANDLERS</code> dictionary pattern:</p> <pre><code># my_ash_plugins/__init__.py\nfrom automated_security_helper.plugins.events import AshEventType\n\ndef handle_scan_complete(**kwargs):\n    \"\"\"Handle scan complete event\"\"\"\n    scanner = kwargs.get('scanner', 'Unknown')\n    remaining_count = kwargs.get('remaining_count', 0)\n    remaining_scanners = kwargs.get('remaining_scanners', [])\n\n    print(f\"Scanner '{scanner}' completed!\")\n    if remaining_count &gt; 0:\n        print(f\"{remaining_count} scanners remaining: {', '.join(remaining_scanners)}\")\n    else:\n        print(\"All scanners completed!\")\n\n    return True\n\ndef handle_report_complete(**kwargs):\n    \"\"\"Handle report complete event\"\"\"\n    phase = kwargs.get('phase', 'Unknown')\n    print(f\"Report phase '{phase}' completed!\")\n    return True\n\n# Event callback registry following the same pattern as ASH_SCANNERS, ASH_REPORTERS, etc.\nASH_EVENT_HANDLERS = {\n    AshEventType.SCAN_COMPLETE: [handle_scan_complete],\n    AshEventType.REPORT_COMPLETE: [handle_report_complete],\n}\n</code></pre>"},{"location":"docs/plugins/plugin-best-practices/#available-event-types","title":"Available Event Types","text":"<ul> <li><code>AshEventType.SCAN_START</code>: Fired when the scan phase begins</li> <li><code>AshEventType.SCAN_COMPLETE</code>: Fired when each individual scanner completes</li> <li><code>AshEventType.CONVERT_START</code>: Fired when the convert phase begins</li> <li><code>AshEventType.CONVERT_COMPLETE</code>: Fired when the convert phase completes</li> <li><code>AshEventType.REPORT_START</code>: Fired when the report phase begins</li> <li><code>AshEventType.REPORT_COMPLETE</code>: Fired when the report phase completes</li> <li><code>AshEventType.ERROR</code>: Fired when errors occur</li> <li><code>AshEventType.WARNING</code>: Fired for warning conditions</li> <li><code>AshEventType.INFO</code>: Fired for informational events</li> </ul>"},{"location":"docs/plugins/plugin-best-practices/#event-data","title":"Event Data","text":"<p>Event subscribers receive keyword arguments with relevant data. For example, <code>SCAN_COMPLETE</code> events include:</p> <ul> <li><code>scanner</code>: Name of the completed scanner</li> <li><code>completed_count</code>: Number of scanners completed so far</li> <li><code>total_count</code>: Total number of scanners</li> <li><code>remaining_count</code>: Number of scanners still running</li> <li><code>remaining_scanners</code>: List of remaining scanner names</li> <li><code>message</code>: Human-readable summary message</li> <li><code>phase</code>: The phase name (\"scan\")</li> <li><code>plugin_context</code>: The current plugin context</li> </ul>"},{"location":"docs/plugins/plugin-best-practices/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Not Handling Errors: Always catch and handle exceptions to prevent the entire scan from failing</li> <li>Ignoring Configuration: Always respect the plugin configuration options</li> <li>Hard-coding Paths: Use the paths provided in the plugin context</li> <li>Not Validating Dependencies: Always check that required dependencies are available</li> <li>Returning Incorrect Types: Make sure to return the expected types from plugin methods</li> </ol>"},{"location":"docs/plugins/plugin-best-practices/#security-considerations","title":"Security Considerations","text":"<ol> <li>Validate User Input: Never trust user input without validation</li> <li>Avoid Shell Injection: Use lists for subprocess commands instead of strings</li> <li>Handle Secrets Securely: Never log or expose sensitive information</li> <li>Limit Resource Usage: Be mindful of memory and CPU usage</li> <li>Clean Up Temporary Files: Always clean up temporary files after use</li> </ol>"},{"location":"docs/plugins/reporter-plugins-diagrams/","title":"Reporter Plugin Diagrams","text":"<p>This document provides visual diagrams of the ASH reporter plugin architecture using Mermaid.</p>"},{"location":"docs/plugins/reporter-plugins-diagrams/#reporter-plugin-lifecycle","title":"Reporter Plugin Lifecycle","text":"<p>The following diagram shows the lifecycle of a reporter plugin during an ASH scan:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant PM as Plugin Manager\n    participant RP as Reporter Plugin\n    participant FS as File System\n    participant ES as Event System\n    participant EXT as External Systems\n\n    ASH-&gt;&gt;PM: Load Reporter Plugins\n    PM-&gt;&gt;RP: Initialize\n    RP--&gt;&gt;PM: Return Initialized Plugin\n\n    ASH-&gt;&gt;ES: Emit ReportingStarted Event\n\n    ASH-&gt;&gt;RP: Validate Reporter\n    RP--&gt;&gt;ASH: Return Validation Status\n\n    ASH-&gt;&gt;RP: report(aggregated_results)\n    RP-&gt;&gt;RP: Process Results\n\n    alt Local Report\n        RP-&gt;&gt;FS: Write Report File\n        FS--&gt;&gt;RP: File Written\n    else External Report\n        RP-&gt;&gt;EXT: Send Report Data\n        EXT--&gt;&gt;RP: Confirmation\n    end\n\n    RP--&gt;&gt;ASH: Return Report URL/Path\n\n    ASH-&gt;&gt;ES: Emit ReportingCompleted Event\n</code></pre>"},{"location":"docs/plugins/reporter-plugins-diagrams/#reporter-plugin-data-flow","title":"Reporter Plugin Data Flow","text":"<p>The following diagram shows the data flow through a reporter plugin:</p> <pre><code>flowchart LR\n    A[AshAggregatedResults] --&gt; B[Reporter Plugin]\n\n    subgraph Reporter Plugin\n        C[Results Processor] --&gt; D[Format Converter]\n        D --&gt; E[Output Generator]\n    end\n\n    B --&gt; F[Local File]\n    B --&gt; G[External System]\n    B --&gt; H[Console Output]\n\n    F --&gt; I[Report Path]\n    G --&gt; J[External URL]\n    H --&gt; K[Terminal Display]\n</code></pre>"},{"location":"docs/plugins/reporter-plugins-diagrams/#reporter-plugin-class-hierarchy","title":"Reporter Plugin Class Hierarchy","text":"<p>The following diagram shows the class hierarchy for reporter plugins:</p> <pre><code>classDiagram\n    class PluginBase {\n        +context: PluginContext\n        +config: Any\n        +validate_plugin_dependencies() bool\n        +model_post_init(context)\n        #_plugin_log(message, level, target_type, append_to_stream)\n        #_run_subprocess(cmd, stdout_preference, stderr_preference)\n    }\n\n    class ReporterPluginBase {\n        +report(model) str\n        +dependencies_satisfied: bool\n    }\n\n    class ReporterPluginConfigBase {\n        +name: str\n        +extension: str\n        +enabled: bool\n        +options: ReporterOptionsBase\n    }\n\n    class ReporterOptionsBase {\n        +include_details: bool\n        +max_findings: int\n    }\n\n    class CustomReporter {\n        +report(model) str\n    }\n\n    PluginBase &lt;|-- ReporterPluginBase\n    ReporterPluginBase &lt;|-- CustomReporter\n    ReporterPluginConfigBase -- CustomReporter : configures\n    ReporterOptionsBase -- ReporterPluginConfigBase : contains\n</code></pre>"},{"location":"docs/plugins/reporter-plugins-diagrams/#reporter-plugin-configuration-flow","title":"Reporter Plugin Configuration Flow","text":"<p>The following diagram shows how configuration flows through a reporter plugin:</p> <pre><code>flowchart TD\n    A[.ash/.ash.yaml] --&gt; B[Configuration Parser]\n    C[CLI Arguments] --&gt; B\n    B --&gt; D[ASH Configuration]\n    D --&gt; E[Reporter Configuration]\n    E --&gt; F[Reporter Plugin]\n\n    subgraph Reporter Plugin\n        G[Validate Config] --&gt; H[Apply Config]\n        H --&gt; I[Use in Report Logic]\n    end\n\n    F --&gt; J[Report Output]\n</code></pre>"},{"location":"docs/plugins/reporter-plugins-diagrams/#reporter-integration-with-external-systems","title":"Reporter Integration with External Systems","text":"<p>The following diagram shows how reporter plugins can integrate with external systems:</p> <pre><code>flowchart LR\n    A[ASH Core] --&gt; B[Reporter Plugin]\n\n    B --&gt; C[Local File System]\n    B --&gt; D[Cloud Storage]\n    B --&gt; E[Issue Trackers]\n    B --&gt; F[CI/CD Systems]\n    B --&gt; G[Dashboards]\n    B --&gt; H[Notification Systems]\n\n    subgraph External Systems\n        D\n        E\n        F\n        G\n        H\n    end\n\n    C --&gt; I[HTML Reports]\n    C --&gt; J[JSON Reports]\n    C --&gt; K[SARIF Reports]\n    C --&gt; L[CSV Reports]\n    C --&gt; M[Markdown Reports]\n</code></pre>"},{"location":"docs/plugins/reporter-plugins/","title":"Reporter Plugins","text":"<p>Reporter plugins generate reports from scan results in various formats. They transform the ASH aggregated results model into human-readable or machine-readable formats.</p> <p>For detailed visual diagrams of reporter plugin architecture and workflow, see Reporter Plugin Diagrams.</p>"},{"location":"docs/plugins/reporter-plugins/#reporter-plugin-interface","title":"Reporter Plugin Interface","text":"<p>Reporter plugins must implement the <code>ReporterPluginBase</code> interface:</p> <pre><code>from automated_security_helper.base.reporter_plugin import ReporterPluginBase, ReporterPluginConfigBase\nfrom automated_security_helper.plugins.decorators import ash_reporter_plugin\n\n@ash_reporter_plugin\nclass MyReporter(ReporterPluginBase):\n    \"\"\"My custom reporter implementation\"\"\"\n\n    def report(self, model):\n        \"\"\"Generate a report from the model\"\"\"\n        # Your code here\n</code></pre>"},{"location":"docs/plugins/reporter-plugins/#reporter-plugin-configuration","title":"Reporter Plugin Configuration","text":"<p>Define a configuration class for your reporter:</p> <pre><code>from typing import Literal\nfrom pydantic import Field\n\nclass MyReporterConfig(ReporterPluginConfigBase):\n    name: Literal[\"my-reporter\"] = \"my-reporter\"\n    extension: str = \"my-report.txt\"\n    enabled: bool = True\n\n    class Options:\n        include_details: bool = Field(default=True, description=\"Include detailed findings\")\n        max_findings: int = Field(default=100, description=\"Maximum number of findings to include\")\n</code></pre>"},{"location":"docs/plugins/reporter-plugins/#reporter-plugin-example","title":"Reporter Plugin Example","text":"<p>Here's a complete example of a custom reporter plugin based on the S3Reporter in your codebase:</p> <pre><code>import json\nimport os\nfrom pathlib import Path\nfrom typing import Annotated, Literal, Optional, TYPE_CHECKING\n\nimport boto3\nfrom pydantic import Field\n\nfrom automated_security_helper.base.options import ReporterOptionsBase\nfrom automated_security_helper.base.reporter_plugin import (\n    ReporterPluginBase,\n    ReporterPluginConfigBase,\n)\nfrom automated_security_helper.plugins.decorators import ash_reporter_plugin\nfrom automated_security_helper.utils.log import ASH_LOGGER\n\nif TYPE_CHECKING:\n    from automated_security_helper.models.asharp_model import AshAggregatedResults\n\nclass S3ReporterConfigOptions(ReporterOptionsBase):\n    aws_region: Annotated[\n        str | None,\n        Field(\n            pattern=r\"(af|il|ap|ca|eu|me|sa|us|cn|us-gov|us-iso|us-isob)-(central|north|(north(?:east|west))|south|south(?:east|west)|east|west)-\\d{1}\"\n        ),\n    ] = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", None))\n    aws_profile: Optional[str] = os.environ.get(\"AWS_PROFILE\", None)\n    bucket_name: str | None = os.environ.get(\"ASH_S3_BUCKET_NAME\", None)\n    key_prefix: str = \"ash-reports/\"\n    file_format: Literal[\"json\", \"yaml\"] = \"json\"\n\nclass S3ReporterConfig(ReporterPluginConfigBase):\n    name: Literal[\"s3\"] = \"s3\"\n    extension: str = \"s3.json\"\n    enabled: bool = True\n    options: S3ReporterConfigOptions = S3ReporterConfigOptions()\n\n@ash_reporter_plugin\nclass S3Reporter(ReporterPluginBase[S3ReporterConfig]):\n    \"\"\"Formats results and uploads to an S3 bucket.\"\"\"\n\n    def model_post_init(self, context):\n        if self.config is None:\n            self.config = S3ReporterConfig()\n        return super().model_post_init(context)\n\n    def validate_plugin_dependencies(self) -&gt; bool:\n        \"\"\"Validate reporter configuration and requirements.\"\"\"\n        self.dependencies_satisfied = False\n        if self.config.options.aws_region is None or self.config.options.bucket_name is None:\n            return self.dependencies_satisfied\n        try:\n            session = boto3.Session(\n                profile_name=self.config.options.aws_profile,\n                region_name=self.config.options.aws_region,\n            )\n            sts_client = session.client(\"sts\")\n            caller_id = sts_client.get_caller_identity()\n\n            # Check if S3 bucket exists and is accessible\n            s3_client = session.client(\"s3\")\n            s3_client.head_bucket(Bucket=self.config.options.bucket_name)\n\n            self.dependencies_satisfied = \"Account\" in caller_id\n        except Exception as e:\n            self._plugin_log(\n                f\"Error when validating S3 access: {e}\",\n                level=\"WARNING\",\n                target_type=\"source\",\n                append_to_stream=\"stderr\",\n            )\n        finally:\n            return self.dependencies_satisfied\n\n    def report(self, model: \"AshAggregatedResults\") -&gt; str:\n        \"\"\"Format ASH model and upload to S3 bucket.\"\"\"\n        if isinstance(self.config, dict):\n            self.config = S3ReporterConfig.model_validate_plugin_dependencies(self.config)\n\n        # Create a unique key for the S3 object\n        timestamp = model.scan_metadata.scan_time.strftime(\"%Y%m%d-%H%M%S\")\n        file_extension = \"json\" if self.config.options.file_format == \"json\" else \"yaml\"\n        s3_key = f\"{self.config.options.key_prefix}ash-report-{timestamp}.{file_extension}\"\n\n        # Format the results based on the specified format\n        if self.config.options.file_format == \"json\":\n            output_dict = model.to_simple_dict()\n            output_content = json.dumps(output_dict, default=str, indent=2)\n        else:\n            import yaml\n            output_dict = model.to_simple_dict()\n            output_content = yaml.dump(output_dict, default_flow_style=False)\n\n        # Create a session with the specified profile and region\n        session = boto3.Session(\n            profile_name=self.config.options.aws_profile,\n            region_name=self.config.options.aws_region,\n        )\n        s3_client = session.client(\"s3\")\n\n        try:\n            # Upload the content to S3\n            s3_client.put_object(\n                Bucket=self.config.options.bucket_name,\n                Key=s3_key,\n                Body=output_content,\n                ContentType=\"application/json\" if file_extension == \"json\" else \"application/yaml\"\n            )\n\n            s3_url = f\"s3://{self.config.options.bucket_name}/{s3_key}\"\n            ASH_LOGGER.info(f\"Successfully uploaded report to {s3_url}\")\n\n            # Also write to local file if needed\n            output_path = Path(self.context.output_dir) / \"reports\" / f\"s3-report.{file_extension}\"\n            output_path.parent.mkdir(parents=True, exist_ok=True)\n\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(output_content)\n\n            return s3_url\n        except Exception as e:\n            error_msg = f\"Error uploading to S3: {str(e)}\"\n            self._plugin_log(\n                error_msg,\n                level=\"ERROR\",\n                append_to_stream=\"stderr\",\n            )\n            return error_msg\n</code></pre>"},{"location":"docs/plugins/reporter-plugins/#simple-reporter-plugin-example","title":"Simple Reporter Plugin Example","text":"<p>Here's a simpler example of a custom reporter plugin:</p> <pre><code>from pathlib import Path\nfrom typing import Literal\n\nfrom pydantic import Field\n\nfrom automated_security_helper.base.options import ReporterOptionsBase\nfrom automated_security_helper.base.reporter_plugin import (\n    ReporterPluginBase,\n    ReporterPluginConfigBase,\n)\nfrom automated_security_helper.plugins.decorators import ash_reporter_plugin\n\nclass SimpleReporterConfigOptions(ReporterOptionsBase):\n    include_details: bool = Field(default=True, description=\"Include detailed findings\")\n    max_findings: int = Field(default=100, description=\"Maximum number of findings to include\")\n    output_file: str = Field(default=\"simple-report.txt\", description=\"Output file name\")\n\nclass SimpleReporterConfig(ReporterPluginConfigBase):\n    name: Literal[\"simple\"] = \"simple\"\n    extension: str = \"simple.txt\"\n    enabled: bool = True\n    options: SimpleReporterConfigOptions = SimpleReporterConfigOptions()\n\n@ash_reporter_plugin\nclass SimpleReporter(ReporterPluginBase[SimpleReporterConfig]):\n    \"\"\"Generates a simple text report.\"\"\"\n\n    def model_post_init(self, context):\n        if self.config is None:\n            self.config = SimpleReporterConfig()\n        return super().model_post_init(context)\n\n    def report(self, model):\n        \"\"\"Generate a simple text report.\"\"\"\n        # Create the report content\n        content = []\n        content.append(\"# Security Scan Report\")\n        content.append(\"\")\n        content.append(f\"Project: {model.project_name}\")\n        content.append(f\"Scan Time: {model.scan_metadata.scan_time}\")\n        content.append(\"\")\n        content.append(\"## Summary\")\n        content.append(\"\")\n        content.append(f\"Total Findings: {model.summary_stats.total_findings}\")\n        content.append(f\"Critical: {model.summary_stats.critical_count}\")\n        content.append(f\"High: {model.summary_stats.high_count}\")\n        content.append(f\"Medium: {model.summary_stats.medium_count}\")\n        content.append(f\"Low: {model.summary_stats.low_count}\")\n        content.append(f\"Info: {model.summary_stats.info_count}\")\n        content.append(\"\")\n\n        # Add detailed findings if configured\n        if self.config.options.include_details:\n            content.append(\"## Detailed Findings\")\n            content.append(\"\")\n\n            # Get flat vulnerabilities\n            vulnerabilities = model.to_flat_vulnerabilities()\n\n            # Limit the number of findings\n            max_findings = min(len(vulnerabilities), self.config.options.max_findings)\n\n            for i, vuln in enumerate(vulnerabilities[:max_findings]):\n                content.append(f\"### Finding {i+1}\")\n                content.append(f\"Title: {vuln.title}\")\n                content.append(f\"Severity: {vuln.severity}\")\n                content.append(f\"File: {vuln.file_path}\")\n                content.append(f\"Line: {vuln.line_number}\")\n                content.append(f\"Description: {vuln.description}\")\n                content.append(\"\")\n\n        # Write the report to a file\n        report_text = \"\\n\".join(content)\n        output_path = Path(self.context.output_dir) / \"reports\" / self.config.options.output_file\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(report_text)\n\n        return report_text\n</code></pre>"},{"location":"docs/plugins/reporter-plugins/#reporter-plugin-best-practices","title":"Reporter Plugin Best Practices","text":"<ol> <li>Handle Configuration: Use Pydantic models for configuration</li> <li>Validate Dependencies: Implement the <code>validate</code> method to check dependencies</li> <li>Error Handling: Use try/except blocks and provide meaningful error messages</li> <li>Output to Files: Write reports to the <code>reports</code> directory</li> <li>Return Content: Return the report content as a string</li> <li>Use Model Methods: Use the model's helper methods like <code>to_simple_dict()</code> and <code>to_flat_vulnerabilities()</code></li> </ol>"},{"location":"docs/plugins/reporter-plugins/#reporter-plugin-configuration-in-ash","title":"Reporter Plugin Configuration in ASH","text":"<p>Configure your reporter in the ASH configuration file:</p> <pre><code># .ash/.ash.yaml\nreporters:\n  simple:\n    enabled: true\n    options:\n      include_details: true\n      max_findings: 50\n      output_file: custom-report.txt\n</code></pre>"},{"location":"docs/plugins/reporter-plugins/#testing-reporter-plugins","title":"Testing Reporter Plugins","text":"<p>Create unit tests for your reporter:</p> <pre><code>import pytest\nfrom pathlib import Path\n\nfrom automated_security_helper.base.plugin_context import PluginContext\nfrom automated_security_helper.models.asharp_model import AshAggregatedResults\nfrom my_ash_plugins.reporters import SimpleReporter\n\ndef test_simple_reporter():\n    # Create a plugin context\n    context = PluginContext(\n        source_dir=Path(\"test_data\"),\n        output_dir=Path(\"test_output\")\n    )\n\n    # Create reporter instance\n    reporter = SimpleReporter(context=context)\n\n    # Create a mock model\n    model = AshAggregatedResults(\n        project_name=\"test-project\",\n        # Add other required fields\n    )\n\n    # Generate the report\n    report = reporter.report(model)\n\n    # Assert report content\n    assert \"Security Scan Report\" in report\n    assert \"test-project\" in report\n</code></pre>"},{"location":"docs/plugins/scanner-plugins-diagrams/","title":"Scanner Plugin Diagrams","text":"<p>This document provides visual diagrams of the ASH scanner plugin architecture using Mermaid.</p>"},{"location":"docs/plugins/scanner-plugins-diagrams/#scanner-plugin-lifecycle","title":"Scanner Plugin Lifecycle","text":"<p>The following diagram shows the lifecycle of a scanner plugin during an ASH scan:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant PM as Plugin Manager\n    participant SP as Scanner Plugin\n    participant FS as File System\n    participant ES as Event System\n\n    ASH-&gt;&gt;PM: Load Scanner Plugins\n    PM-&gt;&gt;SP: Initialize\n    SP--&gt;&gt;PM: Return Initialized Plugin\n\n    ASH-&gt;&gt;ES: Emit ScanStarted Event\n\n    ASH-&gt;&gt;SP: Validate Scanner\n    SP--&gt;&gt;ASH: Return Validation Status\n\n    ASH-&gt;&gt;SP: scan(target, target_type)\n    SP-&gt;&gt;FS: Read Target Files\n    FS--&gt;&gt;SP: Return File Contents\n\n    SP-&gt;&gt;SP: Process Files\n    Note over SP: Run Security Analysis\n\n    SP-&gt;&gt;FS: Write SARIF Report\n    SP--&gt;&gt;ASH: Return ScanResultsContainer\n\n    ASH-&gt;&gt;ES: Emit ScanCompleted Event\n</code></pre>"},{"location":"docs/plugins/scanner-plugins-diagrams/#scanner-plugin-data-flow","title":"Scanner Plugin Data Flow","text":"<p>The following diagram shows the data flow through a scanner plugin:</p> <pre><code>flowchart LR\n    A[Source Files] --&gt; B[Scanner Plugin]\n    C[Converted Files] --&gt; B\n\n    subgraph Scanner Plugin\n        D[File Reader] --&gt; E[Security Analyzer]\n        E --&gt; F[Results Processor]\n        F --&gt; G[SARIF Generator]\n    end\n\n    B --&gt; H[ScanResultsContainer]\n    H --&gt; I[SARIF Report]\n    H --&gt; J[Error Messages]\n    H --&gt; K[Metadata]\n\n    I --&gt; L[Reporter Plugins]\n</code></pre>"},{"location":"docs/plugins/scanner-plugins-diagrams/#scanner-plugin-class-hierarchy","title":"Scanner Plugin Class Hierarchy","text":"<p>The following diagram shows the class hierarchy for scanner plugins:</p> <pre><code>classDiagram\n    class PluginBase {\n        +context: PluginContext\n        +config: Any\n        +validate_plugin_dependencies() bool\n        +model_post_init(context)\n        #_plugin_log(message, level, target_type, append_to_stream)\n        #_run_subprocess(cmd, stdout_preference, stderr_preference)\n    }\n\n    class ScannerPluginBase {\n        +scan(target, target_type, global_ignore_paths, config) ScanResultsContainer\n        +results_dir: Path\n        #_create_sarif_report(findings, tool_name) dict\n        #_write_sarif_report(sarif_report, filename) Path\n    }\n\n    class ScannerPluginConfigBase {\n        +name: str\n        +enabled: bool\n        +options: ScannerOptionsBase\n    }\n\n    class ScannerOptionsBase {\n        +severity_threshold: str\n        +include_tests: bool\n    }\n\n    class CustomScanner {\n        +scan(target, target_type, global_ignore_paths, config) ScanResultsContainer\n    }\n\n    PluginBase &lt;|-- ScannerPluginBase\n    ScannerPluginBase &lt;|-- CustomScanner\n    ScannerPluginConfigBase -- CustomScanner : configures\n    ScannerOptionsBase -- ScannerPluginConfigBase : contains\n</code></pre>"},{"location":"docs/plugins/scanner-plugins-diagrams/#scanner-plugin-configuration-flow","title":"Scanner Plugin Configuration Flow","text":"<p>The following diagram shows how configuration flows through a scanner plugin:</p> <pre><code>flowchart TD\n    A[.ash/.ash.yaml] --&gt; B[Configuration Parser]\n    C[CLI Arguments] --&gt; B\n    B --&gt; D[ASH Configuration]\n    D --&gt; E[Scanner Configuration]\n    E --&gt; F[Scanner Plugin]\n\n    subgraph Scanner Plugin\n        G[Validate Config] --&gt; H[Apply Config]\n        H --&gt; I[Use in Scan Logic]\n    end\n\n    F --&gt; J[ScanResultsContainer]\n</code></pre>"},{"location":"docs/plugins/scanner-plugins-diagrams/#scanner-integration-with-ash-core","title":"Scanner Integration with ASH Core","text":"<p>The following diagram shows how scanner plugins integrate with the ASH core:</p> <pre><code>flowchart TD\n    A[ASH CLI] --&gt; B[ASH Core]\n    B --&gt; C[Plugin Manager]\n    C --&gt; D[Scanner Registry]\n    D --&gt; E[Scanner Plugins]\n\n    E --&gt; F[Scan Results]\n    F --&gt; G[Results Aggregator]\n    G --&gt; H[Reporter Plugins]\n\n    I[Event System] -.-&gt; E\n    I -.-&gt; G\n    I -.-&gt; H\n</code></pre>"},{"location":"docs/plugins/scanner-plugins/","title":"Scanner Plugins","text":"<p>Scanner plugins perform security scans on files and generate findings. They are the core of ASH's security scanning functionality.</p> <p>For detailed visual diagrams of scanner plugin architecture and workflow, see Scanner Plugin Diagrams.</p>"},{"location":"docs/plugins/scanner-plugins/#scanner-plugin-interface","title":"Scanner Plugin Interface","text":"<p>Scanner plugins must implement the <code>ScannerPluginBase</code> interface:</p> <pre><code>from automated_security_helper.base.scanner_plugin import ScannerPluginBase, ScannerPluginConfigBase\nfrom automated_security_helper.plugins.decorators import ash_scanner_plugin\n\n@ash_scanner_plugin\nclass MyScanner(ScannerPluginBase):\n    \"\"\"My custom scanner implementation\"\"\"\n\n    def scan(self, target, target_type, global_ignore_paths=None, config=None):\n        \"\"\"Implement your scanning logic here\"\"\"\n        # Your code here\n</code></pre>"},{"location":"docs/plugins/scanner-plugins/#scanner-plugin-configuration","title":"Scanner Plugin Configuration","text":"<p>Define a configuration class for your scanner:</p> <pre><code>from pydantic import Field\n\nclass MyScannerConfig(ScannerPluginConfigBase):\n    name: str = \"my-scanner\"\n    enabled: bool = True\n\n    class Options:\n        severity_threshold: str = Field(default=\"MEDIUM\", description=\"Minimum severity level\")\n        include_tests: bool = Field(default=False, description=\"Include test files\")\n</code></pre>"},{"location":"docs/plugins/scanner-plugins/#scanner-plugin-example","title":"Scanner Plugin Example","text":"<p>Here's a complete example of a custom scanner plugin:</p> <pre><code>import json\nimport subprocess\nfrom pathlib import Path\nfrom typing import List, Literal\n\nfrom pydantic import Field\n\nfrom automated_security_helper.base.scanner_plugin import ScannerPluginBase, ScannerPluginConfigBase\nfrom automated_security_helper.plugins.decorators import ash_scanner_plugin\nfrom automated_security_helper.models.scan_results_container import ScanResultsContainer\n\nclass CustomScannerConfig(ScannerPluginConfigBase):\n    \"\"\"Configuration for CustomScanner\"\"\"\n    name: str = \"custom-scanner\"\n    enabled: bool = True\n\n    class Options:\n        tool_path: str = Field(default=\"custom-tool\", description=\"Path to the scanning tool\")\n        severity_threshold: str = Field(default=\"MEDIUM\", description=\"Minimum severity level\")\n\n@ash_scanner_plugin\nclass CustomScanner(ScannerPluginBase):\n    \"\"\"Custom scanner implementation\"\"\"\n\n    def model_post_init(self, context):\n        \"\"\"Initialize the scanner with configuration\"\"\"\n        if self.config is None:\n            self.config = CustomScannerConfig()\n\n        self.command = \"custom-tool\"\n        # Enable UV tool execution if your tool should be managed via UV\n        # self.use_uv_tool = True\n        self.tool_type = \"SAST\"  # or \"IAC\", \"SCA\", \"SECRETS\", etc.\n\n        super().model_post_init(context)\n\n    def scan(self, target: Path, target_type: Literal[\"source\", \"converted\"],\n             global_ignore_paths: List = None, config=None):\n        \"\"\"Scan the target using a custom tool\"\"\"\n        if config is None:\n            config = self.config\n\n        # Create results container\n        container = ScanResultsContainer()\n\n        try:\n            # Run the external tool\n            cmd = [config.options.tool_path, \"--scan\", str(target),\n                   \"--severity\", config.options.severity_threshold]\n\n            result = self._run_subprocess(\n                cmd,\n                stdout_preference=\"return\",\n                stderr_preference=\"write\"\n            )\n\n            # Parse the output\n            if result.stdout:\n                findings = json.loads(result.stdout)\n\n                # Create SARIF report\n                sarif_report = {\n                    \"version\": \"2.1.0\",\n                    \"$schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n                    \"runs\": [\n                        {\n                            \"tool\": {\n                                \"driver\": {\n                                    \"name\": config.name,\n                                    \"version\": \"1.0.0\"\n                                }\n                            },\n                            \"results\": []\n                        }\n                    ]\n                }\n\n                # Convert findings to SARIF format\n                for finding in findings:\n                    sarif_report[\"runs\"][0][\"results\"].append({\n                        \"ruleId\": finding[\"id\"],\n                        \"level\": finding[\"severity\"].lower(),\n                        \"message\": {\n                            \"text\": finding[\"message\"]\n                        },\n                        \"locations\": [\n                            {\n                                \"physicalLocation\": {\n                                    \"artifactLocation\": {\n                                        \"uri\": finding[\"file\"]\n                                    },\n                                    \"region\": {\n                                        \"startLine\": finding[\"line\"]\n                                    }\n                                }\n                            }\n                        ]\n                    })\n\n                # Write SARIF report\n                sarif_path = self.results_dir / f\"{config.name}.sarif\"\n                with open(sarif_path, \"w\") as f:\n                    json.dump(sarif_report, f, indent=2)\n\n                container.sarif_report = sarif_report\n\n        except Exception as e:\n            container.add_error(f\"Error running scanner: {str(e)}\")\n\n        return container\n</code></pre>"},{"location":"docs/plugins/scanner-plugins/#uv-tool-integration","title":"UV Tool Integration","text":"<p>ASH v3 supports UV tool integration for scanner plugins that use CLI tools. This provides automatic tool isolation and management without requiring users to manually install tools.</p>"},{"location":"docs/plugins/scanner-plugins/#enabling-uv-tool-integration","title":"Enabling UV Tool Integration","text":"<p>To enable UV tool integration in your scanner plugin:</p> <pre><code>@ash_scanner_plugin\nclass MyScanner(ScannerPluginBase):\n    def model_post_init(self, context):\n        if self.config is None:\n            self.config = MyScannerConfig()\n\n        self.command = \"my-tool\"\n        self.use_uv_tool = True  # Enable UV tool execution\n        self.tool_type = \"SAST\"\n\n        # Get tool version via UV\n        self.tool_version = self._get_uv_tool_version(\"my-tool\")\n\n        super().model_post_init(context)\n\n    def validate_plugin_dependencies(self) -&gt; bool:\n        \"\"\"Validate scanner dependencies\"\"\"\n        # UV tool availability is automatically validated\n        if not self._validate_uv_tool_availability():\n            return False\n\n        # For UV tool-based scanners, dependencies are satisfied if UV is available\n        if self.use_uv_tool:\n            self.dependencies_satisfied = True\n            return True\n\n        return super().validate_plugin_dependencies()\n</code></pre>"},{"location":"docs/plugins/scanner-plugins/#benefits-of-uv-tool-integration","title":"Benefits of UV Tool Integration","text":"<ul> <li>Automatic Tool Management: Tools are downloaded and managed automatically</li> <li>Isolation: Tools run in isolated environments without affecting project dependencies</li> <li>Version Control: Consistent tool versions across environments</li> <li>Fallback Support: Automatic fallback to direct execution if UV is unavailable</li> </ul>"},{"location":"docs/plugins/scanner-plugins/#built-in-uv-tool-scanners","title":"Built-in UV Tool Scanners","text":"<p>The following built-in scanners use UV tool integration: - Checkov: Infrastructure-as-Code scanning - Semgrep: Static Application Security Testing</p>"},{"location":"docs/plugins/scanner-plugins/#scanner-plugin-best-practices","title":"Scanner Plugin Best Practices","text":"<ol> <li>Generate SARIF Reports: SARIF is the standard format for security findings</li> <li>Handle Errors Gracefully: Use try/except blocks to handle errors</li> <li>Respect Global Ignore Paths: Skip files that are in the global ignore paths</li> <li>Use Subprocess Utilities: Use the provided <code>_run_subprocess</code> method for running external commands</li> <li>Add Metadata: Add useful metadata to the results container</li> <li>UV Tool Integration: For CLI tools that can be managed via UV, set <code>use_uv_tool = True</code> in your <code>model_post_init</code> method to enable automatic tool isolation and management</li> </ol>"},{"location":"docs/plugins/scanner-plugins/#scanner-plugin-configuration-in-ash","title":"Scanner Plugin Configuration in ASH","text":"<p>Configure your scanner in the ASH configuration file:</p> <pre><code># .ash/.ash.yaml\nscanners:\n  custom-scanner:\n    enabled: true\n    options:\n      tool_path: /path/to/custom-tool\n      severity_threshold: HIGH\n</code></pre>"},{"location":"docs/plugins/scanner-plugins/#testing-scanner-plugins","title":"Testing Scanner Plugins","text":"<p>Create unit tests for your scanner:</p> <pre><code>import pytest\nfrom pathlib import Path\n\nfrom automated_security_helper.base.plugin_context import PluginContext\nfrom my_ash_plugins.scanners import CustomScanner\n\ndef test_custom_scanner():\n    # Create a plugin context\n    context = PluginContext(\n        source_dir=Path(\"test_data\"),\n        output_dir=Path(\"test_output\")\n    )\n\n    # Create scanner instance\n    scanner = CustomScanner(context=context)\n\n    # Run the scanner\n    results = scanner.scan(Path(\"test_data/sample.py\"), \"source\")\n\n    # Assert results\n    assert results is not None\n    assert results.sarif_report is not None\n    assert len(results.sarif_report[\"runs\"][0][\"results\"]) &gt; 0\n</code></pre>"},{"location":"docs/plugins/aws/","title":"AWS Plugins","text":"<p>ASH includes powerful AWS-specific plugins that extend security reporting capabilities with cloud-native services, enabling enterprise-scale security monitoring, AI-powered analysis, and seamless integration with AWS security services.</p>"},{"location":"docs/plugins/aws/#overview","title":"Overview","text":"<p>AWS plugins provide:</p> <ul> <li>Cloud-native integration with AWS security and monitoring services</li> <li>Scalable storage and processing for large-scale security operations</li> <li>AI-powered analysis using Amazon Bedrock foundation models</li> <li>Compliance reporting through AWS Security Hub integration</li> <li>Real-time monitoring with CloudWatch Logs streaming</li> </ul>"},{"location":"docs/plugins/aws/#available-plugins","title":"Available Plugins","text":"Plugin Purpose Key Features Use Cases Security Hub Reporter AWS Security Hub integration ASFF format, batch processing, compliance mapping Centralized security monitoring, compliance reporting Bedrock Summary Reporter AI-powered summaries Executive summaries, technical analysis, multiple models Management reporting, risk assessment CloudWatch Logs Reporter Real-time logging Structured logging, metric filters, alarms Real-time monitoring, automated alerting S3 Reporter Cloud storage for reports Multiple formats, lifecycle management, analytics integration Long-term archival, data analytics"},{"location":"docs/plugins/aws/#quick-start","title":"Quick Start","text":""},{"location":"docs/plugins/aws/#basic-setup","title":"Basic Setup","text":"<ol> <li> <p>Configure AWS credentials:    <pre><code>aws configure\n# or use IAM roles (recommended)\n</code></pre></p> </li> <li> <p>Enable desired plugins:    <pre><code># ash-config.yml\nreporters:\n  aws-security-hub:\n    enabled: true\n    options:\n      aws_region: \"us-east-1\"\n\n  s3-reporter:\n    enabled: true\n    options:\n      bucket_name: \"my-security-reports\"\n      aws_region: \"us-east-1\"\n</code></pre></p> </li> <li> <p>Run scan with AWS reporters:    <pre><code>ash /path/to/code --reporters aws-security-hub,s3-reporter\n</code></pre></p> </li> </ol>"},{"location":"docs/plugins/aws/#enterprise-setup","title":"Enterprise Setup","text":"<p>For enterprise deployments, combine multiple AWS plugins:</p> <pre><code># Enterprise configuration\nreporters:\n  aws-security-hub:\n    enabled: true\n    options:\n      aws_region: \"us-east-1\"\n\n  bedrock-summary-reporter:\n    enabled: true\n    options:\n      model_id: \"anthropic.claude-3-sonnet-20240229-v1:0\"\n      aws_region: \"us-east-1\"\n      summary_style: \"executive\"\n\n  cloudwatch-logs:\n    enabled: true\n    options:\n      log_group_name: \"/aws/ash/security-scans\"\n      aws_region: \"us-east-1\"\n\n  s3-reporter:\n    enabled: true\n    options:\n      bucket_name: \"enterprise-security-reports\"\n      key_prefix: \"ash-scans\"\n      storage_class: \"STANDARD_IA\"\n      aws_region: \"us-east-1\"\n</code></pre>"},{"location":"docs/plugins/aws/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/plugins/aws/#aws-account-setup","title":"AWS Account Setup","text":"<ol> <li>AWS Account: Active AWS account with appropriate permissions</li> <li>AWS CLI: Installed and configured (<code>aws configure</code>)</li> <li>Service Access: Enable required AWS services in your regions</li> </ol>"},{"location":"docs/plugins/aws/#required-aws-services","title":"Required AWS Services","text":"Plugin Required Services Optional Services Security Hub Reporter AWS Security Hub AWS Config, AWS Inspector Bedrock Summary Reporter Amazon Bedrock - CloudWatch Logs Reporter Amazon CloudWatch Logs CloudWatch Alarms, EventBridge S3 Reporter Amazon S3 Amazon Athena, QuickSight"},{"location":"docs/plugins/aws/#iam-permissions","title":"IAM Permissions","text":"<p>Create an IAM policy for ASH AWS plugins:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"securityhub:BatchImportFindings\",\n        \"securityhub:GetFindings\",\n        \"bedrock:InvokeModel\",\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\",\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/plugins/aws/#authentication-methods","title":"Authentication Methods","text":""},{"location":"docs/plugins/aws/#1-aws-cli-configuration","title":"1. AWS CLI Configuration","text":"<pre><code>aws configure\n</code></pre>"},{"location":"docs/plugins/aws/#2-environment-variables","title":"2. Environment Variables","text":"<pre><code>export AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_REGION=\"us-east-1\"\n</code></pre>"},{"location":"docs/plugins/aws/#3-iam-roles-recommended","title":"3. IAM Roles (Recommended)","text":"<p>For EC2, ECS, Lambda, or other AWS services: <pre><code># No explicit credentials needed\n# IAM role attached to the service\n</code></pre></p>"},{"location":"docs/plugins/aws/#4-aws-profiles","title":"4. AWS Profiles","text":"<pre><code>export AWS_PROFILE=\"security-scanning\"\nash /path/to/code --reporters aws-security-hub\n</code></pre>"},{"location":"docs/plugins/aws/#integration-patterns","title":"Integration Patterns","text":""},{"location":"docs/plugins/aws/#cicd-pipeline-integration","title":"CI/CD Pipeline Integration","text":""},{"location":"docs/plugins/aws/#github-actions","title":"GitHub Actions","text":"<pre><code>name: Security Scan with AWS Integration\non: [push, pull_request]\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write  # For OIDC\n      contents: read\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v2\n      with:\n        role-to-assume: arn:aws:iam::123456789012:role/GitHubActions-ASH\n        aws-region: us-east-1\n\n    - name: Run ASH Security Scan\n      run: |\n        ash . --reporters aws-security-hub,bedrock-summary-reporter,s3-reporter\n\n    - name: Post AI Summary to PR\n      if: github.event_name == 'pull_request'\n      uses: actions/github-script@v6\n      with:\n        script: |\n          const fs = require('fs');\n          if (fs.existsSync('output/bedrock-summary.md')) {\n            const summary = fs.readFileSync('output/bedrock-summary.md', 'utf8');\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: `## \ud83d\udd12 Security Scan Summary\\n\\n${summary}`\n            });\n          }\n</code></pre>"},{"location":"docs/plugins/aws/#jenkins-pipeline","title":"Jenkins Pipeline","text":"<pre><code>pipeline {\n    agent any\n    environment {\n        AWS_REGION = 'us-east-1'\n        ASH_S3_BUCKET = 'jenkins-security-reports'\n    }\n\n    stages {\n        stage('Security Scan') {\n            steps {\n                withAWS(role: 'arn:aws:iam::123456789012:role/Jenkins-ASH') {\n                    sh '''\n                        ash . \\\n                          --reporters aws-security-hub,s3-reporter,cloudwatch-logs \\\n                          --config jenkins-ash-config.yml\n                    '''\n                }\n            }\n        }\n\n        stage('Process Results') {\n            steps {\n                script {\n                    // Archive results and send notifications\n                    def reportUrl = \"https://s3.console.aws.amazon.com/s3/buckets/${env.ASH_S3_BUCKET}\"\n                    currentBuild.description = \"Security Report: ${reportUrl}\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"docs/plugins/aws/#aws-lambda-integration","title":"AWS Lambda Integration","text":"<p>Deploy ASH as a Lambda function for serverless scanning:</p> <pre><code>import json\nimport subprocess\nimport boto3\nfrom pathlib import Path\n\ndef lambda_handler(event, context):\n    # Download code from S3 or CodeCommit\n    # Run ASH scan\n    # Results automatically go to configured AWS services\n\n    try:\n        # Example: Scan code from S3 trigger\n        bucket = event['Records'][0]['s3']['bucket']['name']\n        key = event['Records'][0]['s3']['object']['key']\n\n        # Download and extract code\n        s3 = boto3.client('s3')\n        s3.download_file(bucket, key, '/tmp/code.zip')\n\n        # Extract and scan\n        subprocess.run(['unzip', '/tmp/code.zip', '-d', '/tmp/code'])\n\n        # Run ASH with AWS reporters\n        result = subprocess.run([\n            'ash', 'scan', '/tmp/code',\n            '--reporters', 'aws-security-hub,cloudwatch-logs',\n            '--config', '/opt/ash-lambda-config.yml'\n        ], capture_output=True, text=True)\n\n        return {\n            'statusCode': 200,\n            'body': json.dumps({\n                'message': 'Scan completed successfully',\n                'findings_count': result.stdout.count('finding')\n            })\n        }\n\n    except Exception as e:\n        return {\n            'statusCode': 500,\n            'body': json.dumps({\n                'error': str(e)\n            })\n        }\n</code></pre>"},{"location":"docs/plugins/aws/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"docs/plugins/aws/#cloudwatch-dashboards","title":"CloudWatch Dashboards","text":"<p>Create dashboards to monitor security scan metrics:</p> <pre><code>{\n  \"widgets\": [\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"metrics\": [\n          [\"ASH/Security\", \"CriticalFindings\"],\n          [\".\", \"HighFindings\"],\n          [\".\", \"TotalFindings\"]\n        ],\n        \"period\": 300,\n        \"stat\": \"Sum\",\n        \"region\": \"us-east-1\",\n        \"title\": \"Security Findings Trend\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/plugins/aws/#eventbridge-rules","title":"EventBridge Rules","text":"<p>Automate responses to security findings:</p> <pre><code>{\n  \"Rules\": [\n    {\n      \"Name\": \"ASH-Critical-Finding-Alert\",\n      \"EventPattern\": {\n        \"source\": [\"aws.securityhub\"],\n        \"detail-type\": [\"Security Hub Findings - Imported\"],\n        \"detail\": {\n          \"findings\": {\n            \"ProductName\": [\"ASH\"],\n            \"Severity\": {\n              \"Label\": [\"CRITICAL\"]\n            }\n          }\n        }\n      },\n      \"Targets\": [\n        {\n          \"Id\": \"1\",\n          \"Arn\": \"arn:aws:sns:us-east-1:123456789012:security-critical-alerts\"\n        },\n        {\n          \"Id\": \"2\",\n          \"Arn\": \"arn:aws:lambda:us-east-1:123456789012:function:SecurityIncidentResponse\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/plugins/aws/#cost-management","title":"Cost Management","text":""},{"location":"docs/plugins/aws/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":"<ol> <li>Choose appropriate AWS regions for your workloads</li> <li>Use S3 lifecycle policies for long-term storage</li> <li>Select cost-effective Bedrock models for AI analysis</li> <li>Implement CloudWatch log retention policies</li> <li>Monitor usage with AWS Cost Explorer</li> </ol>"},{"location":"docs/plugins/aws/#cost-estimation","title":"Cost Estimation","text":"Plugin Primary Cost Factors Estimated Monthly Cost* Security Hub Reporter $0.0003 per finding $10-50 Bedrock Summary Reporter Model invocation tokens $5-100 CloudWatch Logs Reporter Log ingestion (GB) $5-25 S3 Reporter Storage and requests $1-10 <p>*Estimates based on typical usage patterns</p>"},{"location":"docs/plugins/aws/#cost-monitoring","title":"Cost Monitoring","text":"<p>Set up billing alerts:</p> <pre><code>aws budgets create-budget \\\n  --account-id 123456789012 \\\n  --budget '{\n    \"BudgetName\": \"ASH-AWS-Plugins\",\n    \"BudgetLimit\": {\n      \"Amount\": \"100\",\n      \"Unit\": \"USD\"\n    },\n    \"TimeUnit\": \"MONTHLY\",\n    \"BudgetType\": \"COST\"\n  }'\n</code></pre>"},{"location":"docs/plugins/aws/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/aws/#common-issues","title":"Common Issues","text":"<p>Authentication Errors <pre><code># Verify AWS credentials\naws sts get-caller-identity\n\n# Check IAM permissions\naws iam simulate-principal-policy \\\n  --policy-source-arn arn:aws:iam::123456789012:user/ash-user \\\n  --action-names securityhub:BatchImportFindings\n</code></pre></p> <p>Service Not Available <pre><code># Check service availability in region\naws securityhub describe-hub --region us-east-1\naws bedrock list-foundation-models --region us-east-1\n</code></pre></p> <p>Permission Denied - Review IAM policies and roles - Check service-specific permissions - Verify resource-based policies (S3 bucket policies, etc.)</p>"},{"location":"docs/plugins/aws/#debug-mode","title":"Debug Mode","text":"<p>Enable comprehensive debugging:</p> <pre><code># Enable debug logging for all AWS plugins\nash /path/to/code \\\n  --reporters aws-security-hub,bedrock-summary-reporter,cloudwatch-logs,s3-reporter \\\n  --log-level DEBUG\n</code></pre>"},{"location":"docs/plugins/aws/#best-practices","title":"Best Practices","text":""},{"location":"docs/plugins/aws/#security","title":"Security","text":"<ol> <li>Use IAM roles instead of access keys when possible</li> <li>Apply least privilege principle to IAM policies</li> <li>Enable CloudTrail for audit logging</li> <li>Encrypt data at rest and in transit</li> <li>Regularly rotate access keys and credentials</li> </ol>"},{"location":"docs/plugins/aws/#performance","title":"Performance","text":"<ol> <li>Choose appropriate AWS regions for latency</li> <li>Use batch processing for large numbers of findings</li> <li>Implement retry logic for transient failures</li> <li>Monitor API rate limits and implement backoff</li> <li>Cache results when appropriate</li> </ol>"},{"location":"docs/plugins/aws/#cost-management_1","title":"Cost Management","text":"<ol> <li>Monitor usage regularly with AWS Cost Explorer</li> <li>Set up billing alerts for unexpected costs</li> <li>Use appropriate storage classes for S3</li> <li>Implement lifecycle policies for log retention</li> <li>Choose cost-effective Bedrock models for your use case</li> </ol>"},{"location":"docs/plugins/aws/#operations","title":"Operations","text":"<ol> <li>Implement monitoring and alerting</li> <li>Set up automated responses to critical findings</li> <li>Create runbooks for common issues</li> <li>Test disaster recovery procedures</li> <li>Document configurations and processes</li> </ol>"},{"location":"docs/plugins/aws/#migration-guide","title":"Migration Guide","text":""},{"location":"docs/plugins/aws/#from-legacy-reporting","title":"From Legacy Reporting","text":"<p>If migrating from file-based reporting to AWS plugins:</p> <ol> <li>Assess current reporting needs</li> <li>Plan AWS service setup and permissions</li> <li>Test with non-production workloads</li> <li>Gradually migrate reporting workflows</li> <li>Update CI/CD pipelines and automation</li> </ol>"},{"location":"docs/plugins/aws/#configuration-migration","title":"Configuration Migration","text":"<pre><code># Before: File-based reporting\nreporters:\n  sarif:\n    enabled: true\n    output_file: \"results.sarif\"\n\n  html:\n    enabled: true\n    output_file: \"report.html\"\n\n# After: AWS-integrated reporting\nreporters:\n  aws-security-hub:\n    enabled: true\n    options:\n      aws_region: \"us-east-1\"\n\n  s3-reporter:\n    enabled: true\n    options:\n      bucket_name: \"security-reports\"\n      formats: [\"sarif\", \"html\"]\n\n  bedrock-summary-reporter:\n    enabled: true\n    options:\n      model_id: \"anthropic.claude-3-sonnet-20240229-v1:0\"\n</code></pre>"},{"location":"docs/plugins/aws/#next-steps","title":"Next Steps","text":"<ul> <li>Security Hub Reporter: Centralized security monitoring</li> <li>Bedrock Summary Reporter: AI-powered analysis</li> <li>CloudWatch Logs Reporter: Real-time monitoring</li> <li>S3 Reporter: Scalable storage and analytics</li> <li>ASH Configuration Guide: Advanced configuration options</li> </ul>"},{"location":"docs/plugins/aws/bedrock-summary-reporter-diagrams/","title":"Bedrock Summary Reporter Diagrams","text":"<p>This document provides visual diagrams of the ASH Bedrock Summary Reporter architecture and workflows using Mermaid.</p>"},{"location":"docs/plugins/aws/bedrock-summary-reporter-diagrams/#architecture-overview","title":"Architecture Overview","text":"<p>The following diagram shows the high-level architecture of the Bedrock Summary Reporter:</p> <pre><code>flowchart TD\n    A[ASH Core] --&gt; B[Bedrock Summary Reporter]\n    B --&gt; C[AWS SDK for Python]\n    C --&gt; D[Amazon Bedrock API]\n    D --&gt; E[Foundation Models]\n\n    B --&gt; F[Scan Results]\n    F --&gt; G[Findings Processor]\n    G --&gt; H[Prompt Generator]\n    H --&gt; C\n\n    E --&gt; I[AI-Generated Summary]\n    I --&gt; J[Report Formatter]\n    J --&gt; K[Markdown Reports]\n\n    subgraph \"AWS Cloud\"\n        D\n        E\n    end\n\n    subgraph \"Local Processing\"\n        A\n        B\n        F\n        G\n        H\n        J\n        K\n    end\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter-diagrams/#sequence-diagram","title":"Sequence Diagram","text":"<p>The following diagram shows the sequence of operations in the Bedrock Summary Reporter:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant BSR as Bedrock Summary Reporter\n    participant SDK as AWS SDK for Python\n    participant Bedrock as Amazon Bedrock\n    participant FM as Foundation Model\n    participant FS as File System\n\n    ASH-&gt;&gt;BSR: report(aggregated_results)\n    BSR-&gt;&gt;BSR: Process Scan Results\n    BSR-&gt;&gt;BSR: Generate Prompt\n\n    BSR-&gt;&gt;SDK: _invoke_bedrock_model()\n    SDK-&gt;&gt;Bedrock: bedrock_runtime.converse()\n    Bedrock-&gt;&gt;FM: Process Prompt\n\n    alt Streaming Response\n        FM--&gt;&gt;Bedrock: Stream Chunks\n        Bedrock--&gt;&gt;SDK: Stream Response\n        SDK--&gt;&gt;BSR: Process Chunks\n        BSR-&gt;&gt;BSR: Accumulate Response\n    else Standard Response\n        FM--&gt;&gt;Bedrock: Complete Response\n        Bedrock--&gt;&gt;SDK: Return Response\n        SDK--&gt;&gt;BSR: Return Response\n    end\n\n    BSR-&gt;&gt;BSR: Format AI Response\n    BSR-&gt;&gt;FS: Write Executive Summary\n    BSR-&gt;&gt;FS: Write Technical Analysis\n    BSR-&gt;&gt;FS: Write Full Report\n\n    BSR--&gt;&gt;ASH: Return Report Path\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter-diagrams/#chunking-and-processing-flow","title":"Chunking and Processing Flow","text":"<p>The following diagram shows how the Bedrock Summary Reporter processes large scan results in chunks:</p> <pre><code>flowchart TD\n    A[Scan Results] --&gt; B[Findings Processor]\n\n    B --&gt; C[Group by Severity]\n    C --&gt; D[Critical Findings]\n    C --&gt; E[High Findings]\n    C --&gt; F[Medium Findings]\n    C --&gt; G[Low Findings]\n\n    D --&gt; H[Chunk 1: Critical]\n    E --&gt; I[Chunk 2: High]\n    F --&gt; J[Chunk 3: Medium]\n    G --&gt; K[Chunk 4: Low]\n\n    H --&gt; L[Generate Prompt 1]\n    I --&gt; M[Generate Prompt 2]\n    J --&gt; N[Generate Prompt 3]\n    K --&gt; O[Generate Prompt 4]\n\n    L --&gt; P[Bedrock API Call 1]\n    M --&gt; Q[Bedrock API Call 2]\n    N --&gt; R[Bedrock API Call 3]\n    O --&gt; S[Bedrock API Call 4]\n\n    P --&gt; T[Response 1]\n    Q --&gt; U[Response 2]\n    R --&gt; V[Response 3]\n    S --&gt; W[Response 4]\n\n    T --&gt; X[Merge Responses]\n    U --&gt; X\n    V --&gt; X\n    W --&gt; X\n\n    X --&gt; Y[Final Report]\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter-diagrams/#report-generation-process","title":"Report Generation Process","text":"<p>The following diagram shows the report generation process:</p> <pre><code>flowchart TD\n    A[Scan Results] --&gt; B[Extract Metadata]\n    A --&gt; C[Extract Findings]\n\n    B --&gt; D[Project Context]\n    C --&gt; E[Findings Analysis]\n\n    D --&gt; F[Generate Executive Context]\n    E --&gt; G[Generate Technical Context]\n\n    F --&gt; H[Executive Prompt]\n    G --&gt; I[Technical Prompt]\n\n    H --&gt; J[Bedrock API]\n    I --&gt; J\n\n    J --&gt; K[Executive Summary]\n    J --&gt; L[Technical Analysis]\n\n    K --&gt; M[Executive Report]\n    L --&gt; N[Technical Report]\n\n    M --&gt; O[Final Report]\n    N --&gt; O\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter-diagrams/#model-selection-logic","title":"Model Selection Logic","text":"<p>The following diagram shows the model selection logic with fallback support:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B{Config has model_id?}\n    B --&gt;|Yes| C[Use configured model]\n    B --&gt;|No| D{Environment variable set?}\n\n    D --&gt;|Yes| E[Use env var model]\n    D --&gt;|No| F[Use default model]\n\n    C --&gt; G[Validate Model Access]\n    E --&gt; G\n    F --&gt; G\n\n    G --&gt;|Access OK| H[Use Selected Model]\n    G --&gt;|No Access| I{Fallback Enabled?}\n\n    I --&gt;|Yes| J[Get Fallback Model]\n    I --&gt;|No| K[Raise Error]\n\n    J --&gt; L[Validate Fallback]\n    L --&gt;|Access OK| H\n    L --&gt;|No Access| K\n\n    H --&gt; M[End]\n    K --&gt; N[End with Error]\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter-diagrams/#cost-optimization-strategy","title":"Cost Optimization Strategy","text":"<p>The following diagram shows the cost optimization strategy:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B[Analyze Scan Results]\n\n    B --&gt; C{Group by Severity?}\n    C --&gt;|Yes| D[Group Findings by Severity]\n    C --&gt;|No| E[Use All Findings]\n\n    D --&gt; F[Process Each Severity Group]\n    E --&gt; G[Process All Findings Together]\n\n    F --&gt; H[Apply Finding Limits]\n    G --&gt; H\n\n    H --&gt; I[Generate Prompt]\n    I --&gt; J[Execute API Call]\n    J --&gt; K[End]\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter-diagrams/#integration-with-aws-services","title":"Integration with AWS Services","text":"<p>The following diagram shows how the Bedrock Summary Reporter integrates with other AWS services:</p> <pre><code>flowchart LR\n    A[ASH Core] --&gt; B[Bedrock Summary Reporter]\n\n    B --&gt; C[Amazon Bedrock]\n    B --&gt; D[Amazon S3]\n    B --&gt; E[AWS CloudWatch]\n\n    C --&gt; F[Foundation Models]\n    D --&gt; G[Report Storage]\n    E --&gt; H[Usage Metrics]\n\n    I[IAM] -.-&gt; C\n    I -.-&gt; D\n    I -.-&gt; E\n\n    J[AWS SDK for Python] -.-&gt; C\n    J -.-&gt; D\n    J -.-&gt; E\n\n    K[AWS CLI] -.-&gt; L[Configure]\n    L -.-&gt; I\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter-diagrams/#error-handling-flow","title":"Error Handling Flow","text":"<p>The following diagram shows the error handling flow with retry logic and fallback models:</p> <pre><code>flowchart TD\n    A[Start API Call] --&gt; B{API Call Successful?}\n\n    B --&gt;|Yes| C[Process Response]\n    B --&gt;|No| D{Error Type?}\n\n    D --&gt;|Throttling| E[Apply Exponential Backoff]\n    D --&gt;|Access Denied| F[Check IAM Permissions]\n    D --&gt;|Model Not Found| G[Check Model Availability]\n    D --&gt;|Other| H[Log Error Details]\n\n    E --&gt; I[Retry API Call]\n    F --&gt; J[Log Permission Error]\n    G --&gt; K{Fallback Enabled?}\n    H --&gt; L[Return Error Status]\n\n    K --&gt;|Yes| M[Try Fallback Model]\n    K --&gt;|No| L\n\n    I --&gt; N{Retry Successful?}\n    N --&gt;|Yes| C\n    N --&gt;|No| O{Max Retries Reached?}\n\n    O --&gt;|Yes| L\n    O --&gt;|No| E\n\n    M --&gt; P{Fallback Available?}\n    P --&gt;|Yes| Q[Use Fallback]\n    P --&gt;|No| L\n\n    Q --&gt; R{Fallback Successful?}\n    R --&gt;|Yes| C\n    R --&gt;|No| L\n\n    C --&gt; S[Complete Processing]\n    J --&gt; T[End with Error]\n    L --&gt; T\n    S --&gt; U[End Successfully]\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter-diagrams/#custom-prompt-flow","title":"Custom Prompt Flow","text":"<p>The following diagram shows the custom prompt generation flow:</p> <pre><code>flowchart TD\n    A[Start Prompt Generation] --&gt; B{Custom Prompt Provided?}\n\n    B --&gt;|Yes| C[Use Custom Prompt Template]\n    B --&gt;|No| D{Summary Style?}\n\n    D --&gt;|Executive| E[Use Executive Template]\n    D --&gt;|Technical| F[Use Technical Template]\n    D --&gt;|Detailed| G[Use Detailed Template]\n\n    C --&gt; H[Insert Scan Metadata]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    H --&gt; I[Insert Finding Data]\n    I --&gt; J{Include Code Snippets?}\n\n    J --&gt;|Yes| K[Add Code Snippets]\n    J --&gt;|No| L[Skip Code Snippets]\n\n    K --&gt; M[Format Final Prompt]\n    L --&gt; M\n\n    M --&gt; N{Prompt Size &gt; Limit?}\n    N --&gt;|Yes| O[Apply Truncation Strategy]\n    N --&gt;|No| P[Use Full Prompt]\n\n    O --&gt; Q[Final Prompt]\n    P --&gt; Q\n\n    Q --&gt; R[End]\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/","title":"Bedrock Summary Reporter","text":"<p>Generates AI-powered executive summaries and detailed security analysis using Amazon Bedrock's foundation models, providing human-readable insights from ASH scan results.</p> <p>For detailed visual diagrams of the Bedrock Summary Reporter architecture and workflow, see Bedrock Summary Reporter Diagrams.</p> <p>Note: This reporter has been updated to match the documentation. All features described in this document are now implemented.</p>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#overview","title":"Overview","text":"<p>The Bedrock Summary Reporter leverages Amazon Bedrock to:</p> <ul> <li>Generate executive summaries for stakeholders and management</li> <li>Provide detailed technical analysis with remediation recommendations</li> <li>Create risk assessments based on finding severity and context</li> <li>Support multiple foundation models for different analysis styles</li> <li>Customize output format for various audiences</li> </ul>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#configuration","title":"Configuration","text":""},{"location":"docs/plugins/aws/bedrock-summary-reporter/#basic-configuration","title":"Basic Configuration","text":"<pre><code>reporters:\n  bedrock-summary-reporter:\n    enabled: true\n    options:\n      model_id: \"anthropic.claude-v2\"\n      aws_region: \"us-east-1\"\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>reporters:\n  bedrock-summary-reporter:\n    enabled: true\n    options:\n      model_id: \"anthropic.claude-instant-v1\"\n      aws_region: \"us-west-2\"\n      max_tokens: 4000\n      temperature: 0.1\n      top_p: 0.9\n      include_code_snippets: true\n      summary_style: \"executive\"  # executive, technical, or detailed\n      custom_prompt: \"Focus on business impact and compliance risks\"\n      # Retry and fallback configuration\n      max_retries: 5      # Default: 3\n      base_delay: 2.0     # Default: 1.0 seconds\n      max_delay: 120.0    # Default: 60.0 seconds\n      enable_fallback_models: true\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#environment-variables","title":"Environment Variables","text":"<pre><code># AWS region\nexport AWS_REGION=\"us-east-1\"\n\n# Bedrock model ID\nexport ASH_BEDROCK_MODEL_ID=\"anthropic.claude-v2\"\n\n# Custom configuration\nexport ASH_BEDROCK_MAX_TOKENS=\"3000\"\nexport ASH_BEDROCK_TEMPERATURE=\"0.2\"\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/plugins/aws/bedrock-summary-reporter/#amazon-bedrock-setup","title":"Amazon Bedrock Setup","text":"<ol> <li>Enable Bedrock in your AWS account and region</li> <li> <p>Request model access for your chosen foundation models:    <pre><code># Check available models\naws bedrock list-foundation-models --region us-east-1\n</code></pre></p> </li> <li> <p>Grant model access through the AWS Console:</p> </li> <li>Navigate to Amazon Bedrock \u2192 Model access</li> <li>Request access to desired models (Claude, Titan, etc.)</li> </ol>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#iam-permissions","title":"IAM Permissions","text":"<p>The reporter requires the following IAM permissions:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"bedrock:InvokeModel\",\n        \"bedrock:InvokeModelWithResponseStream\",\n        \"bedrock:ListFoundationModels\",\n        \"bedrock:GetFoundationModel\"\n      ],\n      \"Resource\": [\n        \"arn:aws:bedrock:*::foundation-model/anthropic.claude-*\",\n        \"arn:aws:bedrock:*::foundation-model/amazon.titan-*\",\n        \"arn:aws:bedrock:*::foundation-model/ai21.j2-*\",\n        \"arn:aws:bedrock:*::foundation-model/cohere.command-*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#aws-credentials","title":"AWS Credentials","text":"<p>Ensure AWS credentials are configured using one of:</p> <ul> <li>AWS CLI: <code>aws configure</code></li> <li>Environment variables: <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code></li> <li>IAM roles (recommended for EC2/ECS/Lambda)</li> <li>AWS profiles: <code>AWS_PROFILE=myprofile</code></li> </ul>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#supported-models","title":"Supported Models","text":"<p>The Bedrock Summary Reporter works with any text-based foundation model available in Amazon Bedrock. Best results have been observed with Amazon Nova and Anthropic Claude models due to their strong reasoning capabilities and context handling.</p>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#model-selection-guidelines","title":"Model Selection Guidelines","text":"<pre><code># Example configuration with Anthropic Claude model\nmodel_id: \"anthropic.claude-v2\"\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#model-recommendations","title":"Model Recommendations","text":"Use Case Recommended Model Types Considerations Detailed Analysis Claude, Nova Best for comprehensive security analysis Quick Summaries Lighter models Faster response, more cost-effective CI/CD Integration Optimized for speed Choose models with lower latency"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#checking-available-models","title":"Checking Available Models","text":"<p>To see which models are available in your AWS account:</p> <pre><code># List available foundation models\naws bedrock list-foundation-models --region us-east-1\n</code></pre> <p>Note: Model availability may vary by region and account. Ensure you have requested access to your preferred models in the Amazon Bedrock console.</p>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#features","title":"Features","text":""},{"location":"docs/plugins/aws/bedrock-summary-reporter/#executive-summary-generation","title":"Executive Summary Generation","text":"<p>Generates concise summaries for leadership:</p> <pre><code># Security Scan Executive Summary\n\n## Overview\nYour codebase scan identified **15 security findings** across 3 categories, with **2 critical issues** requiring immediate attention.\n\n## Key Risks\n- **Hardcoded credentials** in configuration files (CRITICAL)\n- **SQL injection vulnerabilities** in user input handling (HIGH)\n- **Insecure cryptographic practices** in data encryption (MEDIUM)\n\n## Business Impact\n- **Compliance risk**: Potential GDPR violations due to data exposure\n- **Security risk**: Unauthorized access to customer data\n- **Operational risk**: Potential service disruption\n\n## Recommended Actions\n1. Immediately rotate exposed credentials\n2. Implement parameterized queries for database access\n3. Update cryptographic libraries to current standards\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#technical-analysis","title":"Technical Analysis","text":"<p>Provides detailed technical insights:</p> <pre><code># Technical Security Analysis\n\n## Critical Findings Analysis\n\n### 1. Hardcoded API Keys (2 instances)\n**Location**: `src/config/database.py:15`, `src/utils/api_client.py:23`\n**Risk**: Direct exposure of authentication credentials\n**Remediation**:\n- Move credentials to environment variables\n- Implement AWS Secrets Manager integration\n- Add credential scanning to CI/CD pipeline\n\n### 2. SQL Injection Vulnerability\n**Location**: `src/models/user.py:45-52`\n**Risk**: Potential database compromise\n**Code Pattern**: Direct string concatenation in SQL queries\n**Remediation**:\n- Replace with parameterized queries\n- Implement input validation\n- Add SQL injection testing\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#risk-assessment","title":"Risk Assessment","text":"<p>Provides contextual risk analysis:</p> <pre><code># Risk Assessment\n\n## Overall Risk Score: HIGH (7.5/10)\n\n### Risk Breakdown\n- **Critical Issues**: 2 (immediate action required)\n- **High Severity**: 5 (address within 1 week)\n- **Medium Severity**: 6 (address within 1 month)\n- **Low Severity**: 2 (address in next sprint)\n\n### Compliance Impact\n- **SOC 2**: Critical findings may impact Type II compliance\n- **PCI DSS**: Payment processing code requires immediate attention\n- **GDPR**: Data handling practices need review\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#usage-examples","title":"Usage Examples","text":""},{"location":"docs/plugins/aws/bedrock-summary-reporter/#basic-usage","title":"Basic Usage","text":"<pre><code># Generate AI summary with default settings\nash /path/to/code --reporters bedrock-summary-reporter\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#custom-model-and-style","title":"Custom Model and Style","text":"<pre><code># Use a lighter model for faster, cost-effective summaries\nexport ASH_BEDROCK_MODEL_ID=\"anthropic.claude-instant-v1\"\nash /path/to/code --reporters bedrock-summary-reporter\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># GitHub Actions example\n- name: Security Scan with AI Summary\n  env:\n    AWS_REGION: us-east-1\n    ASH_BEDROCK_MODEL_ID: \"anthropic.claude-3-sonnet-20240229-v1:0\"\n    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n  run: |\n    ash . --reporters sarif,bedrock-summary-reporter\n\n- name: Post Summary to PR\n  uses: actions/github-script@v6\n  with:\n    script: |\n      const fs = require('fs');\n      const summary = fs.readFileSync('output/bedrock-summary.md', 'utf8');\n      github.rest.issues.createComment({\n        issue_number: context.issue.number,\n        owner: context.repo.owner,\n        repo: context.repo.repo,\n        body: summary\n      });\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#output-formats","title":"Output Formats","text":""},{"location":"docs/plugins/aws/bedrock-summary-reporter/#markdown-report","title":"Markdown Report","text":"<p>The primary output is a comprehensive Markdown report:</p> <pre><code>output/\n\u251c\u2500\u2500 bedrock-summary.md          # Main summary report\n\u251c\u2500\u2500 bedrock-executive.md        # Executive summary only\n\u2514\u2500\u2500 bedrock-technical.md        # Technical details only\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#customizable-sections","title":"Customizable Sections","text":"<p>Configure which sections to include:</p> <pre><code>options:\n  include_sections:\n    - executive_summary\n    - risk_assessment\n    - technical_analysis\n    - remediation_guide\n    - compliance_impact\n  exclude_sections:\n    - code_snippets\n    - detailed_findings\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#customization","title":"Customization","text":""},{"location":"docs/plugins/aws/bedrock-summary-reporter/#custom-prompts","title":"Custom Prompts","text":"<p>Tailor the AI analysis to your needs:</p> <pre><code>options:\n  custom_prompts:\n    executive: |\n      Create an executive summary focusing on:\n      - Business impact and financial risk\n      - Compliance implications\n      - Strategic recommendations\n      - Timeline for remediation\n\n    technical: |\n      Provide technical analysis including:\n      - Root cause analysis\n      - Specific remediation steps\n      - Code examples where helpful\n      - Testing recommendations\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#industry-specific-analysis","title":"Industry-Specific Analysis","text":"<p>Configure for specific industries:</p> <pre><code>options:\n  industry_context: \"healthcare\"  # healthcare, finance, retail, etc.\n  compliance_frameworks: [\"HIPAA\", \"SOC2\", \"GDPR\"]\n  custom_context: |\n    This application processes patient health information\n    and must comply with HIPAA requirements.\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#cost-optimization","title":"Cost Optimization","text":""},{"location":"docs/plugins/aws/bedrock-summary-reporter/#model-selection","title":"Model Selection","text":"<p>Choose models based on your needs:</p> Model Type Speed Cost Quality Best For Lighter Models Fast Low Good Quick summaries, CI/CD Mid-tier Models Medium Medium Excellent Balanced analysis Advanced Models Slow High Superior Detailed reports"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#token-management","title":"Token Management","text":"<p>Optimize token usage:</p> <pre><code>options:\n  max_tokens: 2000        # Limit response length\n  temperature: 0.1        # Reduce randomness for consistency\n  summarize_findings: true # Pre-process findings to reduce input size\n  batch_processing: true   # Process multiple scans together\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#usage-monitoring","title":"Usage Monitoring","text":"<p>Monitor Bedrock usage and costs:</p> <pre><code># Check Bedrock usage\naws bedrock get-model-invocation-logging-configuration\n\n# Monitor costs with CloudWatch\naws logs filter-log-events \\\n  --log-group-name \"/aws/bedrock/modelinvocations\" \\\n  --start-time 1640995200000\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/aws/bedrock-summary-reporter/#common-issues","title":"Common Issues","text":"<p>Model Access Denied <pre><code># Check model access status\naws bedrock list-foundation-models --region us-east-1\n\n# Request access through AWS Console\n# Bedrock \u2192 Model access \u2192 Request model access\n</code></pre></p> <p>Token Limit Exceeded <pre><code># Reduce max_tokens or enable summarization\noptions:\n  max_tokens: 1500\n  summarize_findings: true\n</code></pre></p> <p>High Costs <pre><code># Use more cost-effective model\noptions:\n  model_id: \"anthropic.claude-instant-v1\"\n  max_tokens: 1000\n</code></pre></p> <p>API Errors <pre><code># Configure retry behavior\noptions:\n  max_retries: 5      # Default: 3\n  base_delay: 2.0     # Default: 1.0 seconds\n  max_delay: 120.0    # Default: 60.0 seconds\n  enable_fallback_models: true  # Enable fallback models\n</code></pre></p>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging:</p> <pre><code>ash /path/to/code --reporters bedrock-summary-reporter --log-level DEBUG\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#best-practices","title":"Best Practices","text":"<ol> <li>Choose appropriate models based on use case and budget</li> <li>Set token limits to control costs</li> <li>Use custom prompts for industry-specific analysis</li> <li>Monitor usage and costs regularly</li> <li>Cache results for repeated analysis of the same codebase</li> <li>Combine with other reporters for comprehensive reporting</li> <li>Review AI-generated content for accuracy and relevance</li> </ol>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#integration-examples","title":"Integration Examples","text":""},{"location":"docs/plugins/aws/bedrock-summary-reporter/#slack-integration","title":"Slack Integration","text":"<p>Post summaries to Slack channels:</p> <pre><code>import requests\nimport json\n\ndef post_to_slack(summary_file, webhook_url):\n    with open(summary_file, 'r') as f:\n        summary = f.read()\n\n    payload = {\n        \"text\": \"Security Scan Summary\",\n        \"attachments\": [{\n            \"color\": \"warning\",\n            \"text\": summary[:1000] + \"...\" if len(summary) &gt; 1000 else summary\n        }]\n    }\n\n    requests.post(webhook_url, json=payload)\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#email-reports","title":"Email Reports","text":"<p>Send executive summaries via email:</p> <pre><code>import boto3\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\n\ndef send_email_summary(summary_file, recipients):\n    ses = boto3.client('ses')\n\n    with open(summary_file, 'r') as f:\n        summary = f.read()\n\n    msg = MIMEMultipart()\n    msg['Subject'] = 'Security Scan Executive Summary'\n    msg['From'] = 'security@company.com'\n    msg['To'] = ', '.join(recipients)\n\n    msg.attach(MIMEText(summary, 'plain'))\n\n    ses.send_raw_email(\n        Source=msg['From'],\n        Destinations=recipients,\n        RawMessage={'Data': msg.as_string()}\n    )\n</code></pre>"},{"location":"docs/plugins/aws/bedrock-summary-reporter/#related-documentation","title":"Related Documentation","text":"<ul> <li>Amazon Bedrock User Guide</li> <li>Foundation Models in Amazon Bedrock</li> <li>ASH Configuration Guide</li> <li>Other AWS Reporters</li> </ul>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter-diagrams/","title":"CloudWatch Logs Reporter Diagrams","text":"<p>This document provides visual diagrams of the ASH CloudWatch Logs Reporter architecture and workflows using Mermaid.</p>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter-diagrams/#architecture-overview","title":"Architecture Overview","text":"<p>The following diagram shows the high-level architecture of the CloudWatch Logs Reporter:</p> <pre><code>flowchart TD\n    A[ASH Core] --&gt; B[CloudWatch Logs Reporter]\n    B --&gt; C[AWS SDK for Python]\n    C --&gt; D[CloudWatch Logs API]\n\n    B --&gt; E[Scan Results]\n    E --&gt; F[JSON Formatter]\n    F --&gt; G[Log Event Creator]\n    G --&gt; C\n\n    D --&gt; H[CloudWatch Logs]\n    H --&gt; I[CloudWatch Dashboards]\n    H --&gt; J[CloudWatch Alarms]\n    H --&gt; K[CloudWatch Insights]\n\n    subgraph \"AWS Cloud\"\n        D\n        H\n        I\n        J\n        K\n    end\n\n    subgraph \"Local Processing\"\n        A\n        B\n        E\n        F\n        G\n    end\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter-diagrams/#sequence-diagram","title":"Sequence Diagram","text":"<p>The following diagram shows the sequence of operations in the CloudWatch Logs Reporter:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant CLR as CloudWatch Logs Reporter\n    participant SDK as AWS SDK\n    participant CWL as CloudWatch Logs\n\n    ASH-&gt;&gt;CLR: report(aggregated_results)\n    CLR-&gt;&gt;CLR: Process Scan Results\n    CLR-&gt;&gt;CLR: Convert to JSON\n\n    CLR-&gt;&gt;SDK: Create Log Stream Request\n    SDK-&gt;&gt;CWL: API Call\n    CWL--&gt;&gt;SDK: Response\n    SDK--&gt;&gt;CLR: Return Response\n\n    CLR-&gt;&gt;CLR: Create Log Event\n    CLR-&gt;&gt;SDK: Put Log Events Request\n    SDK-&gt;&gt;CWL: API Call\n    CWL--&gt;&gt;SDK: Response\n    SDK--&gt;&gt;CLR: Return Response\n\n    CLR-&gt;&gt;CLR: Process API Response\n    CLR-&gt;&gt;CLR: Handle Errors\n\n    CLR--&gt;&gt;ASH: Return Report Status\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter-diagrams/#log-event-creation-process","title":"Log Event Creation Process","text":"<p>The following diagram shows the log event creation process:</p> <pre><code>flowchart TD\n    A[ASH Aggregated Results] --&gt; B[Extract Metadata]\n    A --&gt; C[Convert to Simple Dict]\n\n    B --&gt; D[Generate Timestamp]\n    C --&gt; E[Serialize to JSON]\n\n    D --&gt; F[Create Log Event]\n    E --&gt; F\n\n    F --&gt; G[Log Event Object]\n    G --&gt; H[CloudWatch Logs API]\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter-diagrams/#error-handling-flow","title":"Error Handling Flow","text":"<p>The following diagram shows the error handling flow with retry logic:</p> <pre><code>flowchart TD\n    A[Start API Call] --&gt; B{API Call Successful?}\n\n    B --&gt;|Yes| C[Process Response]\n    B --&gt;|No| D{Error Type?}\n\n    D --&gt;|Throttling| E[Apply Exponential Backoff]\n    D --&gt;|Access Denied| F[Check IAM Permissions]\n    D --&gt;|Resource Not Found| G[Log Resource Error]\n    D --&gt;|Other| H[Log Error Details]\n\n    E --&gt; I[Retry API Call]\n    F --&gt; J[Log Permission Error]\n    G --&gt; K[Return Error Status]\n    H --&gt; K\n\n    I --&gt; L{Retry Successful?}\n    L --&gt;|Yes| C\n    L --&gt;|No| M{Max Retries Reached?}\n\n    M --&gt;|Yes| K\n    M --&gt;|No| E\n\n    C --&gt; N[Return Success Response]\n    J --&gt; O[Return Error Message]\n    K --&gt; O\n\n    N --&gt; P[End]\n    O --&gt; P\n</code></pre> <p>Note: The implementation now includes retry logic with exponential backoff, improving the reliability of the CloudWatch Logs Reporter.</p>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter-diagrams/#integration-with-cloudwatch-services","title":"Integration with CloudWatch Services","text":"<p>The following diagram shows how the CloudWatch Logs Reporter integrates with other CloudWatch services:</p> <pre><code>flowchart LR\n    A[ASH Scan Results] --&gt; B[CloudWatch Logs Reporter]\n\n    B --&gt; C[CloudWatch Logs]\n\n    C --&gt; D[CloudWatch Dashboards]\n    C --&gt; E[CloudWatch Alarms]\n    C --&gt; F[CloudWatch Insights]\n    C --&gt; G[CloudWatch Metrics]\n\n    D --&gt; H[Visualization]\n    E --&gt; I[Notifications]\n    F --&gt; J[Log Analysis]\n    G --&gt; K[Metrics Analysis]\n\n    subgraph \"CloudWatch Ecosystem\"\n        C\n        D\n        E\n        F\n        G\n        H\n        I\n        J\n        K\n    end\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter-diagrams/#log-group-and-stream-management","title":"Log Group and Stream Management","text":"<p>The following diagram shows the log group and stream management process:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B{Log Group Exists?}\n\n    B --&gt;|Yes| C[Use Existing Log Group]\n    B --&gt;|No| D[Error: Log Group Required]\n\n    C --&gt; E{Log Stream Exists?}\n\n    E --&gt;|Yes| F[Use Existing Log Stream]\n    E --&gt;|No| G[Create Log Stream with Retry]\n\n    G --&gt; H{Creation Successful?}\n\n    H --&gt;|Yes| F\n    H --&gt;|No| I[Log Error and Continue]\n\n    F --&gt; J[Put Log Events with Retry]\n    I --&gt; J\n\n    J --&gt; K{API Call Successful?}\n\n    K --&gt;|Yes| L[Return Success]\n    K --&gt;|No| M[Log Error]\n\n    M --&gt; N[Return Error]\n    L --&gt; O[End]\n    N --&gt; O\n    D --&gt; O\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter-diagrams/#configuration-flow","title":"Configuration Flow","text":"<p>The following diagram shows the configuration flow:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B{AWS Region Set?}\n\n    B --&gt;|Yes| C{Log Group Name Set?}\n    B --&gt;|No| D[Use AWS_REGION Environment Variable]\n\n    D --&gt; E{Environment Variable Set?}\n\n    E --&gt;|Yes| C\n    E --&gt;|No| F[Validation Fails]\n\n    C --&gt;|Yes| G[Validation Succeeds]\n    C --&gt;|No| H[Use ASH_CLOUDWATCH_LOG_GROUP_NAME Environment Variable]\n\n    H --&gt; I{Environment Variable Set?}\n\n    I --&gt;|Yes| G\n    I --&gt;|No| F\n\n    G --&gt; J[Reporter Ready]\n    F --&gt; K[Reporter Disabled]\n\n    J --&gt; L[End]\n    K --&gt; L\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/","title":"CloudWatch Logs Reporter","text":"<p>Streams ASH scan results to Amazon CloudWatch Logs for real-time monitoring and analysis.</p> <p>For detailed visual diagrams of the CloudWatch Logs Reporter architecture and workflow, see CloudWatch Logs Reporter Diagrams.</p>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#overview","title":"Overview","text":"<p>The CloudWatch Logs Reporter publishes security scan results directly to Amazon CloudWatch Logs, enabling:</p> <ul> <li>Real-time monitoring of security scan results</li> <li>Integration with CloudWatch alarms for automated alerting</li> <li>Centralized logging across multiple scan environments</li> <li>Long-term retention with configurable log retention policies</li> </ul>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#configuration","title":"Configuration","text":""},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#basic-configuration","title":"Basic Configuration","text":"<pre><code>reporters:\n  cloudwatch-logs:\n    enabled: true\n    options:\n      aws_region: \"us-east-1\"\n      log_group_name: \"/aws/ash/scan-results\"\n      log_stream_name: \"ASHScanResults\"\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#environment-variables","title":"Environment Variables","text":"<p>The reporter supports configuration via environment variables:</p> <pre><code># AWS region (falls back to AWS_DEFAULT_REGION)\nexport AWS_REGION=\"us-east-1\"\n\n# CloudWatch log group name\nexport ASH_CLOUDWATCH_LOG_GROUP_NAME=\"/aws/ash/scan-results\"\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#complete-configuration-example","title":"Complete Configuration Example","text":"<pre><code>reporters:\n  cloudwatch-logs:\n    enabled: true\n    options:\n      aws_region: \"us-east-1\"\n      log_group_name: \"/aws/ash/security-scans\"\n      log_stream_name: \"production-scans\"\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#aws-permissions","title":"AWS Permissions","text":"<p>The reporter requires the following IAM permissions:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\",\n        \"logs:DescribeLogGroups\",\n        \"logs:DescribeLogStreams\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:*:*:log-group:/aws/ash/*\",\n        \"arn:aws:logs:*:*:log-group:/aws/ash/*:*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#aws-credentials","title":"AWS Credentials","text":"<p>Ensure AWS credentials are configured using one of:</p> <ul> <li>AWS CLI: <code>aws configure</code></li> <li>Environment variables: <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code></li> <li>IAM roles (recommended for EC2/ECS/Lambda)</li> <li>AWS profiles: <code>AWS_PROFILE=myprofile</code></li> </ul>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#cloudwatch-log-group","title":"CloudWatch Log Group","text":"<p>The log group must exist before publishing logs:</p> <pre><code># Create log group\naws logs create-log-group --log-group-name \"/aws/ash/scan-results\"\n\n# Set retention policy (optional)\naws logs put-retention-policy \\\n  --log-group-name \"/aws/ash/scan-results\" \\\n  --retention-in-days 30\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#features","title":"Features","text":""},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#structured-logging","title":"Structured Logging","text":"<p>Results are published as structured JSON logs:</p> <pre><code>{\n  \"timestamp\": \"2024-06-11T00:00:00Z\",\n  \"scan_id\": \"ash-scan-20240611-000000\",\n  \"source\": \"ASH\",\n  \"level\": \"INFO\",\n  \"message\": \"Security scan completed\",\n  \"results\": {\n    \"total_findings\": 15,\n    \"critical\": 2,\n    \"high\": 5,\n    \"medium\": 6,\n    \"low\": 2,\n    \"scanners_executed\": [\"bandit\", \"semgrep\", \"checkov\"],\n    \"scan_duration\": 45.2\n  }\n}\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#automatic-log-stream-management","title":"Automatic Log Stream Management","text":"<ul> <li>Auto-creation: Log streams are created automatically if they don't exist</li> <li>Timestamped entries: Each log entry includes precise timestamps</li> <li>Batch processing: Multiple findings are efficiently batched for performance</li> </ul>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#integration-with-cloudwatch-features","title":"Integration with CloudWatch Features","text":"<ul> <li>CloudWatch Insights: Query and analyze scan results using CloudWatch Logs Insights</li> <li>Metric Filters: Create custom metrics from log data</li> <li>Alarms: Set up alarms based on security findings</li> <li>Dashboards: Visualize security trends over time</li> </ul>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#usage-examples","title":"Usage Examples","text":""},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#basic-usage","title":"Basic Usage","text":"<pre><code># Run scan with CloudWatch Logs reporting\nash /path/to/code --reporters cloudwatch-logs\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code># Set log group via environment variable\nexport ASH_CLOUDWATCH_LOG_GROUP_NAME=\"/security/ash-scans\"\nash /path/to/code --reporters cloudwatch-logs\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># GitHub Actions example\n- name: Run ASH Security Scan\n  env:\n    AWS_REGION: us-east-1\n    ASH_CLOUDWATCH_LOG_GROUP_NAME: \"/ci-cd/security-scans\"\n  run: |\n    ash . --reporters cloudwatch-logs,sarif\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#cloudwatch-insights-queries","title":"CloudWatch Insights Queries","text":""},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#query-recent-scan-results","title":"Query Recent Scan Results","text":"<pre><code>fields @timestamp, scan_id, results.total_findings, results.critical, results.high\n| filter @message like /Security scan completed/\n| sort @timestamp desc\n| limit 20\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#find-high-severity-findings","title":"Find High-Severity Findings","text":"<pre><code>fields @timestamp, scan_id, results.critical, results.high\n| filter results.critical &gt; 0 or results.high &gt; 0\n| sort @timestamp desc\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#analyze-scanner-performance","title":"Analyze Scanner Performance","text":"<pre><code>fields @timestamp, scan_id, results.scan_duration, results.scanners_executed\n| stats avg(results.scan_duration) by bin(5m)\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#cloudwatch-alarms","title":"CloudWatch Alarms","text":"<p>Create alarms for critical security findings:</p> <pre><code>aws cloudwatch put-metric-alarm \\\n  --alarm-name \"ASH-Critical-Findings\" \\\n  --alarm-description \"Alert on critical security findings\" \\\n  --metric-name \"CriticalFindings\" \\\n  --namespace \"ASH/Security\" \\\n  --statistic \"Sum\" \\\n  --period 300 \\\n  --threshold 1 \\\n  --comparison-operator \"GreaterThanOrEqualToThreshold\"\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#metric-filters","title":"Metric Filters","text":"<p>Extract metrics from log data:</p> <pre><code>aws logs put-metric-filter \\\n  --log-group-name \"/aws/ash/scan-results\" \\\n  --filter-name \"CriticalFindings\" \\\n  --filter-pattern '[timestamp, scan_id, source, level, message, results.critical &gt; 0]' \\\n  --metric-transformations \\\n    metricName=CriticalFindings,metricNamespace=ASH/Security,metricValue=1\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#common-issues","title":"Common Issues","text":"<p>Permission Denied <pre><code># Check IAM permissions\naws sts get-caller-identity\naws logs describe-log-groups --log-group-name-prefix \"/aws/ash\"\n</code></pre></p> <p>Log Group Not Found <pre><code># Create the log group\naws logs create-log-group --log-group-name \"/aws/ash/scan-results\"\n</code></pre></p> <p>Region Mismatch <pre><code># Verify AWS region configuration\naws configure get region\necho $AWS_REGION\n</code></pre></p>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging to troubleshoot issues:</p> <pre><code># Run with debug output\nash /path/to/code --reporters cloudwatch-logs --log-level DEBUG\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#retry-configuration","title":"Retry Configuration","text":"<p>Configure retry behavior for API calls:</p> <pre><code>reporters:\n  cloudwatch-logs:\n    enabled: true\n    options:\n      aws_region: \"us-east-1\"\n      log_group_name: \"/aws/ash/scan-results\"\n      log_stream_name: \"ASHScanResults\"\n      # Retry configuration\n      max_retries: 5      # Default: 3\n      base_delay: 2.0     # Default: 1.0 seconds\n      max_delay: 120.0    # Default: 60.0 seconds\n``````\n\n### Debug Mode\n\nEnable debug logging to troubleshoot issues:\n\n```bash\n# Run with debug output\nash /path/to/code --reporters cloudwatch-logs --log-level DEBUG\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#cost-considerations","title":"Cost Considerations","text":"<p>CloudWatch Logs pricing includes:</p> <ul> <li>Ingestion: $0.50 per GB ingested</li> <li>Storage: $0.03 per GB per month</li> <li>Insights queries: $0.005 per GB scanned</li> </ul>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#cost-optimization-tips","title":"Cost Optimization Tips","text":"<ol> <li>Set retention policies to automatically delete old logs</li> <li>Use log filtering to reduce ingestion volume</li> <li>Compress large scan results before logging</li> <li>Monitor usage with CloudWatch billing alarms</li> </ol>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#integration-examples","title":"Integration Examples","text":""},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#with-aws-lambda","title":"With AWS Lambda","text":"<pre><code>import boto3\nimport json\n\ndef lambda_handler(event, context):\n    # Trigger ASH scan and log results\n    # Implementation depends on your Lambda setup\n    pass\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#with-amazon-eventbridge","title":"With Amazon EventBridge","text":"<p>Create rules to trigger actions based on scan results:</p> <pre><code>{\n  \"Rules\": [\n    {\n      \"Name\": \"ASH-Critical-Findings\",\n      \"EventPattern\": {\n        \"source\": [\"aws.logs\"],\n        \"detail\": {\n          \"results\": {\n            \"critical\": [{\"numeric\": [\"&gt;\", 0]}]\n          }\n        }\n      },\n      \"Targets\": [\n        {\n          \"Id\": \"1\",\n          \"Arn\": \"arn:aws:sns:us-east-1:123456789012:security-alerts\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive log group names that reflect your organization structure</li> <li>Set appropriate retention policies to manage costs</li> <li>Create metric filters for key security metrics</li> <li>Set up alarms for critical and high-severity findings</li> <li>Use CloudWatch Insights for regular security analysis</li> <li>Tag log groups for better resource management</li> <li>Monitor costs and optimize log retention as needed</li> </ol>"},{"location":"docs/plugins/aws/cloudwatch-logs-reporter/#related-documentation","title":"Related Documentation","text":"<ul> <li>AWS CloudWatch Logs Documentation</li> <li>CloudWatch Logs Insights Query Syntax</li> <li>ASH Configuration Guide</li> <li>Other AWS Reporters</li> </ul>"},{"location":"docs/plugins/aws/s3-reporter-diagrams/","title":"S3 Reporter Diagrams","text":"<p>This document provides visual diagrams of the ASH S3 Reporter architecture and workflows using Mermaid.</p>"},{"location":"docs/plugins/aws/s3-reporter-diagrams/#architecture-overview","title":"Architecture Overview","text":"<p>The following diagram shows the high-level architecture of the S3 Reporter:</p> <pre><code>flowchart LR\n    A[ASH Core] --&gt; B[S3 Reporter]\n    B --&gt; C[AWS SDK for Python]\n    C --&gt; D[Amazon S3 API]\n\n    B --&gt; E[Scan Results]\n    E --&gt; F[Format Converter]\n    F --&gt; G[S3 Object Creator]\n    G --&gt; C\n\n    D --&gt; H[S3 Bucket]\n    H --&gt; I[S3 Object]\n\n    subgraph \"AWS Cloud\"\n        D\n        H\n        I\n    end\n\n    subgraph \"Local Processing\"\n        A\n        B\n        E\n        F\n        G\n    end\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter-diagrams/#sequence-diagram","title":"Sequence Diagram","text":"<p>The following diagram shows the sequence of operations in the S3 Reporter:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant SR as S3 Reporter\n    participant SDK as AWS SDK\n    participant S3 as Amazon S3\n    participant FS as File System\n\n    ASH-&gt;&gt;SR: report(aggregated_results)\n    SR-&gt;&gt;SR: Process Scan Results\n\n    alt JSON Format\n        SR-&gt;&gt;SR: Convert to JSON\n    else YAML Format\n        SR-&gt;&gt;SR: Convert to YAML\n    end\n\n    SR-&gt;&gt;SR: Generate S3 Key with Timestamp\n\n    SR-&gt;&gt;SDK: Create S3 Session\n    SR-&gt;&gt;SDK: Put Object Request with Retry\n    SDK-&gt;&gt;S3: API Call\n    S3--&gt;&gt;SDK: Response\n    SDK--&gt;&gt;SR: Return Response\n\n    SR-&gt;&gt;FS: Write Local Copy\n\n    SR-&gt;&gt;SR: Process API Response\n    SR-&gt;&gt;SR: Handle Errors\n\n    SR--&gt;&gt;ASH: Return S3 URL or Error\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter-diagrams/#format-conversion-process","title":"Format Conversion Process","text":"<p>The following diagram shows the format conversion process:</p> <pre><code>flowchart TD\n    A[ASH Aggregated Results] --&gt; B[Convert to Simple Dict]\n\n    B --&gt; C{Format Type?}\n\n    C --&gt;|JSON| D[Serialize to JSON]\n    C --&gt;|YAML| E[Serialize to YAML]\n\n    D --&gt; F[Set Content Type to application/json]\n    E --&gt; G[Set Content Type to application/yaml]\n\n    F --&gt; H[S3 Object Content]\n    G --&gt; H\n\n    H --&gt; I[S3 API]\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter-diagrams/#error-handling-flow","title":"Error Handling Flow","text":"<p>The following diagram shows the error handling flow with retry logic:</p> <pre><code>flowchart TD\n    A[Start API Call] --&gt; B{API Call Successful?}\n\n    B --&gt;|Yes| C[Process Response]\n    B --&gt;|No| D{Error Type?}\n\n    D --&gt;|Throttling| E[Apply Exponential Backoff]\n    D --&gt;|Access Denied| F[Check IAM Permissions]\n    D --&gt;|No Such Bucket| G[Check Bucket Exists]\n    D --&gt;|Other| H[Log Error Details]\n\n    E --&gt; I[Retry API Call]\n    F --&gt; J[Log Permission Error]\n    G --&gt; K[Return Error Status]\n    H --&gt; K\n\n    I --&gt; L{Retry Successful?}\n    L --&gt;|Yes| C\n    L --&gt;|No| M{Max Retries Reached?}\n\n    M --&gt;|Yes| K\n    M --&gt;|No| E\n\n    C --&gt; N[Generate S3 URL]\n    J --&gt; O[Return Error Message]\n    K --&gt; O\n\n    N --&gt; P[Return S3 URL]\n\n    P --&gt; Q[End]\n    O --&gt; Q\n</code></pre> <p>Note: The implementation now includes retry logic with exponential backoff, improving the reliability of the S3 Reporter.</p>"},{"location":"docs/plugins/aws/s3-reporter-diagrams/#s3-object-naming-and-organization","title":"S3 Object Naming and Organization","text":"<p>The following diagram shows the S3 object naming and organization process:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B[Get Timestamp from Scan Results]\n\n    B --&gt; C[Get Key Prefix from Configuration]\n\n    C --&gt; D{File Format?}\n\n    D --&gt;|JSON| E[Set Extension to .json]\n    D --&gt;|YAML| F[Set Extension to .yaml]\n\n    E --&gt; G[Generate S3 Key]\n    F --&gt; G\n\n    G --&gt; H[Final S3 Key: prefix/ash-report-timestamp.extension]\n\n    H --&gt; I[End]\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter-diagrams/#configuration-flow","title":"Configuration Flow","text":"<p>The following diagram shows the configuration flow:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B{AWS Region Set?}\n\n    B --&gt;|Yes| C{Bucket Name Set?}\n    B --&gt;|No| D[Use AWS_REGION Environment Variable]\n\n    D --&gt; E{Environment Variable Set?}\n\n    E --&gt;|Yes| C\n    E --&gt;|No| F[Validation Fails]\n\n    C --&gt;|Yes| G[Validation Succeeds]\n    C --&gt;|No| H[Use ASH_S3_BUCKET_NAME Environment Variable]\n\n    H --&gt; I{Environment Variable Set?}\n\n    I --&gt;|Yes| G\n    I --&gt;|No| F\n\n    G --&gt; J{AWS Profile Set?}\n\n    J --&gt;|Yes| K[Use Configured Profile]\n    J --&gt;|No| L[Use Default Profile]\n\n    K --&gt; M[Reporter Ready]\n    L --&gt; M\n\n    F --&gt; N[Reporter Disabled]\n\n    M --&gt; O[End]\n    N --&gt; O\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter-diagrams/#local-file-output","title":"Local File Output","text":"<p>The following diagram shows the local file output process:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B[Generate Report Content]\n\n    B --&gt; C[Upload to S3 with Retry]\n\n    C --&gt; D[Create Reports Directory]\n\n    D --&gt; E[Write Local Copy]\n\n    E --&gt; F[Return S3 URL]\n\n    F --&gt; G[End]\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter-diagrams/#integration-with-other-aws-services","title":"Integration with Other AWS Services","text":"<p>The following diagram shows how the S3 Reporter can integrate with other AWS services:</p> <pre><code>flowchart LR\n    A[ASH Scan Results] --&gt; B[S3 Reporter]\n\n    B --&gt; C[Amazon S3]\n\n    C --&gt; D[Amazon Athena]\n    C --&gt; E[AWS Lambda]\n    C --&gt; F[Amazon QuickSight]\n    C --&gt; G[AWS Glue]\n\n    D --&gt; H[SQL Analysis]\n    E --&gt; I[Automated Processing]\n    F --&gt; J[Visualization]\n    G --&gt; K[ETL Processing]\n\n    subgraph \"AWS Ecosystem\"\n        C\n        D\n        E\n        F\n        G\n        H\n        I\n        J\n        K\n    end\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/","title":"S3 Reporter","text":"<p>Uploads ASH security scan reports to Amazon S3 for centralized storage and archival.</p>"},{"location":"docs/plugins/aws/s3-reporter/#overview","title":"Overview","text":"<p>The S3 Reporter provides:</p> <ul> <li>Simple upload of ASH scan results to S3 buckets</li> <li>JSON or YAML format support for report files</li> <li>Configurable S3 key prefixes for organization</li> <li>Retry logic for reliable uploads</li> <li>Local backup of uploaded reports</li> </ul>"},{"location":"docs/plugins/aws/s3-reporter/#configuration","title":"Configuration","text":""},{"location":"docs/plugins/aws/s3-reporter/#basic-configuration","title":"Basic Configuration","text":"<pre><code>reporters:\n  s3:\n    enabled: true\n    options:\n      bucket_name: myorg-ash-reports\n      aws_region: us-east-1\n      file_format: json  # or yaml\n      key_prefix: ash-reports/\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>reporters:\n  s3:\n    enabled: true\n    options:\n      # Use environment variables to insert the bucket name\n      bucket_name: !ENV ASH_S3_BUCKET_NAME\n      aws_region: !ENV AWS_REGION\n      aws_profile: !ENV AWS_PROFILE\n      file_format: json\n      key_prefix: security-scans/\n      # Retry configuration\n      max_retries: 5      # Default: 3\n      base_delay: 2.0     # Default: 1.0 seconds\n      max_delay: 120.0    # Default: 60.0 seconds\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/#environment-variables","title":"Environment Variables","text":"<pre><code># Required: S3 bucket name\nexport ASH_S3_BUCKET_NAME=\"my-security-reports\"\n\n# Required: AWS region\nexport AWS_REGION=\"us-east-1\"\n\n# Optional: AWS profile\nexport AWS_PROFILE=\"my-profile\"\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/plugins/aws/s3-reporter/#s3-bucket-setup","title":"S3 Bucket Setup","text":"<ol> <li> <p>Create S3 bucket:    <pre><code>aws s3 mb s3://my-security-reports --region us-east-1\n</code></pre></p> </li> <li> <p>Configure bucket policy (optional):    <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:role/ASH-Scanner-Role\"\n      },\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:PutObjectAcl\"\n      ],\n      \"Resource\": \"arn:aws:s3:::my-security-reports/*\"\n    }\n  ]\n}\n</code></pre></p> </li> <li> <p>Enable versioning (recommended):    <pre><code>aws s3api put-bucket-versioning \\\n  --bucket my-security-reports \\\n  --versioning-configuration Status=Enabled\n</code></pre></p> </li> </ol>"},{"location":"docs/plugins/aws/s3-reporter/#iam-permissions","title":"IAM Permissions","text":"<p>The reporter requires the following IAM permissions:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:HeadBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::my-security-reports\",\n        \"arn:aws:s3:::my-security-reports/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sts:GetCallerIdentity\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/#aws-credentials","title":"AWS Credentials","text":"<p>Ensure AWS credentials are configured using one of:</p> <ul> <li>AWS CLI: <code>aws configure</code></li> <li>Environment variables: <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code></li> <li>IAM roles (recommended for EC2/ECS/Lambda)</li> <li>AWS profiles: <code>AWS_PROFILE=myprofile</code></li> </ul>"},{"location":"docs/plugins/aws/s3-reporter/#features","title":"Features","text":""},{"location":"docs/plugins/aws/s3-reporter/#file-format-support","title":"File Format Support","text":"<p>The S3 reporter supports two output formats:</p> <pre><code>options:\n  file_format: json  # Default: JSON format\n  # OR\n  file_format: yaml  # YAML format\n</code></pre> <p>Note: Only one format can be selected per configuration. The reporter uploads the ASH aggregated results in the specified format.</p>"},{"location":"docs/plugins/aws/s3-reporter/#usage-examples","title":"Usage Examples","text":""},{"location":"docs/plugins/aws/s3-reporter/#basic-usage","title":"Basic Usage","text":"<pre><code># Store reports in S3\nexport ASH_S3_BUCKET_NAME=\"my-security-reports\"\nexport AWS_REGION=\"us-east-1\"\nash /path/to/code --reporters s3\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code># Set bucket and prefix via environment variables\nexport ASH_S3_BUCKET_NAME=\"security-reports-prod\"\nexport AWS_REGION=\"us-east-1\"\nash /path/to/code --reporters s3,sarif\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># GitHub Actions example\n- name: Run Security Scan\n  env:\n    AWS_REGION: us-east-1\n    ASH_S3_BUCKET_NAME: \"ci-security-reports\"\n    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n  run: |\n    ash . --reporters s3,sarif\n\n- name: Generate Report URL\n  run: |\n    echo \"Report available at: https://s3.console.aws.amazon.com/s3/buckets/ci-security-reports\"\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/#jenkins-pipeline","title":"Jenkins Pipeline","text":"<pre><code>pipeline {\n    agent any\n    environment {\n        AWS_REGION = 'us-east-1'\n        ASH_S3_BUCKET_NAME = 'jenkins-security-reports'\n    }\n    stages {\n        stage('Security Scan') {\n            steps {\n                sh 'ash . --reporters s3,html --'\n            }\n        }\n        stage('Archive Results') {\n            steps {\n                script {\n                    def reportUrl = \"https://s3.console.aws.amazon.com/s3/buckets/${env.ASH_S3_BUCKET_NAME}/\"\n                    currentBuild.description = \"Security Report: ${reportUrl}\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/#output-format","title":"Output Format","text":"<p>The S3 reporter uploads ASH aggregated results in the specified format (JSON or YAML). The uploaded file contains:</p> <ul> <li>Metadata: Scan information, timestamps, and configuration</li> <li>Findings: Security issues discovered by scanners</li> <li>Summary statistics: Counts by severity level</li> <li>Scanner details: Information about which scanners were run</li> </ul>"},{"location":"docs/plugins/aws/s3-reporter/#file-naming","title":"File Naming","text":"<p>Files are uploaded with the following naming pattern: <pre><code>{key_prefix}ash-report-{timestamp}.{extension}\n</code></pre></p> <p>For example: - <code>ash-reports/ash-report-2024-01-15T10:30:00Z.json</code> - <code>security-scans/ash-report-2024-01-15T10:30:00Z.yaml</code></p>"},{"location":"docs/plugins/aws/s3-reporter/#local-backup","title":"Local Backup","text":"<p>The reporter also creates a local copy of the uploaded report in: <pre><code>{output_dir}/reports/s3-report.{extension}\n</code></pre></p>"},{"location":"docs/plugins/aws/s3-reporter/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/aws/s3-reporter/#common-issues","title":"Common Issues","text":"<p>Access Denied - Verify that the AWS credentials have the required IAM permissions - Check that the bucket exists and is in the correct region - Ensure the bucket name is correctly configured</p> <p>Bucket Not Found - Verify the bucket name is correct - Check that the bucket exists in the specified AWS region - Ensure AWS credentials have access to the bucket</p> <p>Upload Failures - Check network connectivity to AWS - Verify AWS credentials are valid and not expired - Review the retry configuration if uploads are timing out</p>"},{"location":"docs/plugins/aws/s3-reporter/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging to see detailed error information:</p> <pre><code>ash /path/to/code --reporters s3 --log-level DEBUG\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/#retry-configuration","title":"Retry Configuration","text":"<p>The reporter includes built-in retry logic. You can configure retry behavior:</p> <pre><code>reporters:\n  s3:\n    enabled: true\n    options:\n      bucket_name: \"my-security-reports\"\n      aws_region: \"us-east-1\"\n      # Retry configuration\n      max_retries: 5      # Default: 3\n      base_delay: 2.0     # Default: 1.0 seconds\n      max_delay: 120.0    # Default: 60.0 seconds\n</code></pre>"},{"location":"docs/plugins/aws/s3-reporter/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive bucket names that reflect your organization</li> <li>Implement lifecycle policies to manage costs</li> <li>Enable versioning for important reports</li> <li>Set up monitoring for upload failures</li> <li>Use appropriate storage classes based on access patterns</li> <li>Implement proper access controls with IAM</li> <li>Enable encryption for sensitive data</li> <li>Monitor costs and optimize storage regularly</li> <li>Set up automated cleanup for old reports</li> <li>Use cross-region replication for critical reports</li> </ol>"},{"location":"docs/plugins/aws/s3-reporter/#related-documentation","title":"Related Documentation","text":"<ul> <li>Amazon S3 User Guide</li> <li>S3 Storage Classes</li> <li>S3 Lifecycle Management</li> <li>ASH Configuration Guide</li> <li>Other AWS Reporters</li> </ul>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/","title":"Security Hub Reporter Diagrams","text":"<p>This document provides visual diagrams of the ASH Security Hub Reporter architecture and workflows using Mermaid.</p>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/#architecture-overview","title":"Architecture Overview","text":"<p>The following diagram shows the high-level architecture of the Security Hub Reporter:</p> <pre><code>flowchart LR\n    A[ASH Core] --&gt; B[Security Hub Reporter]\n    B --&gt; C[AWS SDK for Python]\n    C --&gt; D[AWS Security Hub API]\n\n    B --&gt; E[Scan Results]\n    E --&gt; F[ASFF Converter]\n    F --&gt; G[Batch Processor]\n    G --&gt; C\n\n    D --&gt; H[Security Hub Findings]\n    H --&gt; I[Security Hub Console]\n    H --&gt; J[EventBridge Rules]\n    J --&gt; K[Automated Response]\n\n    subgraph \"AWS Cloud\"\n        D\n        H\n        I\n        J\n        K\n    end\n\n    subgraph \"Local Processing\"\n        A\n        B\n        E\n        F\n        G\n    end\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/#sequence-diagram","title":"Sequence Diagram","text":"<p>The following diagram shows the sequence of operations in the Security Hub Reporter:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant SHR as Security Hub Reporter\n    participant SDK as AWS SDK\n    participant SH as AWS Security Hub\n    participant EB as EventBridge\n    participant AR as Automated Response\n\n    ASH-&gt;&gt;SHR: report(aggregated_results)\n    SHR-&gt;&gt;SHR: Process Scan Results\n    SHR-&gt;&gt;SHR: Convert to ASFF Format\n\n    SHR-&gt;&gt;SHR: Group Findings into Batches\n\n    loop For Each Batch\n        SHR-&gt;&gt;SDK: BatchImportFindings Request\n        SDK-&gt;&gt;SH: API Call\n        SH--&gt;&gt;SDK: Response\n        SDK--&gt;&gt;SHR: Return Response\n    end\n\n    SH-&gt;&gt;EB: Emit Finding Events\n    EB-&gt;&gt;AR: Trigger Automated Response\n\n    SHR-&gt;&gt;SHR: Process API Responses\n    SHR-&gt;&gt;SHR: Handle Errors\n\n    SHR--&gt;&gt;ASH: Return Report Status\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/#asff-conversion-process","title":"ASFF Conversion Process","text":"<p>The following diagram shows the ASFF conversion process:</p> <pre><code>flowchart TD\n    A[ASH Finding] --&gt; B[Extract Metadata]\n    A --&gt; C[Extract Vulnerability Details]\n    A --&gt; D[Extract Resource Information]\n\n    B --&gt; E[Generate ASFF Base Fields]\n    C --&gt; F[Generate ASFF Vulnerability Fields]\n    D --&gt; G[Generate ASFF Resource Fields]\n\n    E --&gt; H[Create ASFF Finding]\n    F --&gt; H\n    G --&gt; H\n\n    H --&gt; I[Add AWS Account ID]\n    I --&gt; J[Add Product ARN]\n    J --&gt; K[Add Finding ID]\n\n    K --&gt; L[Add Severity Mapping]\n    L --&gt; M[Add Types Mapping]\n    M --&gt; N[Add Compliance Status]\n\n    N --&gt; O[Final ASFF Finding]\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/#batch-processing-flow","title":"Batch Processing Flow","text":"<p>The following diagram shows the batch processing flow:</p> <pre><code>flowchart TD\n    A[All Findings] --&gt; B[Group by Max Batch Size]\n\n    B --&gt; C[Batch 1]\n    B --&gt; D[Batch 2]\n    B --&gt; E[Batch 3]\n\n    C --&gt; F[Process Batch 1]\n    D --&gt; G[Process Batch 2]\n    E --&gt; H[Process Batch 3]\n\n    F --&gt; I{API Call Successful?}\n    G --&gt; I\n    H --&gt; I\n\n    I --&gt;|Yes| J[Record Success]\n    I --&gt;|No| K{Retryable Error?}\n\n    K --&gt;|Yes| L[Apply Backoff]\n    K --&gt;|No| M[Record Failure]\n\n    L --&gt; N[Retry Batch]\n    N --&gt; I\n\n    J --&gt; O[Aggregate Results]\n    M --&gt; O\n\n    O --&gt; P[Final Report]\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/#finding-lifecycle-management","title":"Finding Lifecycle Management","text":"<p>The following diagram shows the finding lifecycle management process:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B{Finding Exists?}\n\n    B --&gt;|Yes| C[Get Existing Finding]\n    B --&gt;|No| D[Create New Finding]\n\n    C --&gt; E{Status Changed?}\n    E --&gt;|Yes| F[Update Status]\n    E --&gt;|No| G{Details Changed?}\n\n    G --&gt;|Yes| H[Update Details]\n    G --&gt;|No| I[No Update Needed]\n\n    D --&gt; J[Set Initial Status]\n    F --&gt; K[Update Finding]\n    H --&gt; K\n\n    J --&gt; L[Import New Finding]\n    K --&gt; L\n    I --&gt; M[Skip Update]\n\n    L --&gt; N[End]\n    M --&gt; N\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/#integration-with-security-hub","title":"Integration with Security Hub","text":"<p>The following diagram shows the integration with AWS Security Hub:</p> <pre><code>flowchart LR\n    A[ASH Findings] --&gt; B[Security Hub Reporter]\n\n    B --&gt; C[Security Hub]\n\n    C --&gt; D[Security Hub Console]\n    C --&gt; E[Security Hub API]\n    C --&gt; F[Security Hub Insights]\n\n    C --&gt; G[EventBridge]\n    G --&gt; H[Lambda Functions]\n    G --&gt; I[SNS Topics]\n    G --&gt; J[Step Functions]\n\n    H --&gt; K[Automated Remediation]\n    I --&gt; L[Notifications]\n    J --&gt; M[Workflows]\n\n    C --&gt; N[Security Standards]\n    N --&gt; O[Compliance Status]\n\n    subgraph \"AWS Security Hub\"\n        C\n        D\n        E\n        F\n        N\n        O\n    end\n\n    subgraph \"Integration Points\"\n        G\n        H\n        I\n        J\n        K\n        L\n        M\n    end\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/#error-handling-flow","title":"Error Handling Flow","text":"<p>The following diagram shows the error handling flow with retry logic:</p> <pre><code>flowchart TD\n    A[Start API Call] --&gt; B{API Call Successful?}\n\n    B --&gt;|Yes| C[Process Response]\n    B --&gt;|No| D{Error Type?}\n\n    D --&gt;|Throttling| E[Apply Exponential Backoff]\n    D --&gt;|Access Denied| F[Check IAM Permissions]\n    D --&gt;|Invalid Format| G[Fix ASFF Format]\n    D --&gt;|Other| H[Log Error Details]\n\n    E --&gt; I[Retry API Call]\n    F --&gt; J[Log Permission Error]\n    G --&gt; K[Retry with Fixed Format]\n    H --&gt; L[Return Error Status]\n\n    I --&gt; M{Retry Successful?}\n    M --&gt;|Yes| C\n    M --&gt;|No| N{Max Retries Reached?}\n\n    N --&gt;|Yes| L\n    N --&gt;|No| E\n\n    K --&gt; O{Format Fix Successful?}\n    O --&gt;|Yes| P[Retry with Fixed Format]\n    O --&gt;|No| L\n\n    P --&gt; Q{Retry Successful?}\n    Q --&gt;|Yes| C\n    Q --&gt;|No| L\n\n    C --&gt; R[Complete Processing]\n    J --&gt; S[End with Error]\n    L --&gt; S\n    R --&gt; T[End Successfully]\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/#cross-account-integration","title":"Cross-Account Integration","text":"<p>The following diagram shows the cross-account integration flow:</p> <pre><code>flowchart TD\n    A[ASH in Account A] --&gt; B[Security Hub Reporter]\n\n    B --&gt; C{Cross-Account Mode?}\n\n    C --&gt;|Yes| D[Assume Role in Target Account]\n    C --&gt;|No| E[Use Local Account]\n\n    D --&gt; F[Security Hub in Account B]\n    E --&gt; G[Security Hub in Account A]\n\n    F --&gt; H[Findings in Central Account]\n    G --&gt; I[Findings in Local Account]\n\n    H --&gt; J[Cross-Account Aggregation]\n    I --&gt; J\n\n    J --&gt; K[Centralized View]\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/#compliance-framework-mapping","title":"Compliance Framework Mapping","text":"<p>The following diagram shows the compliance framework mapping:</p> <pre><code>flowchart LR\n    A[ASH Finding] --&gt; B[Extract Vulnerability Type]\n\n    B --&gt; C{Mapping Available?}\n\n    C --&gt;|Yes| D[Map to Compliance Controls]\n    C --&gt;|No| E[Use Default Mapping]\n\n    D --&gt; F[AWS Foundational Security Best Practices]\n    D --&gt; G[CIS AWS Foundations]\n    D --&gt; H[PCI DSS]\n    D --&gt; I[NIST 800-53]\n\n    E --&gt; J[Generic Security Finding]\n\n    F --&gt; K[Security Hub Standards]\n    G --&gt; K\n    H --&gt; K\n    I --&gt; K\n    J --&gt; K\n\n    K --&gt; L[Compliance Dashboard]\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter-diagrams/#cost-optimization-strategy","title":"Cost Optimization Strategy","text":"<p>The following diagram shows the cost optimization strategy:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B[Analyze Scan Results]\n\n    B --&gt; C{Finding Count &gt; Threshold?}\n    C --&gt;|Yes| D[Apply Filtering]\n    C --&gt;|No| E[Use All Findings]\n\n    D --&gt; F[Filter by Severity]\n    D --&gt; G[Filter by Type]\n    D --&gt; H[Filter by Resource]\n\n    F --&gt; I[Combine Filters]\n    G --&gt; I\n    H --&gt; I\n\n    I --&gt; J[Selected Findings]\n    E --&gt; J\n\n    J --&gt; K[Batch Processing]\n    K --&gt; L[Import to Security Hub]\n\n    L --&gt; M[Monitor Costs]\n    M --&gt; N[End]\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/","title":"Security Hub Reporter","text":"<p>Sends ASH security findings directly to AWS Security Hub in AWS Security Finding Format (ASFF), enabling centralized security monitoring and compliance reporting.</p> <p>For detailed visual diagrams of the Security Hub Reporter architecture and workflow, see Security Hub Reporter Diagrams.</p>"},{"location":"docs/plugins/aws/security-hub-reporter/#overview","title":"Overview","text":"<p>The Security Hub Reporter integrates ASH scan results with AWS Security Hub by:</p> <ul> <li>Converting findings to ASFF format for standardized security reporting</li> <li>Batch uploading findings to Security Hub for efficient processing</li> <li>Maintaining finding lifecycle with proper status tracking</li> <li>Supporting compliance frameworks like AWS Foundational Security Standard</li> </ul>"},{"location":"docs/plugins/aws/security-hub-reporter/#configuration","title":"Configuration","text":""},{"location":"docs/plugins/aws/security-hub-reporter/#basic-configuration","title":"Basic Configuration","text":"<pre><code>reporters:\n  aws-security-hub:\n    enabled: true\n    options:\n      aws_region: \"us-east-1\"\n      aws_profile: \"default\"  # optional\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#environment-variables","title":"Environment Variables","text":"<p>The reporter supports configuration via environment variables:</p> <pre><code># AWS region (falls back to AWS_DEFAULT_REGION)\nexport AWS_REGION=\"us-east-1\"\n\n# AWS profile (optional)\nexport AWS_PROFILE=\"security-scanning\"\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#complete-configuration-example","title":"Complete Configuration Example","text":"<pre><code>reporters:\n  aws-security-hub:\n    enabled: true\n    options:\n      aws_region: \"us-west-2\"\n      aws_profile: \"production\"\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/plugins/aws/security-hub-reporter/#aws-security-hub-setup","title":"AWS Security Hub Setup","text":"<ol> <li> <p>Enable Security Hub in your AWS account:    <pre><code>aws securityhub enable-security-hub --region us-east-1\n</code></pre></p> </li> <li> <p>Enable standards (optional but recommended):    <pre><code>aws securityhub batch-enable-standards \\\n  --standards-subscription-requests StandardsArn=arn:aws:securityhub:::ruleset/finding-format/aws-foundational-security-standard/v/1.0.0\n</code></pre></p> </li> </ol>"},{"location":"docs/plugins/aws/security-hub-reporter/#iam-permissions","title":"IAM Permissions","text":"<p>The reporter requires the following IAM permissions:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"securityhub:BatchImportFindings\",\n        \"securityhub:GetFindings\",\n        \"securityhub:UpdateFindings\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#aws-credentials","title":"AWS Credentials","text":"<p>Ensure AWS credentials are configured using one of:</p> <ul> <li>AWS CLI: <code>aws configure</code></li> <li>Environment variables: <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code></li> <li>IAM roles (recommended for EC2/ECS/Lambda)</li> <li>AWS profiles: <code>AWS_PROFILE=myprofile</code></li> </ul>"},{"location":"docs/plugins/aws/security-hub-reporter/#features","title":"Features","text":""},{"location":"docs/plugins/aws/security-hub-reporter/#asff-format-conversion","title":"ASFF Format Conversion","text":"<p>ASH findings are automatically converted to AWS Security Finding Format (ASFF):</p> <pre><code>{\n  \"SchemaVersion\": \"2018-10-08\",\n  \"Id\": \"ash-finding-12345\",\n  \"ProductArn\": \"arn:aws:securityhub:us-east-1:123456789012:product/123456789012/default\",\n  \"GeneratorId\": \"ASH\",\n  \"AwsAccountId\": \"123456789012\",\n  \"Types\": [\"Sensitive Data Identifications/PII\"],\n  \"CreatedAt\": \"2024-06-11T00:00:00.000Z\",\n  \"UpdatedAt\": \"2024-06-11T00:00:00.000Z\",\n  \"Severity\": {\n    \"Label\": \"HIGH\",\n    \"Normalized\": 70\n  },\n  \"Title\": \"Hardcoded API Key Detected\",\n  \"Description\": \"A hardcoded API key was found in the source code\",\n  \"Resources\": [\n    {\n      \"Type\": \"Other\",\n      \"Id\": \"file:///path/to/file.py\",\n      \"Region\": \"us-east-1\"\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#batch-processing","title":"Batch Processing","text":"<ul> <li>Efficient uploads: Findings are batched for optimal performance</li> <li>Rate limiting: Respects AWS API rate limits</li> <li>Error handling: Robust error handling with retry logic</li> <li>Deduplication: Prevents duplicate findings in Security Hub</li> </ul>"},{"location":"docs/plugins/aws/security-hub-reporter/#finding-lifecycle-management","title":"Finding Lifecycle Management","text":"<ul> <li>New findings: Automatically created with appropriate severity</li> <li>Updated findings: Existing findings are updated when re-scanned</li> <li>Status tracking: Maintains finding status (NEW, NOTIFIED, RESOLVED)</li> </ul>"},{"location":"docs/plugins/aws/security-hub-reporter/#usage-examples","title":"Usage Examples","text":""},{"location":"docs/plugins/aws/security-hub-reporter/#basic-usage","title":"Basic Usage","text":"<pre><code># Run scan with Security Hub reporting\nash /path/to/code --reporters aws-security-hub\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#with-multiple-reporters","title":"With Multiple Reporters","text":"<pre><code># Generate both SARIF and Security Hub reports\nash /path/to/code --reporters sarif,aws-security-hub\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># GitHub Actions example\n- name: Run ASH Security Scan\n  env:\n    AWS_REGION: us-east-1\n    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n  run: |\n    ash . --reporters aws-security-hub,sarif\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#security-hub-integration","title":"Security Hub Integration","text":""},{"location":"docs/plugins/aws/security-hub-reporter/#viewing-findings","title":"Viewing Findings","text":"<ol> <li>AWS Console: Navigate to Security Hub \u2192 Findings</li> <li>Filter by product: Look for \"ASH\" or \"Automated Security Helper\"</li> <li>Review details: Click on findings to see detailed information</li> </ol>"},{"location":"docs/plugins/aws/security-hub-reporter/#custom-insights","title":"Custom Insights","text":"<p>Create custom insights to track ASH findings:</p> <pre><code>aws securityhub create-insight \\\n  --name \"ASH Critical Findings\" \\\n  --filters '{\n    \"ProductName\": [{\"Value\": \"ASH\", \"Comparison\": \"EQUALS\"}],\n    \"SeverityLabel\": [{\"Value\": \"CRITICAL\", \"Comparison\": \"EQUALS\"}]\n  }' \\\n  --group-by-attribute \"ResourceId\"\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#automated-response","title":"Automated Response","text":"<p>Use EventBridge to trigger automated responses:</p> <pre><code>{\n  \"Rules\": [\n    {\n      \"Name\": \"ASH-Critical-Finding-Response\",\n      \"EventPattern\": {\n        \"source\": [\"aws.securityhub\"],\n        \"detail-type\": [\"Security Hub Findings - Imported\"],\n        \"detail\": {\n          \"findings\": {\n            \"ProductName\": [\"ASH\"],\n            \"Severity\": {\n              \"Label\": [\"CRITICAL\", \"HIGH\"]\n            }\n          }\n        }\n      },\n      \"Targets\": [\n        {\n          \"Id\": \"1\",\n          \"Arn\": \"arn:aws:sns:us-east-1:123456789012:security-alerts\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/aws/security-hub-reporter/#common-issues","title":"Common Issues","text":"<p>Security Hub Not Enabled <pre><code># Check if Security Hub is enabled\naws securityhub get-enabled-standards --region us-east-1\n\n# Enable Security Hub if needed\naws securityhub enable-security-hub --region us-east-1\n</code></pre></p> <p>Permission Denied <pre><code># Check IAM permissions\naws sts get-caller-identity\naws securityhub describe-hub --region us-east-1\n</code></pre></p> <p>Region Mismatch <pre><code># Verify AWS region configuration\naws configure get region\necho $AWS_REGION\n</code></pre></p> <p>Findings Not Appearing - Check Security Hub console filters - Verify findings aren't suppressed - Confirm account ID matches</p>"},{"location":"docs/plugins/aws/security-hub-reporter/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging to troubleshoot issues:</p> <pre><code># Run with debug output\nash /path/to/code --reporters aws-security-hub --log-level DEBUG\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#cost-considerations","title":"Cost Considerations","text":"<p>Security Hub pricing includes:</p> <ul> <li>Finding ingestion: $0.0003 per finding per month</li> <li>Compliance checks: Additional costs for enabled standards</li> <li>API calls: Standard AWS API pricing applies</li> </ul>"},{"location":"docs/plugins/aws/security-hub-reporter/#cost-optimization-tips","title":"Cost Optimization Tips","text":"<ol> <li>Filter findings by severity to reduce ingestion costs</li> <li>Use suppression rules for false positives</li> <li>Monitor usage with AWS Cost Explorer</li> <li>Archive resolved findings to reduce storage costs</li> </ol>"},{"location":"docs/plugins/aws/security-hub-reporter/#compliance-integration","title":"Compliance Integration","text":""},{"location":"docs/plugins/aws/security-hub-reporter/#aws-foundational-security-standard","title":"AWS Foundational Security Standard","text":"<p>ASH findings automatically map to relevant controls:</p> <ul> <li>[IAM.1] Password policies for IAM users</li> <li>[S3.1] S3 bucket public access</li> <li>[EC2.1] Security group rules</li> </ul>"},{"location":"docs/plugins/aws/security-hub-reporter/#custom-standards","title":"Custom Standards","text":"<p>Create custom standards that include ASH findings:</p> <pre><code>aws securityhub create-custom-action \\\n  --name \"Mark ASH Finding as Accepted Risk\" \\\n  --description \"Accept ASH finding as business risk\" \\\n  --id \"ash-accept-risk\"\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#best-practices","title":"Best Practices","text":"<ol> <li>Enable Security Hub in all regions where you scan code</li> <li>Set up cross-region aggregation for centralized monitoring</li> <li>Create custom insights for ASH-specific findings</li> <li>Use suppression rules for known false positives</li> <li>Integrate with incident response workflows</li> <li>Monitor costs and optimize finding ingestion</li> <li>Regular review of findings and their resolution status</li> </ol>"},{"location":"docs/plugins/aws/security-hub-reporter/#integration-examples","title":"Integration Examples","text":""},{"location":"docs/plugins/aws/security-hub-reporter/#with-aws-config","title":"With AWS Config","text":"<p>Correlate ASH findings with AWS Config compliance:</p> <pre><code>import boto3\n\ndef correlate_findings():\n    securityhub = boto3.client('securityhub')\n    config = boto3.client('config')\n\n    # Get ASH findings\n    findings = securityhub.get_findings(\n        Filters={'ProductName': [{'Value': 'ASH', 'Comparison': 'EQUALS'}]}\n    )\n\n    # Correlate with Config rules\n    for finding in findings['Findings']:\n        # Implementation depends on your specific use case\n        pass\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#with-aws-systems-manager","title":"With AWS Systems Manager","text":"<p>Create Systems Manager documents for automated remediation:</p> <pre><code>schemaVersion: \"0.3\"\ndescription: \"Remediate ASH Security Finding\"\nassumeRole: \"{{ AutomationAssumeRole }}\"\nparameters:\n  FindingId:\n    type: String\n    description: \"Security Hub Finding ID\"\nmainSteps:\n  - name: \"RemediateFinding\"\n    action: \"aws:executeScript\"\n    inputs:\n      Runtime: \"python3.8\"\n      Handler: \"remediate_finding\"\n      Script: |\n        def remediate_finding(events, context):\n            # Implement remediation logic\n            pass\n</code></pre>"},{"location":"docs/plugins/aws/security-hub-reporter/#related-documentation","title":"Related Documentation","text":"<ul> <li>AWS Security Hub User Guide</li> <li>AWS Security Finding Format (ASFF)</li> <li>ASH Configuration Guide</li> <li>Other AWS Reporters</li> </ul>"},{"location":"docs/plugins/builtin/","title":"Built-in Plugins","text":"<p>ASH ships with a comprehensive set of built-in plugins that provide core security scanning, reporting, and file processing capabilities. These plugins are automatically available and can be configured to meet your specific security requirements.</p>"},{"location":"docs/plugins/builtin/#overview","title":"Overview","text":"<p>Built-in plugins are organized into four main categories:</p> Category Purpose Count Location Scanners Analyze code and infrastructure for security vulnerabilities 10 <code>scanners/</code> Reporters Generate scan results in various output formats 13 <code>reporters/</code> Converters Process and prepare files for scanning 2 <code>converters/</code> Event Handlers Handle scan lifecycle events and notifications 1 <code>event_handlers/</code>"},{"location":"docs/plugins/builtin/#quick-start","title":"Quick Start","text":"<p>All built-in plugins are enabled by default and require no additional configuration to get started:</p> <pre><code># Run with default built-in scanners\nash /path/to/code\n\n# Use specific built-in scanners only\nash /path/to/code --scanners bandit,semgrep\n\n# Generate reports in multiple formats\nash /path/to/code --reporters sarif,html,csv\n</code></pre>"},{"location":"docs/plugins/builtin/#configuration","title":"Configuration","text":"<p>Built-in plugins can be customized through configuration files:</p> <pre><code># ash-config.yml\nscanners:\n  bandit:\n    enabled: true\n    severity_threshold: \"MEDIUM\"\n    options:\n      confidence_level: \"HIGH\"\n\n  semgrep:\n    enabled: true\n    options:\n      rules: \"auto\"\n      timeout: 300\n\nreporters:\n  html:\n    enabled: true\n    options:\n      include_suppressed: false\n\n  sarif:\n    enabled: true\n    options:\n      include_rule_metadata: true\n</code></pre>"},{"location":"docs/plugins/builtin/#plugin-categories","title":"Plugin Categories","text":""},{"location":"docs/plugins/builtin/#security-scanners","title":"Security Scanners","text":"<p>Built-in scanners cover a wide range of security analysis:</p> <ul> <li>Static Analysis: Bandit, Semgrep, OpenGrep</li> <li>Infrastructure Security: CDK-Nag, CFN-Nag, Checkov</li> <li>Dependency Scanning: NPM Audit, Grype</li> <li>Secret Detection: Detect-Secrets</li> <li>SBOM Generation: Syft</li> </ul>"},{"location":"docs/plugins/builtin/#output-formats","title":"Output Formats","text":"<p>Multiple output formats support different use cases:</p> <ul> <li>CI/CD Integration: SARIF, JUnit XML, GitLab SAST</li> <li>Human Readable: HTML, Markdown, Text</li> <li>Data Processing: CSV, JSON, YAML</li> <li>Compliance: SPDX, CycloneDX, OCSF</li> </ul>"},{"location":"docs/plugins/builtin/#file-processing","title":"File Processing","text":"<p>Converters handle various file types:</p> <ul> <li>Archives: Automatic extraction of zip, tar, and other compressed formats</li> <li>Notebooks: Jupyter notebook processing for Python code analysis</li> </ul>"},{"location":"docs/plugins/builtin/#dependencies","title":"Dependencies","text":"<p>Built-in plugins may require external tools to be installed:</p> <pre><code># Check plugin dependencies\nash dependencies --check\n\n# Install missing dependencies (where possible)\nash dependencies --install\n</code></pre>"},{"location":"docs/plugins/builtin/#advanced-usage","title":"Advanced Usage","text":""},{"location":"docs/plugins/builtin/#plugin-specific-configuration","title":"Plugin-Specific Configuration","text":"<p>Each plugin supports specific configuration options:</p> <pre><code>scanners:\n  checkov:\n    options:\n      framework: [\"terraform\", \"cloudformation\"]\n      check: [\"CKV_AWS_*\"]\n      skip_check: [\"CKV_AWS_123\"]\n      external_checks_dir: \"/path/to/custom/checks\"\n</code></pre>"},{"location":"docs/plugins/builtin/#selective-plugin-execution","title":"Selective Plugin Execution","text":"<p>Control which plugins run:</p> <pre><code># Run only infrastructure scanners\nash --scanners cdk-nag,cfn-nag,checkov\n\n# Exclude specific scanners\nash --exclude-scanners grype,syft\n\n# Generate only compliance reports\nash --reporters spdx,cyclonedx\n</code></pre>"},{"location":"docs/plugins/builtin/#integration-with-external-tools","title":"Integration with External Tools","text":"<p>Built-in plugins integrate with popular security tools:</p> <ul> <li>Semgrep: Uses Semgrep Registry rules</li> <li>Bandit: Leverages Python AST analysis</li> <li>Checkov: Supports custom policy frameworks</li> <li>Grype: Integrates with vulnerability databases</li> </ul>"},{"location":"docs/plugins/builtin/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p>"},{"location":"docs/plugins/builtin/#scanner-not-found","title":"Scanner Not Found","text":"<pre><code># Check if scanner dependencies are installed\nash dependencies --check --scanner bandit\n\n# Install missing dependencies\npip install bandit\n</code></pre>"},{"location":"docs/plugins/builtin/#configuration-issues","title":"Configuration Issues","text":"<pre><code># Validate configuration\nash config --validate\n\n# Show effective configuration\nash config --show\n</code></pre>"},{"location":"docs/plugins/builtin/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Run scanners in parallel (default)\nash --parallel\n\n# Limit concurrent scanners\nash --max-workers 2\n\n# Skip time-intensive scanners for quick feedback\nash --exclude-scanners grype,syft\n</code></pre>"},{"location":"docs/plugins/builtin/#next-steps","title":"Next Steps","text":"<ul> <li>Scanner Details: Detailed information about each security scanner</li> <li>Reporter Details: Complete guide to output formats</li> <li>Configuration Guide: Advanced configuration options</li> <li>Plugin Development: Create custom plugins</li> </ul>"},{"location":"docs/plugins/builtin/converters-diagrams/","title":"Built-in Converter Diagrams","text":"<p>This document provides visual diagrams of the ASH built-in converter architecture and workflows using Mermaid.</p>"},{"location":"docs/plugins/builtin/converters-diagrams/#converter-architecture-overview","title":"Converter Architecture Overview","text":"<p>The following diagram shows the high-level architecture of the ASH built-in converters:</p> <pre><code>flowchart TD\n    A[ASH Core] --&gt; B[Plugin Manager]\n    B --&gt; C[Converter Registry]\n\n    C --&gt; D[Built-in Converters]\n\n    D --&gt; E[Archive Converter]\n    D --&gt; F[Jupyter Converter]\n\n    G[Source Files] --&gt; D\n\n    E --&gt; H[Extracted Files]\n    F --&gt; I[Python Files]\n\n    H --&gt; J[Scanner Plugins]\n    I --&gt; J\n</code></pre>"},{"location":"docs/plugins/builtin/converters-diagrams/#converter-execution-flow","title":"Converter Execution Flow","text":"<p>The following diagram shows the execution flow of the built-in converters:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant PM as Plugin Manager\n    participant CR as Converter Registry\n    participant CV as Converter\n    participant FS as File System\n    participant ES as Event System\n\n    ASH-&gt;&gt;PM: Load Converters\n    PM-&gt;&gt;CR: Get Registered Converters\n    CR--&gt;&gt;PM: Return Converter List\n\n    ASH-&gt;&gt;ES: Emit ConversionStarted Event\n\n    loop For Each Converter\n        ASH-&gt;&gt;CV: Validate Converter\n        CV--&gt;&gt;ASH: Return Validation Status\n\n        alt Converter Valid\n            ASH-&gt;&gt;CV: convert(target)\n            CV-&gt;&gt;FS: Read Source Files\n            FS--&gt;&gt;CV: Return File Contents\n\n            CV-&gt;&gt;CV: Process Files\n            Note over CV: Transform File Content\n\n            CV-&gt;&gt;FS: Write Converted Files\n            FS--&gt;&gt;CV: Files Written\n\n            CV--&gt;&gt;ASH: Return Converted Path\n        else Converter Invalid\n            ASH-&gt;&gt;ES: Emit ConverterError Event\n        end\n    end\n\n    ASH-&gt;&gt;ES: Emit ConversionCompleted Event\n</code></pre>"},{"location":"docs/plugins/builtin/converters-diagrams/#archive-converter-workflow","title":"Archive Converter Workflow","text":"<p>The following diagram shows the workflow of the Archive Converter:</p> <pre><code>flowchart TD\n    A[Source Archive] --&gt; B[Archive Converter]\n\n    B --&gt; C{Archive Type?}\n\n    C --&gt;|ZIP| D[Extract ZIP]\n    C --&gt;|TAR| E[Extract TAR]\n    C --&gt;|TAR.GZ| F[Extract TAR.GZ]\n\n    D --&gt; G[Extracted Files]\n    E --&gt; G\n    F --&gt; G\n\n    G --&gt; H{Contains Nested Archives?}\n\n    H --&gt;|Yes| I{Max Depth Reached?}\n    H --&gt;|No| J[Proceed to Scanning]\n\n    I --&gt;|Yes| K[Skip Nested Archives]\n    I --&gt;|No| L[Process Nested Archives]\n\n    L --&gt; B\n\n    K --&gt; J\n\n    J --&gt; M[Scanner Plugins]\n</code></pre>"},{"location":"docs/plugins/builtin/converters-diagrams/#jupyter-converter-workflow","title":"Jupyter Converter Workflow","text":"<p>The following diagram shows the workflow of the Jupyter Converter:</p> <pre><code>flowchart TD\n    A[Jupyter Notebook] --&gt; B[Jupyter Converter]\n\n    B --&gt; C[Parse Notebook JSON]\n\n    C --&gt; D[Extract Cells]\n\n    D --&gt; E{Cell Type?}\n\n    E --&gt;|Code| F[Extract Code]\n    E --&gt;|Markdown| G{Extract Markdown?}\n\n    G --&gt;|Yes| H[Convert to Comments]\n    G --&gt;|No| I[Skip Cell]\n\n    F --&gt; J[Combine Code]\n    H --&gt; J\n\n    J --&gt; K[Add Cell Markers]\n\n    K --&gt; L[Write Python File]\n\n    L --&gt; M[Scanner Plugins]\n</code></pre>"},{"location":"docs/plugins/builtin/converters-diagrams/#converter-configuration-flow","title":"Converter Configuration Flow","text":"<p>The following diagram shows how configuration flows through the built-in converters:</p> <pre><code>flowchart TD\n    A[.ash/.ash.yaml] --&gt; B[Configuration Parser]\n    C[CLI Arguments] --&gt; B\n    B --&gt; D[ASH Configuration]\n    D --&gt; E[Converter Configuration]\n\n    E --&gt; F[Global Converter Settings]\n    E --&gt; G[Converter-Specific Settings]\n\n    F --&gt; H[Converted Directory]\n    F --&gt; I[Enabled Converters]\n\n    G --&gt; J[Archive Converter Config]\n    G --&gt; K[Jupyter Converter Config]\n\n    J --&gt; L[Archive Converter]\n    K --&gt; M[Jupyter Converter]\n</code></pre>"},{"location":"docs/plugins/builtin/converters-diagrams/#file-type-processing","title":"File Type Processing","text":"<p>The following diagram shows how different file types are processed by converters:</p> <pre><code>flowchart LR\n    A[Source Files] --&gt; B{File Type?}\n\n    B --&gt;|.zip, .tar, .tar.gz| C[Archive Converter]\n    B --&gt;|.ipynb| D[Jupyter Converter]\n    B --&gt;|Other| E[No Conversion]\n\n    C --&gt; F[Extracted Files]\n    D --&gt; G[Python Files]\n    E --&gt; H[Original Files]\n\n    F --&gt; I[Scanner Plugins]\n    G --&gt; I\n    H --&gt; I\n</code></pre>"},{"location":"docs/plugins/builtin/converters-diagrams/#converter-error-handling","title":"Converter Error Handling","text":"<p>The following diagram shows the error handling flow in converters:</p> <pre><code>flowchart TD\n    A[Start Conversion] --&gt; B{Converter Available?}\n\n    B --&gt;|Yes| C[Run Converter]\n    B --&gt;|No| D[Skip Conversion]\n\n    C --&gt; E{Conversion Successful?}\n\n    E --&gt;|Yes| F[Return Converted Path]\n    E --&gt;|No| G{Error Type?}\n\n    G --&gt;|File Format Error| H[Log Format Error]\n    G --&gt;|Extraction Error| I[Log Extraction Error]\n    G --&gt;|File System Error| J[Log File System Error]\n    G --&gt;|Other| K[Log Generic Error]\n\n    H --&gt; L{Continue on Error?}\n    I --&gt; L\n    J --&gt; L\n    K --&gt; L\n\n    L --&gt;|Yes| D\n    L --&gt;|No| M[Abort Conversion]\n\n    F --&gt; N[Proceed to Scanning]\n    D --&gt; N\n    M --&gt; O[End with Error]\n</code></pre>"},{"location":"docs/plugins/builtin/converters-diagrams/#converter-integration-with-scanners","title":"Converter Integration with Scanners","text":"<p>The following diagram shows how converters integrate with scanners:</p> <pre><code>flowchart LR\n    A[Source Files] --&gt; B[Converters]\n\n    B --&gt; C[Converted Files]\n\n    C --&gt; D[Scanner 1]\n    C --&gt; E[Scanner 2]\n    C --&gt; F[Scanner 3]\n\n    D --&gt; G[Results 1]\n    E --&gt; H[Results 2]\n    F --&gt; I[Results 3]\n\n    G --&gt; J[Results Aggregator]\n    H --&gt; J\n    I --&gt; J\n\n    J --&gt; K[Reporter Plugins]\n</code></pre>"},{"location":"docs/plugins/builtin/converters/","title":"Built-in Converters","text":"<p>ASH includes 2 built-in converters that preprocess files to make them suitable for security scanning. Converters handle file format transformations and archive extraction automatically.</p> <p>For detailed visual diagrams of the built-in converter architecture and workflows, see Built-in Converter Diagrams.</p>"},{"location":"docs/plugins/builtin/converters/#converter-overview","title":"Converter Overview","text":"Converter Purpose Input Formats Output Archive Converter Extract compressed archives zip, tar, tar.gz Extracted files of known scannable extensions Jupyter Converter Process Jupyter notebooks .ipynb Python source code"},{"location":"docs/plugins/builtin/converters/#converter-details","title":"Converter Details","text":""},{"location":"docs/plugins/builtin/converters/#archive-converter","title":"Archive Converter","text":"<p>Purpose: Automatically extracts compressed archives to enable scanning of contained files.</p> <p>Supported Formats: - ZIP files (.zip) - TAR archives (.tar, .tar.gz, .tgz)</p> <p>Configuration: <pre><code>converters:\n  archive:\n    enabled: true\n    options:\n      max_extraction_depth: 3\n      max_file_size: \"100MB\"\n      preserve_permissions: true\n      extract_nested: true\n</code></pre></p> <p>Key Features: - Recursive extraction of nested archives - Size and depth limits for security - Permission preservation - Automatic cleanup after scanning</p> <p>Use Cases: - Scanning packaged applications - Analyzing deployment artifacts - Processing downloaded dependencies - Auditing compressed source code</p>"},{"location":"docs/plugins/builtin/converters/#jupyter-converter","title":"Jupyter Converter","text":"<p>Purpose: Extracts Python code from Jupyter notebooks for security analysis.</p> <p>Configuration: <pre><code>converters:\n  jupyter:\n    enabled: true\n    options:\n      extract_code_cells: true\n      extract_markdown_cells: false\n      preserve_cell_numbers: true\n      output_format: \"python\"\n</code></pre></p> <p>Key Features: - Code cell extraction - Cell number preservation for accurate line mapping - Markdown cell processing (optional) - Python syntax validation</p> <p>Use Cases: - Data science project security - ML model code analysis - Educational content scanning - Research code auditing</p>"},{"location":"docs/plugins/builtin/converters/#configuration-examples","title":"Configuration Examples","text":""},{"location":"docs/plugins/builtin/converters/#basic-configuration","title":"Basic Configuration","text":"<pre><code>converters:\n  archive:\n    enabled: true\n  jupyter:\n    enabled: true\n</code></pre>"},{"location":"docs/plugins/builtin/converters/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>converters:\n  archive:\n    enabled: true\n    options:\n      max_extraction_depth: 2\n      max_file_size: \"50MB\"\n      allowed_extensions: [\".zip\", \".tar.gz\", \".7z\"]\n      exclude_patterns: [\"*.exe\", \"*.dll\"]\n\n  jupyter:\n    enabled: true\n    options:\n      extract_code_cells: true\n      extract_markdown_cells: true\n      cell_separator: \"# %%\"\n      validate_syntax: true\n</code></pre>"},{"location":"docs/plugins/builtin/converters/#best-practices","title":"Best Practices","text":""},{"location":"docs/plugins/builtin/converters/#archive-security","title":"Archive Security","text":"<pre><code>converters:\n  archive:\n    options:\n      max_extraction_depth: 3    # Prevent zip bombs\n      max_file_size: \"100MB\"     # Limit resource usage\n      scan_extracted_only: true  # Don't scan original archives\n</code></pre>"},{"location":"docs/plugins/builtin/converters/#jupyter-processing","title":"Jupyter Processing","text":"<pre><code>converters:\n  jupyter:\n    options:\n      preserve_cell_numbers: true  # Accurate line mapping\n      validate_syntax: true        # Skip malformed cells\n</code></pre>"},{"location":"docs/plugins/builtin/converters/#integration-with-scanners","title":"Integration with Scanners","text":"<p>Converters automatically prepare files for scanner consumption:</p> <pre><code># Archives are extracted, then contents scanned\nash project.zip --scanners bandit,semgrep\n\n# Jupyter notebooks converted to Python, then scanned\nash analysis.ipynb --scanners bandit,detect-secrets\n</code></pre>"},{"location":"docs/plugins/builtin/converters/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/builtin/converters/#archive-issues","title":"Archive Issues","text":"<p>Extraction failures: <pre><code>converters:\n  archive:\n    options:\n      ignore_extraction_errors: true\n      log_extraction_details: true\n</code></pre></p> <p>Large archives: <pre><code>converters:\n  archive:\n    options:\n      max_file_size: \"500MB\"\n      max_extraction_depth: 1\n</code></pre></p>"},{"location":"docs/plugins/builtin/converters/#jupyter-issues","title":"Jupyter Issues","text":"<p>Malformed notebooks: <pre><code>converters:\n  jupyter:\n    options:\n      skip_invalid_cells: true\n      validate_json: true\n</code></pre></p>"},{"location":"docs/plugins/builtin/converters/#next-steps","title":"Next Steps","text":"<ul> <li>Scanner Configuration: Configure security scanners</li> <li>File Processing: Advanced file handling</li> </ul>"},{"location":"docs/plugins/builtin/event-handlers-diagrams/","title":"Built-in Event Handler Diagrams","text":"<p>This document provides visual diagrams of the ASH built-in event handler architecture and workflows using Mermaid.</p>"},{"location":"docs/plugins/builtin/event-handlers-diagrams/#event-system-architecture","title":"Event System Architecture","text":"<p>The following diagram shows the high-level architecture of the ASH event system:</p> <pre><code>flowchart TD\n    A[ASH Core] --&gt; B[Event Emitter]\n\n    B --&gt; C[Event Registry]\n\n    C --&gt; D[Built-in Event Handlers]\n    C --&gt; E[Custom Event Handlers]\n\n    D --&gt; F[Scan Completion Logger]\n    D --&gt; G[Suppression Expiration Checker]\n\n    H[Scanner Plugins] -.-&gt; |Emit Events| B\n    I[Converter Plugins] -.-&gt; |Emit Events| B\n    J[Reporter Plugins] -.-&gt; |Emit Events| B\n\n    F -.-&gt; |Log Messages| K[Console Output]\n    G -.-&gt; |Warning Messages| K\n</code></pre>"},{"location":"docs/plugins/builtin/event-handlers-diagrams/#event-flow-sequence","title":"Event Flow Sequence","text":"<p>The following diagram shows the sequence of events during an ASH scan:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant EM as Event Manager\n    participant SCL as Scan Completion Logger\n    participant SEC as Suppression Expiration Checker\n    participant SP as Scanner Plugins\n    participant CP as Converter Plugins\n    participant RP as Reporter Plugins\n\n    ASH-&gt;&gt;EM: Emit EXECUTION_START\n    EM-&gt;&gt;SEC: Notify EXECUTION_START\n    SEC-&gt;&gt;SEC: Check Suppression Expirations\n    SEC--&gt;&gt;ASH: Log Expiration Warnings\n\n    ASH-&gt;&gt;EM: Emit CONVERSION_START\n    EM-&gt;&gt;CP: Notify CONVERSION_START\n\n    CP-&gt;&gt;CP: Convert Files\n    CP--&gt;&gt;ASH: Return Converted Files\n\n    ASH-&gt;&gt;EM: Emit CONVERSION_COMPLETE\n\n    ASH-&gt;&gt;EM: Emit SCAN_START\n\n    loop For Each Scanner\n        SP-&gt;&gt;SP: Scan Files\n        SP--&gt;&gt;ASH: Return Results\n\n        ASH-&gt;&gt;EM: Emit SCAN_COMPLETE\n        EM-&gt;&gt;SCL: Notify SCAN_COMPLETE\n        SCL-&gt;&gt;SCL: Process Completion\n        SCL--&gt;&gt;ASH: Log Remaining Scanners\n    end\n\n    ASH-&gt;&gt;EM: Emit REPORTING_START\n\n    RP-&gt;&gt;RP: Generate Reports\n    RP--&gt;&gt;ASH: Return Reports\n\n    ASH-&gt;&gt;EM: Emit REPORTING_COMPLETE\n\n    ASH-&gt;&gt;EM: Emit EXECUTION_COMPLETE\n</code></pre>"},{"location":"docs/plugins/builtin/event-handlers-diagrams/#event-handler-registration","title":"Event Handler Registration","text":"<p>The following diagram shows how event handlers are registered:</p> <pre><code>flowchart TD\n    A[ASH Core] --&gt; B[Plugin Manager]\n\n    B --&gt; C[Event Registry]\n\n    D[Built-in Event Handlers] --&gt; E[ASH_EVENT_HANDLERS Dictionary]\n    F[Custom Event Handlers] --&gt; G[User ASH_EVENT_HANDLERS Dictionary]\n\n    E --&gt; H[Event Type Mapping]\n    G --&gt; H\n\n    H --&gt; I[SCAN_COMPLETE Handlers]\n    H --&gt; J[EXECUTION_START Handlers]\n    H --&gt; K[Other Event Type Handlers]\n\n    I --&gt; L[Scan Completion Logger]\n    J --&gt; M[Suppression Expiration Checker]\n\n    C --&gt; I\n    C --&gt; J\n    C --&gt; K\n</code></pre>"},{"location":"docs/plugins/builtin/event-handlers-diagrams/#scan-completion-logger-flow","title":"Scan Completion Logger Flow","text":"<p>The following diagram shows the flow of the Scan Completion Logger:</p> <pre><code>flowchart TD\n    A[SCAN_COMPLETE Event] --&gt; B[Scan Completion Logger]\n\n    B --&gt; C[Extract Event Data]\n\n    C --&gt; D[Get Scanner Name]\n    C --&gt; E[Get Completed Count]\n    C --&gt; F[Get Total Count]\n    C --&gt; G[Get Remaining Count]\n    C --&gt; H[Get Remaining Scanners]\n\n    D --&gt; I[Format Log Message]\n    E --&gt; I\n    F --&gt; I\n    G --&gt; I\n    H --&gt; I\n\n    I --&gt; J{Remaining Scanners?}\n\n    J --&gt;|Yes| K[Log Remaining Scanners]\n    J --&gt;|No| L[Log All Complete]\n\n    K --&gt; M[Return True]\n    L --&gt; M\n</code></pre>"},{"location":"docs/plugins/builtin/event-handlers-diagrams/#suppression-expiration-checker-flow","title":"Suppression Expiration Checker Flow","text":"<p>The following diagram shows the flow of the Suppression Expiration Checker:</p> <pre><code>flowchart TD\n    A[EXECUTION_START Event] --&gt; B[Suppression Expiration Checker]\n\n    B --&gt; C[Extract Event Data]\n\n    C --&gt; D[Get Configuration]\n\n    D --&gt; E[Extract Suppressions]\n\n    E --&gt; F{Has Suppressions?}\n\n    F --&gt;|Yes| G[Check Expiration Dates]\n    F --&gt;|No| H[Skip Check]\n\n    G --&gt; I{Any Expiring Soon?}\n\n    I --&gt;|Yes| J[Format Warning Message]\n    I --&gt;|No| K[Skip Warning]\n\n    J --&gt; L[Log Warning Message]\n\n    L --&gt; M[Return True]\n    H --&gt; M\n    K --&gt; M\n</code></pre>"},{"location":"docs/plugins/builtin/event-handlers-diagrams/#event-type-hierarchy","title":"Event Type Hierarchy","text":"<p>The following diagram shows the hierarchy of event types:</p> <pre><code>flowchart TD\n    A[ASH Events] --&gt; B[Phase Events]\n    A --&gt; C[Status Events]\n\n    B --&gt; D[EXECUTION_START]\n    B --&gt; E[EXECUTION_COMPLETE]\n    B --&gt; F[CONVERSION_START]\n    B --&gt; G[CONVERSION_COMPLETE]\n    B --&gt; H[SCAN_START]\n    B --&gt; I[SCAN_COMPLETE]\n    B --&gt; J[REPORTING_START]\n    B --&gt; K[REPORTING_COMPLETE]\n\n    C --&gt; L[INFO]\n    C --&gt; M[WARNING]\n    C --&gt; N[ERROR]\n\n    D --&gt; O[Suppression Expiration Checker]\n    I --&gt; P[Scan Completion Logger]\n</code></pre>"},{"location":"docs/plugins/builtin/event-handlers-diagrams/#event-data-flow","title":"Event Data Flow","text":"<p>The following diagram shows the data flow through the event system:</p> <pre><code>flowchart LR\n    A[Event Source] --&gt; B[Event Data]\n\n    B --&gt; C[Event Type]\n    B --&gt; D[Phase Data]\n    B --&gt; E[Plugin Context]\n    B --&gt; F[Additional Data]\n\n    C --&gt; G[Event Registry]\n\n    G --&gt; H[Event Handlers]\n\n    D --&gt; H\n    E --&gt; H\n    F --&gt; H\n\n    H --&gt; I[Handler Actions]\n\n    I --&gt; J[Logging]\n    I --&gt; K[Notifications]\n    I --&gt; L[Custom Logic]\n</code></pre>"},{"location":"docs/plugins/builtin/event-handlers-diagrams/#custom-event-handler-integration","title":"Custom Event Handler Integration","text":"<p>The following diagram shows how custom event handlers can be integrated:</p> <pre><code>flowchart TD\n    A[Custom Plugin Module] --&gt; B[__init__.py]\n\n    B --&gt; C[Define Event Handlers]\n    C --&gt; D[Create ASH_EVENT_HANDLERS Dictionary]\n\n    D --&gt; E{Event Type}\n\n    E --&gt;|SCAN_COMPLETE| F[Handler List 1]\n    E --&gt;|EXECUTION_START| G[Handler List 2]\n    E --&gt;|ERROR| H[Handler List 3]\n\n    F --&gt; I[Handler Function 1]\n    F --&gt; J[Handler Function 2]\n\n    G --&gt; K[Handler Function 3]\n\n    H --&gt; L[Handler Function 4]\n\n    M[ASH Plugin Manager] --&gt; N[Load Plugin Module]\n    N --&gt; O[Register Event Handlers]\n\n    D --&gt; O\n</code></pre>"},{"location":"docs/plugins/builtin/event-handlers/","title":"Built-in Event Handlers","text":"<p>ASH includes built-in event handlers that respond to scan lifecycle events.</p> <p>For detailed visual diagrams of the built-in event handler architecture and workflows, see Built-in Event Handler Diagrams.</p>"},{"location":"docs/plugins/builtin/event-handlers/#event-handler-overview","title":"Event Handler Overview","text":"Handler Purpose Events Key Features Scan Completion Logger Enhanced scan progress logging SCAN_COMPLETE Remaining scanner tracking Suppression Expiration Checker Check for expiring suppressions EXECUTION_START Proactive expiration warnings"},{"location":"docs/plugins/builtin/event-handlers/#built-in-event-handlers_1","title":"Built-in Event Handlers","text":""},{"location":"docs/plugins/builtin/event-handlers/#scan-completion-logger","title":"Scan Completion Logger","text":"<p>Purpose: Provides enhanced logging when scanners complete, showing remaining scanner information.</p> <p>Events: <code>SCAN_COMPLETE</code></p> <p>Module: <code>automated_security_helper.plugin_modules.ash_builtin.event_handlers.scan_completion_logger</code></p> <p>Example Output: <pre><code>INFO: Remaining scanners (2): semgrep, checkov\nINFO: All scanners completed!\n</code></pre></p>"},{"location":"docs/plugins/builtin/event-handlers/#suppression-expiration-checker","title":"Suppression Expiration Checker","text":"<p>Purpose: Checks for suppressions that are approaching their expiration date and warns users at the start of execution.</p> <p>Events: <code>EXECUTION_START</code></p> <p>Module: <code>automated_security_helper.plugin_modules.ash_builtin.event_handlers.suppression_expiration_checker</code></p> <p>Key Features: - Warns about suppressions expiring within 30 days - Provides helpful guidance on updating suppressions - Respects the <code>--ignore-suppressions</code> flag - Detailed logging with rule ID, file path, expiration date, and reason</p> <p>Example Output: <pre><code>The following suppressions will expire within 30 days:\n  - Rule B101 for src/test.py expires on 2024-07-15. Reason: Test file with intentional assert\nTo update suppression expiration dates, modify your ASH configuration file\n</code></pre></p>"},{"location":"docs/plugins/builtin/event-handlers/#enhanced-offline-mode-logging","title":"Enhanced Offline Mode Logging","text":"<p>The offline mode validation system has been enhanced with emoji-prefixed logging:</p> <ul> <li><code>Semgrep offline mode: Found 150 rule files in cache</code></li> <li><code>Semgrep offline mode: SEMGREP_RULES_CACHE_DIR not set, falling back to p/ci</code></li> <li><code>Grype offline mode: Cache directory validated with 5 files</code></li> <li><code>Grype offline mode: Database is 45 days old, consider updating</code></li> </ul>"},{"location":"docs/plugins/builtin/event-handlers/#custom-event-handlers","title":"Custom Event Handlers","text":"<p>You can create custom event handlers:</p> <pre><code>from automated_security_helper.plugins.events import AshEventType\nfrom automated_security_helper.plugins import ash_plugin_manager\n\ndef my_custom_callback(**kwargs):\n    scanner = kwargs.get(\"scanner\", \"unknown\")\n    print(f\"Scanner {scanner} completed!\")\n    return True\n\nash_plugin_manager.subscribe(AshEventType.SCAN_COMPLETE, my_custom_callback)\n</code></pre>"},{"location":"docs/plugins/builtin/event-handlers/#benefits","title":"Benefits","text":"<ol> <li>Better Observability - Comprehensive visibility into scan execution</li> <li>Integration Capabilities - Easy integration with external systems</li> <li>Custom Automation - Trigger custom actions based on scan events</li> <li>Improved Debugging - Detailed event logging for troubleshooting</li> </ol>"},{"location":"docs/plugins/builtin/reporters-diagrams/","title":"Built-in Reporter Diagrams","text":"<p>This document provides visual diagrams of the ASH built-in reporter architecture and workflows using Mermaid.</p>"},{"location":"docs/plugins/builtin/reporters-diagrams/#reporter-architecture-overview","title":"Reporter Architecture Overview","text":"<p>The following diagram shows the high-level architecture of the ASH built-in reporters:</p> <pre><code>flowchart TD\n    A[ASH Core] --&gt; B[Plugin Manager]\n    B --&gt; C[Reporter Registry]\n\n    C --&gt; D[Built-in Reporters]\n\n    D --&gt; E[CSV Reporter]\n    D --&gt; F[CycloneDX Reporter]\n    D --&gt; G[Flat JSON Reporter]\n    D --&gt; H[GitLab SAST Reporter]\n    D --&gt; I[HTML Reporter]\n    D --&gt; J[JUnit XML Reporter]\n    D --&gt; K[Markdown Reporter]\n    D --&gt; L[OCSF Reporter]\n    D --&gt; M[SARIF Reporter]\n    D --&gt; N[SPDX Reporter]\n    D --&gt; O[Text Reporter]\n    D --&gt; P[YAML Reporter]\n\n    Q[Aggregated Results] --&gt; D\n\n    E --&gt; R[Output Files]\n    F --&gt; R\n    G --&gt; R\n    H --&gt; R\n    I --&gt; R\n    J --&gt; R\n    K --&gt; R\n    L --&gt; R\n    M --&gt; R\n    N --&gt; R\n    O --&gt; R\n    P --&gt; R\n</code></pre>"},{"location":"docs/plugins/builtin/reporters-diagrams/#reporter-execution-flow","title":"Reporter Execution Flow","text":"<p>The following diagram shows the execution flow of the built-in reporters:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant PM as Plugin Manager\n    participant RR as Reporter Registry\n    participant RP as Reporter\n    participant FS as File System\n    participant ES as Event System\n\n    ASH-&gt;&gt;PM: Load Reporters\n    PM-&gt;&gt;RR: Get Registered Reporters\n    RR--&gt;&gt;PM: Return Reporter List\n\n    ASH-&gt;&gt;ES: Emit ReportingStarted Event\n\n    loop For Each Reporter\n        ASH-&gt;&gt;RP: Validate Reporter\n        RP--&gt;&gt;ASH: Return Validation Status\n\n        alt Reporter Valid\n            ASH-&gt;&gt;RP: report(aggregated_results)\n            RP-&gt;&gt;RP: Process Results\n            RP-&gt;&gt;FS: Write Report File\n            RP--&gt;&gt;ASH: Return Report Path/URL\n        else Reporter Invalid\n            ASH-&gt;&gt;ES: Emit ReporterError Event\n        end\n    end\n\n    ASH-&gt;&gt;ES: Emit ReportingCompleted Event\n</code></pre>"},{"location":"docs/plugins/builtin/reporters-diagrams/#reporter-format-classification","title":"Reporter Format Classification","text":"<p>The following diagram shows the classification of built-in reporters by output format:</p> <pre><code>flowchart TD\n    A[Built-in Reporters] --&gt; B[Human-Readable]\n    A --&gt; C[Machine-Readable]\n    A --&gt; D[Compliance]\n    A --&gt; E[CI/CD Integration]\n\n    B --&gt; F[Text Reporter]\n    B --&gt; G[HTML Reporter]\n    B --&gt; H[Markdown Reporter]\n\n    C --&gt; I[JSON Reporter]\n    C --&gt; J[YAML Reporter]\n    C --&gt; K[CSV Reporter]\n\n    D --&gt; L[SPDX Reporter]\n    D --&gt; M[CycloneDX Reporter]\n    D --&gt; N[OCSF Reporter]\n\n    E --&gt; O[SARIF Reporter]\n    E --&gt; P[JUnit XML Reporter]\n    E --&gt; Q[GitLab SAST Reporter]\n</code></pre>"},{"location":"docs/plugins/builtin/reporters-diagrams/#reporter-data-flow","title":"Reporter Data Flow","text":"<p>The following diagram shows the data flow through the built-in reporters:</p> <pre><code>flowchart LR\n    A[ASH Aggregated Results] --&gt; B[Reporter]\n\n    B --&gt; C{Format Type}\n\n    C --&gt;|Text-based| D[Text Formatter]\n    C --&gt;|Structured| E[Data Formatter]\n    C --&gt;|Visual| F[Visual Formatter]\n\n    D --&gt; G[Text Output]\n    D --&gt; H[Markdown Output]\n\n    E --&gt; I[JSON Output]\n    E --&gt; J[YAML Output]\n    E --&gt; K[XML Output]\n    E --&gt; L[CSV Output]\n\n    F --&gt; M[HTML Output]\n\n    G --&gt; N[Output Files]\n    H --&gt; N\n    I --&gt; N\n    J --&gt; N\n    K --&gt; N\n    L --&gt; N\n    M --&gt; N\n</code></pre>"},{"location":"docs/plugins/builtin/reporters-diagrams/#reporter-configuration-flow","title":"Reporter Configuration Flow","text":"<p>The following diagram shows how configuration flows through the built-in reporters:</p> <pre><code>flowchart TD\n    A[.ash/.ash.yaml] --&gt; B[Configuration Parser]\n    C[CLI Arguments] --&gt; B\n    B --&gt; D[ASH Configuration]\n    D --&gt; E[Reporter Configuration]\n\n    E --&gt; F[Global Reporter Settings]\n    E --&gt; G[Reporter-Specific Settings]\n\n    F --&gt; H[Output Directory]\n    F --&gt; I[Include Suppressed]\n    F --&gt; J[Output Format]\n\n    G --&gt; K[HTML Config]\n    G --&gt; L[SARIF Config]\n    G --&gt; M[CSV Config]\n    G --&gt; N[Other Reporter Configs]\n\n    K --&gt; O[HTML Reporter]\n    L --&gt; P[SARIF Reporter]\n    M --&gt; Q[CSV Reporter]\n    N --&gt; R[Other Reporters]\n</code></pre>"},{"location":"docs/plugins/builtin/reporters-diagrams/#multi-reporter-workflow","title":"Multi-Reporter Workflow","text":"<p>The following diagram shows the workflow when multiple reporters are enabled:</p> <pre><code>flowchart TD\n    A[ASH Scan Results] --&gt; B[Results Aggregator]\n\n    B --&gt; C[HTML Reporter]\n    B --&gt; D[SARIF Reporter]\n    B --&gt; E[Text Reporter]\n    B --&gt; F[CSV Reporter]\n\n    C --&gt; G[HTML Report]\n    D --&gt; H[SARIF Report]\n    E --&gt; I[Text Report]\n    F --&gt; J[CSV Report]\n\n    G --&gt; K[Developer Review]\n    H --&gt; L[IDE Integration]\n    I --&gt; M[Console Output]\n    J --&gt; N[Data Analysis]\n\n    K --&gt; O[Security Team]\n    L --&gt; O\n    M --&gt; O\n    N --&gt; O\n</code></pre>"},{"location":"docs/plugins/builtin/reporters-diagrams/#reporter-integration-points","title":"Reporter Integration Points","text":"<p>The following diagram shows the integration points of built-in reporters:</p> <pre><code>flowchart LR\n    A[ASH Reports] --&gt; B[Development Tools]\n    A --&gt; C[CI/CD Systems]\n    A --&gt; D[Security Tools]\n    A --&gt; E[Compliance Systems]\n\n    B --&gt; F[IDEs]\n    B --&gt; G[Code Review Tools]\n\n    C --&gt; H[GitHub Actions]\n    C --&gt; I[GitLab CI]\n    C --&gt; J[Jenkins]\n\n    D --&gt; K[SIEM Systems]\n    D --&gt; L[Security Dashboards]\n\n    E --&gt; M[Compliance Dashboards]\n    E --&gt; N[Audit Systems]\n\n    F --&gt; O[VS Code]\n    F --&gt; P[IntelliJ]\n\n    G --&gt; Q[GitHub PR Comments]\n    G --&gt; R[GitLab MR Comments]\n\n    H --&gt; S[GitHub Security Tab]\n    I --&gt; T[GitLab Security Dashboard]\n    J --&gt; U[Jenkins Test Results]\n\n    K --&gt; V[Splunk]\n    K --&gt; W[ELK Stack]\n\n    L --&gt; X[Security Metrics]\n\n    M --&gt; Y[Compliance Reports]\n    N --&gt; Z[Audit Logs]\n</code></pre>"},{"location":"docs/plugins/builtin/reporters-diagrams/#reporter-output-organization","title":"Reporter Output Organization","text":"<p>The following diagram shows the organization of reporter outputs:</p> <pre><code>flowchart TD\n    A[Output Directory] --&gt; B[reports/]\n\n    B --&gt; C[html/]\n    B --&gt; D[sarif/]\n    B --&gt; E[csv/]\n    B --&gt; F[json/]\n    B --&gt; G[xml/]\n    B --&gt; H[markdown/]\n\n    C --&gt; I[index.html]\n    C --&gt; J[assets/]\n\n    D --&gt; K[results.sarif]\n\n    E --&gt; L[findings.csv]\n    E --&gt; M[summary.csv]\n\n    F --&gt; N[results.json]\n    F --&gt; O[summary.json]\n\n    G --&gt; P[junit.xml]\n    G --&gt; Q[spdx.xml]\n\n    H --&gt; R[report.md]\n    H --&gt; S[summary.md]\n</code></pre>"},{"location":"docs/plugins/builtin/reporters-diagrams/#reporter-error-handling","title":"Reporter Error Handling","text":"<p>The following diagram shows the error handling flow in reporters:</p> <pre><code>flowchart TD\n    A[Start Reporting] --&gt; B{Reporter Available?}\n\n    B --&gt;|Yes| C[Run Reporter]\n    B --&gt;|No| D[Log Error]\n\n    C --&gt; E{Reporting Successful?}\n\n    E --&gt;|Yes| F[Return Report Path/URL]\n    E --&gt;|No| G{Error Type?}\n\n    G --&gt;|Dependency Missing| H[Log Dependency Error]\n    G --&gt;|File System Error| I[Log File System Error]\n    G --&gt;|Formatting Error| J[Log Formatting Error]\n    G --&gt;|Other| K[Log Generic Error]\n\n    F --&gt; L[End]\n    H --&gt; L\n    I --&gt; L\n    J --&gt; L\n    K --&gt; L\n    D --&gt; L\n</code></pre>"},{"location":"docs/plugins/builtin/reporters/","title":"Built-in Reporters","text":"<p>ASH includes 13 built-in reporters that generate scan results in various formats to support different use cases, from human-readable reports to machine-processable data formats for CI/CD integration.</p> <p>For detailed visual diagrams of the built-in reporter architecture and workflows, see Built-in Reporter Diagrams.</p>"},{"location":"docs/plugins/builtin/reporters/#reporter-overview","title":"Reporter Overview","text":"Reporter Format Use Case Key Features CSV Reporter CSV Data analysis, spreadsheets Tabular data, easy filtering CycloneDX Reporter JSON/XML SBOM compliance Software Bill of Materials Flat JSON Reporter JSON Simple data processing Flattened structure GitLab SAST Reporter JSON GitLab Security Dashboard GitLab CI/CD integration HTML Reporter HTML Interactive reports Web-based, searchable JUnit XML Reporter XML CI/CD test results Test framework integration Markdown Reporter Markdown Documentation, README Human-readable, version control friendly OCSF Reporter JSON Security data lakes Open Cybersecurity Schema Framework SARIF Reporter JSON IDE integration, CI/CD Static Analysis Results Interchange Format SPDX Reporter JSON License compliance Software Package Data Exchange Text Reporter Plain text Console output, logs Simple, lightweight YAML Reporter YAML Configuration-style output Human-readable structured data"},{"location":"docs/plugins/builtin/reporters/#reporter-details","title":"Reporter Details","text":""},{"location":"docs/plugins/builtin/reporters/#csv-reporter","title":"CSV Reporter","text":"<p>Purpose: Exports findings in comma-separated values format for spreadsheet analysis.</p> <p>Configuration: <pre><code>reporters:\n  csv:\n    enabled: true\n    options:\n      include_suppressed: false\n      delimiter: \",\"\n      quote_char: \"\\\"\"\n</code></pre></p> <p>Output Structure: - Scanner name - File path - Line number - Severity level - Rule ID - Description - Suppression status</p> <p>Use Cases: - Data analysis in Excel/Google Sheets - Custom reporting dashboards - Bulk finding management</p>"},{"location":"docs/plugins/builtin/reporters/#cyclonedx-reporter","title":"CycloneDX Reporter","text":"<p>Purpose: Generates Software Bill of Materials (SBOM) in CycloneDX format.</p> <p>Configuration: <pre><code>reporters:\n  cyclonedx:\n    enabled: true\n    options:\n      format: \"json\"  # json, xml\n      include_licenses: true\n      include_vulnerabilities: true\n</code></pre></p> <p>Key Features: - Component inventory - Dependency relationships - Vulnerability mappings - License information - Supply chain transparency</p> <p>Use Cases: - Software supply chain security - Compliance reporting - Vulnerability management - License tracking</p>"},{"location":"docs/plugins/builtin/reporters/#flat-json-reporter","title":"Flat JSON Reporter","text":"<p>Purpose: Simplified JSON format with flattened structure for easy processing.</p> <p>Configuration: <pre><code>reporters:\n  flatjson:\n    enabled: true\n    options:\n      pretty_print: true\n      include_metadata: true\n</code></pre></p> <p>Output Structure: <pre><code>{\n  \"findings\": [\n    {\n      \"scanner\": \"bandit\",\n      \"file\": \"src/app.py\",\n      \"line\": 42,\n      \"severity\": \"HIGH\",\n      \"rule_id\": \"B602\",\n      \"message\": \"subprocess call with shell=True\",\n      \"suppressed\": false\n    }\n  ]\n}\n</code></pre></p> <p>Use Cases: - Simple data processing scripts - Custom integrations - Lightweight parsing</p>"},{"location":"docs/plugins/builtin/reporters/#gitlab-sast-reporter","title":"GitLab SAST Reporter","text":"<p>Purpose: Generates reports compatible with GitLab Security Dashboard.</p> <p>Configuration: <pre><code>reporters:\n  gitlab_sast:\n    enabled: true\n    options:\n      version: \"15.0.4\"\n      include_dismissed: false\n</code></pre></p> <p>Key Features: - GitLab Security Dashboard integration - Vulnerability tracking - Merge request security widgets - Pipeline security reports</p> <p>Use Cases: - GitLab CI/CD pipelines - Security dashboard visualization - Merge request security gates</p>"},{"location":"docs/plugins/builtin/reporters/#gitlab-sast-reporter_1","title":"GitLab SAST Reporter","text":"<p>Purpose: Generates reports in GitLab Security Dashboard format for seamless CI/CD integration.</p> <p>Configuration: <pre><code>reporters:\n  gitlab-sast:\n    enabled: true\n    options:\n      include_suppressed: false\n</code></pre></p> <p>Output Structure: - GitLab SAST report format - Vulnerability details with locations - Severity mapping to GitLab standards - Scanner metadata and timestamps</p> <p>Use Cases: - GitLab CI/CD pipeline integration - GitLab Security Dashboard visualization - Compliance with GitLab security workflows</p> <p>Integration Example: <pre><code># .gitlab-ci.yml\nsecurity_scan:\n  stage: test\n  script:\n    - ash . --reporters gitlab-sast\n  artifacts:\n    reports:\n      sast: output/gl-sast-report.json\n</code></pre></p>"},{"location":"docs/plugins/builtin/reporters/#html-reporter","title":"HTML Reporter","text":"<p>Purpose: Interactive web-based report with search and filtering capabilities.</p> <p>Configuration: <pre><code>reporters:\n  html:\n    enabled: true\n    options:\n      include_suppressed: false\n      theme: \"light\"  # light, dark\n      show_metrics: true\n      embed_assets: true\n</code></pre></p> <p>Key Features: - Interactive filtering and search - Severity-based color coding - Expandable finding details - Summary statistics - Responsive design</p> <p>Use Cases: - Security team reviews - Executive reporting - Developer feedback - Audit documentation</p>"},{"location":"docs/plugins/builtin/reporters/#junit-xml-reporter","title":"JUnit XML Reporter","text":"<p>Purpose: Formats results as JUnit XML for CI/CD test result integration.</p> <p>Configuration: <pre><code>reporters:\n  junitxml:\n    enabled: true\n    options:\n      suite_name: \"ASH Security Scan\"\n      failure_on_finding: true\n</code></pre></p> <p>Key Features: - Test framework compatibility - CI/CD integration - Pass/fail status per scanner - Detailed failure messages</p> <p>Use Cases: - Jenkins test results - GitLab CI test reporting - GitHub Actions test summaries - Build pipeline gates</p>"},{"location":"docs/plugins/builtin/reporters/#markdown-reporter","title":"Markdown Reporter","text":"<p>Purpose: Human-readable report in Markdown format for documentation.</p> <p>Configuration: <pre><code>reporters:\n  markdown:\n    enabled: true\n    options:\n      include_toc: true\n      include_suppressed: false\n      max_findings_per_scanner: 50\n</code></pre></p> <p>Key Features: - GitHub/GitLab compatible - Table of contents - Code syntax highlighting - Collapsible sections</p> <p>Use Cases: - README security sections - Pull request comments - Documentation sites - Security runbooks</p>"},{"location":"docs/plugins/builtin/reporters/#ocsf-reporter","title":"OCSF Reporter","text":"<p>Purpose: Outputs findings in Open Cybersecurity Schema Framework format.</p> <p>Configuration: <pre><code>reporters:\n  ocsf:\n    enabled: true\n    options:\n      version: \"1.0.0\"\n      include_raw_data: false\n</code></pre></p> <p>Key Features: - Standardized security data format - Cloud-native security tools integration - Rich metadata support - Event correlation capabilities</p> <p>Use Cases: - Security data lakes - SIEM integration - Security analytics platforms - Compliance reporting</p>"},{"location":"docs/plugins/builtin/reporters/#sarif-reporter","title":"SARIF Reporter","text":"<p>Purpose: Static Analysis Results Interchange Format for tool interoperability.</p> <p>Configuration: <pre><code>reporters:\n  sarif:\n    enabled: true\n    options:\n      include_rule_metadata: true\n      schema_version: \"2.1.0\"\n      pretty_print: false\n</code></pre></p> <p>Key Features: - IDE integration (VS Code, IntelliJ) - GitHub Security tab integration - Rich metadata and locations - Tool interoperability</p> <p>Use Cases: - IDE security annotations - GitHub Advanced Security - Security tool chains - Compliance reporting</p>"},{"location":"docs/plugins/builtin/reporters/#spdx-reporter","title":"SPDX Reporter","text":"<p>Purpose: Software Package Data Exchange format for license compliance.</p> <p>Configuration: <pre><code>reporters:\n  spdx:\n    enabled: true\n    options:\n      format: \"json\"  # json, yaml, tag-value\n      include_files: true\n      document_name: \"ASH-SPDX-Report\"\n</code></pre></p> <p>Key Features: - License identification - Copyright information - Package relationships - File-level details</p> <p>Use Cases: - License compliance - Open source governance - Legal review processes - Supply chain transparency</p>"},{"location":"docs/plugins/builtin/reporters/#text-reporter","title":"Text Reporter","text":"<p>Purpose: Simple plain text output for console display and logging.</p> <p>Configuration: <pre><code>reporters:\n  text:\n    enabled: true\n    options:\n      show_summary: true\n      show_suppressed: false\n      max_line_length: 120\n      color_output: true\n</code></pre></p> <p>Key Features: - Console-friendly output - Color-coded severity levels - Compact summary format - Configurable verbosity</p> <p>Use Cases: - Command-line usage - Log file output - Simple CI/CD notifications - Quick security overviews</p>"},{"location":"docs/plugins/builtin/reporters/#yaml-reporter","title":"YAML Reporter","text":"<p>Purpose: Structured YAML output for configuration-style data representation.</p> <p>Configuration: <pre><code>reporters:\n  yaml:\n    enabled: true\n    options:\n      pretty_print: true\n      include_metadata: true\n      flow_style: false\n</code></pre></p> <p>Key Features: - Human-readable structure - Configuration file compatibility - Hierarchical data organization - Comment support</p> <p>Use Cases: - Configuration-based workflows - Infrastructure as Code integration - Human-readable data exchange - Custom processing pipelines</p>"},{"location":"docs/plugins/builtin/reporters/#multi-reporter-usage","title":"Multi-Reporter Usage","text":""},{"location":"docs/plugins/builtin/reporters/#common-combinations","title":"Common Combinations","text":"<pre><code># Development workflow\nash --reporters text,html,sarif\n\n# CI/CD pipeline\nash --reporters sarif,junitxml,gitlab-sast\n\n# Compliance reporting\nash --reporters spdx,cyclonedx,ocsf\n\n# Executive reporting\nash --reporters html,markdown,csv\n</code></pre>"},{"location":"docs/plugins/builtin/reporters/#configuration-example","title":"Configuration Example","text":"<pre><code>reporters:\n  # Quick feedback\n  text:\n    enabled: true\n    options:\n      show_summary: true\n      color_output: true\n\n  # Detailed analysis\n  html:\n    enabled: true\n    options:\n      theme: \"light\"\n      include_suppressed: false\n\n  # CI/CD integration\n  sarif:\n    enabled: true\n    options:\n      include_rule_metadata: true\n\n  # Data processing\n  csv:\n    enabled: true\n    options:\n      include_suppressed: true\n</code></pre>"},{"location":"docs/plugins/builtin/reporters/#best-practices","title":"Best Practices","text":""},{"location":"docs/plugins/builtin/reporters/#reporter-selection","title":"Reporter Selection","text":"<p>Choose reporters based on your audience and use case:</p> <pre><code># For developers\nreporters: [text, sarif, html]\n\n# For security teams\nreporters: [html, csv, ocsf]\n\n# For compliance\nreporters: [spdx, cyclonedx, markdown]\n\n# For CI/CD\nreporters: [sarif, junitxml, gitlab-sast]\n</code></pre>"},{"location":"docs/plugins/builtin/reporters/#performance-considerations","title":"Performance Considerations","text":"<pre><code># Optimize for speed\nreporters:\n  html:\n    options:\n      embed_assets: false  # Faster generation\n\n  csv:\n    options:\n      include_suppressed: false  # Smaller files\n</code></pre>"},{"location":"docs/plugins/builtin/reporters/#output-organization","title":"Output Organization","text":"<pre><code># Organize outputs by type\nash --output-dir results/ \\\n  --reporters sarif,html,csv \\\n  --output-format \"{reporter}/{timestamp}\"\n</code></pre>"},{"location":"docs/plugins/builtin/reporters/#integration-examples","title":"Integration Examples","text":""},{"location":"docs/plugins/builtin/reporters/#github-actions","title":"GitHub Actions","text":"<pre><code>- name: Security Scan\n  run: ash --reporters sarif,text\n\n- name: Upload SARIF\n  uses: github/codeql-action/upload-sarif@v2\n  with:\n    sarif_file: results/sarif/results.sarif\n</code></pre>"},{"location":"docs/plugins/builtin/reporters/#gitlab-ci","title":"GitLab CI","text":"<pre><code>security_scan:\n  script:\n    - ash --reporters gitlab-sast,text\n  artifacts:\n    reports:\n      sast: results/gitlab-sast/results.json\n</code></pre>"},{"location":"docs/plugins/builtin/reporters/#jenkins","title":"Jenkins","text":"<pre><code>pipeline {\n  stages {\n    stage('Security Scan') {\n      steps {\n        sh 'ash --reporters junitxml,html'\n        publishTestResults testResultsPattern: 'results/junitxml/*.xml'\n        publishHTML([\n          allowMissing: false,\n          alwaysLinkToLastBuild: true,\n          keepAll: true,\n          reportDir: 'results/html',\n          reportFiles: 'index.html',\n          reportName: 'Security Report'\n        ])\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"docs/plugins/builtin/reporters/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/builtin/reporters/#common-issues","title":"Common Issues","text":"<p>Large report files: <pre><code>reporters:\n  html:\n    options:\n      max_findings_per_scanner: 100\n      include_suppressed: false\n</code></pre></p> <p>Encoding issues: <pre><code>reporters:\n  csv:\n    options:\n      encoding: \"utf-8\"\n</code></pre></p> <p>CI/CD integration failures: <pre><code># Validate output format\nash --reporters sarif --validate-output\n</code></pre></p>"},{"location":"docs/plugins/builtin/reporters/#next-steps","title":"Next Steps","text":"<ul> <li>Scanner Configuration: Configure security scanners</li> <li>Suppressions: Manage false positives</li> <li>CI/CD Integration: Automate security scanning</li> </ul>"},{"location":"docs/plugins/builtin/scanners-diagrams/","title":"Built-in Scanner Diagrams","text":"<p>This document provides visual diagrams of the ASH built-in scanner architecture and workflows using Mermaid.</p>"},{"location":"docs/plugins/builtin/scanners-diagrams/#scanner-architecture-overview","title":"Scanner Architecture Overview","text":"<p>The following diagram shows the high-level architecture of the ASH built-in scanners:</p> <pre><code>flowchart TD\n    A[ASH Core] --&gt; B[Plugin Manager]\n    B --&gt; C[Scanner Registry]\n\n    C --&gt; D[Built-in Scanners]\n\n    D --&gt; E[Bandit]\n    D --&gt; F[CDK-Nag]\n    D --&gt; G[CFN-Nag]\n    D --&gt; H[Checkov]\n    D --&gt; I[Detect-Secrets]\n    D --&gt; J[Grype]\n    D --&gt; K[NPM Audit]\n    D --&gt; L[Opengrep]\n    D --&gt; M[Semgrep]\n    D --&gt; N[Syft]\n\n    E --&gt; O[SARIF Results]\n    F --&gt; O\n    G --&gt; O\n    H --&gt; O\n    I --&gt; O\n    J --&gt; O\n    K --&gt; O\n    L --&gt; O\n    M --&gt; O\n    N --&gt; O\n\n    O --&gt; P[Results Aggregator]\n    P --&gt; Q[Reporter Plugins]\n</code></pre>"},{"location":"docs/plugins/builtin/scanners-diagrams/#scanner-execution-flow","title":"Scanner Execution Flow","text":"<p>The following diagram shows the execution flow of the built-in scanners:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant PM as Plugin Manager\n    participant SR as Scanner Registry\n    participant SC as Scanner\n    participant FS as File System\n    participant ES as Event System\n\n    ASH-&gt;&gt;PM: Load Scanners\n    PM-&gt;&gt;SR: Get Registered Scanners\n    SR--&gt;&gt;PM: Return Scanner List\n\n    loop For Each Scanner\n        ASH-&gt;&gt;ES: Emit ScannerStarted Event\n        ASH-&gt;&gt;SC: Validate Scanner\n        SC--&gt;&gt;ASH: Return Validation Status\n\n        alt Scanner Valid\n            ASH-&gt;&gt;SC: scan(target, target_type)\n            SC-&gt;&gt;FS: Read Target Files\n            FS--&gt;&gt;SC: Return File Contents\n\n            SC-&gt;&gt;SC: Process Files\n            Note over SC: Run Security Analysis\n\n            SC-&gt;&gt;FS: Write SARIF Report\n            SC--&gt;&gt;ASH: Return ScanResultsContainer\n\n            ASH-&gt;&gt;ES: Emit ScannerCompleted Event\n        else Scanner Invalid\n            ASH-&gt;&gt;ES: Emit ScannerError Event\n        end\n    end\n\n    ASH-&gt;&gt;ES: Emit ScanCompleted Event\n</code></pre>"},{"location":"docs/plugins/builtin/scanners-diagrams/#scanner-dependency-graph","title":"Scanner Dependency Graph","text":"<p>The following diagram shows the dependency relationships between the built-in scanners:</p> <pre><code>flowchart LR\n    A[ASH Core] --&gt; B[Scanner Base]\n\n    B --&gt; C[Bandit]\n    B --&gt; D[CDK-Nag]\n    B --&gt; E[CFN-Nag]\n    B --&gt; F[Checkov]\n    B --&gt; G[Detect-Secrets]\n    B --&gt; H[Grype]\n    B --&gt; I[NPM Audit]\n    B --&gt; J[Opengrep]\n    B --&gt; K[Semgrep]\n    B --&gt; L[Syft]\n\n    C -.-&gt; M[bandit Python package]\n    D -.-&gt; N[AWS CDK CLI]\n    E -.-&gt; O[cfn-nag Ruby gem]\n    F -.-&gt; P[checkov Python package]\n    G -.-&gt; Q[detect-secrets Python package]\n    H -.-&gt; R[grype binary]\n    I -.-&gt; S[Node.js &amp; npm]\n    J -.-&gt; T[opengrep binary]\n    K -.-&gt; U[semgrep Python package]\n    L -.-&gt; V[syft binary]\n\n    H -.-&gt; L\n</code></pre>"},{"location":"docs/plugins/builtin/scanners-diagrams/#scanner-configuration-flow","title":"Scanner Configuration Flow","text":"<p>The following diagram shows how configuration flows through the built-in scanners:</p> <pre><code>flowchart TD\n    A[.ash/.ash.yaml] --&gt; B[Configuration Parser]\n    C[CLI Arguments] --&gt; B\n    B --&gt; D[ASH Configuration]\n    D --&gt; E[Scanner Configuration]\n\n    E --&gt; F[Global Scanner Settings]\n    E --&gt; G[Scanner-Specific Settings]\n\n    F --&gt; H[Severity Threshold]\n    F --&gt; I[Ignore Paths]\n    F --&gt; J[Max Workers]\n\n    G --&gt; K[Bandit Config]\n    G --&gt; L[Semgrep Config]\n    G --&gt; M[Checkov Config]\n    G --&gt; N[Other Scanner Configs]\n\n    K --&gt; O[Bandit Scanner]\n    L --&gt; P[Semgrep Scanner]\n    M --&gt; Q[Checkov Scanner]\n    N --&gt; R[Other Scanners]\n</code></pre>"},{"location":"docs/plugins/builtin/scanners-diagrams/#scanner-type-classification","title":"Scanner Type Classification","text":"<p>The following diagram shows the classification of built-in scanners by type:</p> <pre><code>flowchart TD\n    A[Built-in Scanners] --&gt; B[Static Analysis]\n    A --&gt; C[Dependency Analysis]\n    A --&gt; D[Secret Detection]\n    A --&gt; E[Infrastructure Analysis]\n    A --&gt; F[Container Analysis]\n\n    B --&gt; G[Bandit]\n    B --&gt; H[Semgrep]\n    B --&gt; I[Opengrep]\n\n    C --&gt; J[NPM Audit]\n    C --&gt; K[Syft]\n\n    D --&gt; L[Detect-Secrets]\n\n    E --&gt; M[CDK-Nag]\n    E --&gt; N[CFN-Nag]\n    E --&gt; O[Checkov]\n\n    F --&gt; P[Grype]\n    F --&gt; Q[Syft]\n</code></pre>"},{"location":"docs/plugins/builtin/scanners-diagrams/#scanner-language-support","title":"Scanner Language Support","text":"<p>The following diagram shows the language support of built-in scanners:</p> <pre><code>flowchart TD\n    A[Languages] --&gt; B[Python]\n    A --&gt; C[JavaScript/TypeScript]\n    A --&gt; D[Infrastructure as Code]\n    A --&gt; E[Multiple Languages]\n    A --&gt; F[Container/Binary]\n\n    B --&gt; G[Bandit]\n\n    C --&gt; H[NPM Audit]\n\n    D --&gt; I[CDK-Nag]\n    D --&gt; J[CFN-Nag]\n    D --&gt; K[Checkov]\n\n    E --&gt; L[Semgrep]\n    E --&gt; M[Opengrep]\n    E --&gt; N[Detect-Secrets]\n\n    F --&gt; O[Grype]\n    F --&gt; P[Syft]\n</code></pre>"},{"location":"docs/plugins/builtin/scanners-diagrams/#scanner-result-processing","title":"Scanner Result Processing","text":"<p>The following diagram shows how scanner results are processed:</p> <pre><code>flowchart LR\n    A[Scanner Output] --&gt; B{Format?}\n\n    B --&gt;|SARIF| C[Direct Integration]\n    B --&gt;|JSON| D[SARIF Converter]\n    B --&gt;|XML| E[SARIF Converter]\n    B --&gt;|Text| F[SARIF Converter]\n\n    C --&gt; G[Results Container]\n    D --&gt; G\n    E --&gt; G\n    F --&gt; G\n\n    G --&gt; H[Results Aggregator]\n    H --&gt; I[Reporter Plugins]\n</code></pre>"},{"location":"docs/plugins/builtin/scanners-diagrams/#scanner-parallel-execution","title":"Scanner Parallel Execution","text":"<p>The following diagram shows the parallel execution of scanners:</p> <pre><code>sequenceDiagram\n    participant ASH as ASH Core\n    participant WP as Worker Pool\n    participant W1 as Worker 1\n    participant W2 as Worker 2\n    participant W3 as Worker 3\n\n    ASH-&gt;&gt;WP: Create Worker Pool (max_workers)\n\n    par Parallel Execution\n        WP-&gt;&gt;W1: Run Bandit Scanner\n        WP-&gt;&gt;W2: Run Semgrep Scanner\n        WP-&gt;&gt;W3: Run Checkov Scanner\n    end\n\n    W1--&gt;&gt;WP: Return Results\n    W2--&gt;&gt;WP: Return Results\n    W3--&gt;&gt;WP: Return Results\n\n    WP--&gt;&gt;ASH: Aggregate Results\n</code></pre>"},{"location":"docs/plugins/builtin/scanners-diagrams/#scanner-error-handling","title":"Scanner Error Handling","text":"<p>The following diagram shows the error handling flow in scanners:</p> <pre><code>flowchart TD\n    A[Start Scan] --&gt; B{Scanner Available?}\n\n    B --&gt;|Yes| C[Run Scanner]\n    B --&gt;|No| D[Log Error]\n\n    C --&gt; E{Scan Successful?}\n\n    E --&gt;|Yes| F[Process Results]\n    E --&gt;|No| G{Error Type?}\n\n    G --&gt;|Dependency Missing| H[Log Dependency Error]\n    G --&gt;|Timeout| I[Log Timeout Error]\n    G --&gt;|Parsing Error| J[Log Parsing Error]\n    G --&gt;|Other| K[Log Generic Error]\n\n    F --&gt; L[Return Results Container]\n    H --&gt; M[Return Empty Container with Error]\n    I --&gt; M\n    J --&gt; M\n    K --&gt; M\n    D --&gt; M\n\n    L --&gt; N[End]\n    M --&gt; N\n</code></pre>"},{"location":"docs/plugins/builtin/scanners/","title":"Built-in Security Scanners","text":"<p>ASH includes 10 built-in security scanners that analyze different aspects of your code and infrastructure. Each scanner specializes in specific security domains and file types.</p> <p>For detailed visual diagrams of the built-in scanner architecture and workflows, see Built-in Scanner Diagrams.</p>"},{"location":"docs/plugins/builtin/scanners/#scanner-overview","title":"Scanner Overview","text":"Scanner Purpose Languages/Formats Key Features Bandit Python security linter Python AST-based analysis, security-focused rules CDK-Nag AWS CDK security checker TypeScript, Python, Java CDK-specific security rules CFN-Nag CloudFormation security YAML, JSON AWS resource security validation Checkov Infrastructure-as-Code scanner Terraform, CF, K8s, Docker Policy-as-code framework Detect-Secrets Secret detection All text files Entropy-based secret detection Grype Container vulnerability scanner Container images, SBOMs CVE database matching NPM Audit Node.js dependency scanner package.json, package-lock.json NPM vulnerability database Opengrep Code pattern matching Multiple languages Custom rule engine Semgrep Static analysis scanner 30+ languages Community and custom rules Syft SBOM generator Container images, filesystems Software inventory generation"},{"location":"docs/plugins/builtin/scanners/#scanner-details","title":"Scanner Details","text":""},{"location":"docs/plugins/builtin/scanners/#bandit","title":"Bandit","text":"<p>Purpose: Identifies common security issues in Python code through AST analysis.</p> <p>Configuration: <pre><code>scanners:\n  bandit:\n    enabled: true\n    severity_threshold: \"MEDIUM\"\n    options:\n      confidence_level: \"HIGH\"  # LOW, MEDIUM, HIGH\n      skips: [\"B101\", \"B601\"]   # Skip specific test IDs\n      tests: [\"B201\", \"B301\"]   # Run only specific tests\n</code></pre></p> <p>Key Checks: - SQL injection vulnerabilities - Hardcoded passwords and secrets - Use of insecure functions - Shell injection risks - Cryptographic weaknesses</p> <p>Dependencies: <code>bandit</code> Python package</p>"},{"location":"docs/plugins/builtin/scanners/#cdk-nag","title":"CDK-Nag","text":"<p>Purpose: Validates AWS CDK constructs against security best practices.</p> <p>Configuration: <pre><code>scanners:\n  cdk_nag:\n    enabled: true\n    options:\n      rules_to_suppress: [\"AwsSolutions-S1\", \"AwsSolutions-S2\"]\n      verbose: true\n</code></pre></p> <p>Key Checks: - S3 bucket security configurations - IAM policy validation - VPC and networking security - Encryption requirements - Logging and monitoring setup</p> <p>Dependencies: AWS CDK CLI, Node.js</p>"},{"location":"docs/plugins/builtin/scanners/#cfn-nag","title":"CFN-Nag","text":"<p>Purpose: Scans CloudFormation templates for security anti-patterns.</p> <p>Configuration: <pre><code>scanners:\n  cfn_nag:\n    enabled: true\n    options:\n      rules_to_suppress: [\"W1\", \"W2\"]\n      fail_on_warnings: false\n</code></pre></p> <p>Key Checks: - IAM policies with excessive permissions - Security groups with open access - Unencrypted resources - Missing logging configurations - Insecure resource configurations</p> <p>Dependencies: <code>cfn-nag</code> Ruby gem</p>"},{"location":"docs/plugins/builtin/scanners/#checkov","title":"Checkov","text":"<p>Purpose: Comprehensive infrastructure-as-code security scanner with policy-as-code framework.</p> <p>Configuration: <pre><code>scanners:\n  checkov:\n    enabled: true\n    options:\n      framework: [\"terraform\", \"cloudformation\", \"kubernetes\"]\n      check: [\"CKV_AWS_*\", \"CKV_K8S_*\"]\n      skip_check: [\"CKV_AWS_123\"]\n      external_checks_dir: \"/path/to/custom/checks\"\n      compact: true\n</code></pre></p> <p>Key Checks: - Cloud resource misconfigurations - Kubernetes security policies - Docker security best practices - Terraform module validation - Custom policy enforcement</p> <p>Dependencies: Managed via <code>uv tool run</code> (automatically downloaded when needed)</p>"},{"location":"docs/plugins/builtin/scanners/#detect-secrets","title":"Detect-Secrets","text":"<p>Purpose: Prevents secrets from being committed to version control through entropy-based detection.</p> <p>Configuration: <pre><code>scanners:\n  detect_secrets:\n    enabled: true\n    options:\n      plugins: [\"ArtifactoryDetector\", \"AWSKeyDetector\", \"Base64HighEntropyString\"]\n      exclude_files: \".*\\\\.lock$\"\n      exclude_lines: \"password.*=.*\\\\{\\\\{.*\\\\}\\\\}\"\n</code></pre></p> <p>Key Checks: - High entropy strings (potential secrets) - AWS access keys and secret keys - Private keys and certificates - Database connection strings - API keys and tokens</p> <p>Dependencies: <code>detect-secrets</code> Python package</p>"},{"location":"docs/plugins/builtin/scanners/#grype","title":"Grype","text":"<p>Purpose: Vulnerability scanner for container images and filesystems using CVE databases.</p> <p>Configuration: <pre><code>scanners:\n  grype:\n    enabled: true\n    options:\n      scope: \"all-layers\"  # all-layers, squashed\n      fail_on: \"medium\"    # negligible, low, medium, high, critical\n</code></pre></p> <p>Key Checks: - Known CVEs in installed packages - Operating system vulnerabilities - Language-specific package vulnerabilities - Container base image issues</p> <p>Dependencies: <code>grype</code> binary</p>"},{"location":"docs/plugins/builtin/scanners/#npm-audit","title":"NPM Audit","text":"<p>Purpose: Identifies known vulnerabilities in Node.js dependencies.</p> <p>Configuration: <pre><code>scanners:\n  npm_audit:\n    enabled: true\n    options:\n      audit_level: \"moderate\"  # info, low, moderate, high, critical\n      production_only: false\n</code></pre></p> <p>Key Checks: - Known vulnerabilities in npm packages - Dependency tree analysis - Severity-based filtering - Fix recommendations</p> <p>Dependencies: Node.js, npm</p>"},{"location":"docs/plugins/builtin/scanners/#opengrep","title":"Opengrep","text":"<p>Purpose: Open source fork of Semgrep. Static analysis with extensive rule library covering security, correctness, and performance.</p> <p>Configuration: <pre><code>scanners:\n  opengrep:\n    enabled: true\n    options:\n      rules: \"auto\"  # auto, or path to rules\n      timeout: 300\n      max_memory: 5000\n</code></pre></p> <p>Key Checks: - Custom security patterns - Code quality issues - Best practice violations - Language-specific anti-patterns</p> <p>Dependencies: <code>opengrep</code> binary</p>"},{"location":"docs/plugins/builtin/scanners/#semgrep","title":"Semgrep","text":"<p>Purpose: Static analysis with extensive rule library covering security, correctness, and performance.</p> <p>Configuration: <pre><code>scanners:\n  semgrep:\n    enabled: true\n    options:\n      rules: \"auto\"  # auto, p/security, p/owasp-top-10, or custom\n      timeout: 300\n      max_memory: 5000\n      exclude: [\"test/\", \"*.min.js\"]\n</code></pre></p> <p>Key Checks: - OWASP Top 10 vulnerabilities - Language-specific security issues - Code quality and maintainability - Custom organizational rules</p> <p>Dependencies: Managed via <code>uv tool run</code> (automatically downloaded when needed)</p>"},{"location":"docs/plugins/builtin/scanners/#syft","title":"Syft","text":"<p>Purpose: Generates Software Bill of Materials (SBOM) for dependency tracking and compliance.</p> <p>Configuration: <pre><code>scanners:\n  syft:\n    enabled: true\n    options:\n      scope: \"all-layers\"  # all-layers, squashed\n      format: \"spdx-json\"  # spdx-json, cyclonedx-json, syft-json\n</code></pre></p> <p>Key Features: - Package discovery across multiple ecosystems - SBOM generation in standard formats - Container and filesystem analysis - License identification</p> <p>Dependencies: <code>syft</code> binary</p>"},{"location":"docs/plugins/builtin/scanners/#best-practices","title":"Best Practices","text":""},{"location":"docs/plugins/builtin/scanners/#scanner-selection","title":"Scanner Selection","text":"<p>Choose scanners based on your technology stack:</p> <pre><code># Python projects\nash --scanners bandit,detect-secrets,semgrep\n\n# Infrastructure projects\nash --scanners checkov,cfn-nag,cdk-nag\n\n# Container projects\nash --scanners grype,syft,checkov\n\n# Node.js projects\nash --scanners npm-audit,detect-secrets,semgrep\n</code></pre>"},{"location":"docs/plugins/builtin/scanners/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Optimize for speed\nscanners:\n  semgrep:\n    options:\n      timeout: 60\n      max_memory: 2000\n\n  grype:\n    options:\n      scope: \"squashed\"  # Faster than all-layers\n</code></pre>"},{"location":"docs/plugins/builtin/scanners/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># Different thresholds for different environments\nscanners:\n  bandit:\n    severity_threshold: \"LOW\"    # Strict for production\n\n  checkov:\n    severity_threshold: \"MEDIUM\" # Balanced for staging\n</code></pre>"},{"location":"docs/plugins/builtin/scanners/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/builtin/scanners/#common-issues","title":"Common Issues","text":"<p>Scanner not found: <pre><code># Check dependencies\nash dependencies --check --scanner bandit\n\n# Install missing tools\npip install bandit semgrep detect-secrets\n</code></pre></p> <p>Performance issues: <pre><code># Run with fewer concurrent scanners\nash --max-workers 2\n\n# Exclude resource-intensive scanners\nash --exclude-scanners grype,syft\n</code></pre></p> <p>False positives: <pre><code># Suppress specific findings\nscanners:\n  bandit:\n    options:\n      skips: [\"B101\"]  # Skip assert_used test\n</code></pre></p>"},{"location":"docs/plugins/builtin/scanners/#next-steps","title":"Next Steps","text":"<ul> <li>Reporter Configuration: Configure output formats</li> <li>Suppressions Guide: Manage false positives</li> <li>Custom Rules: Create organization-specific rules</li> </ul>"},{"location":"docs/plugins/community/","title":"Community Plugins","text":"<p>This section is dedicated to community-developed plugins for ASH. Community plugins extend ASH's functionality with additional scanners, reporters, converters, and event handlers developed by the community.</p>"},{"location":"docs/plugins/community/#available-community-plugins","title":"Available Community Plugins","text":""},{"location":"docs/plugins/community/#security-scanners","title":"Security Scanners","text":"<ul> <li>Ferret Scan Plugin - Integrates Ferret Scan for comprehensive sensitive data detection (credit cards, passports, SSNs, API keys, secrets, and more)</li> <li>Snyk Code Plugin - Integrates Snyk Code for static application security testing (SAST) of source code vulnerabilities</li> <li>Trivy Plugin - Integrates Aquasec's Trivy for vulnerability, misconfiguration, secret, and license scanning</li> </ul>"},{"location":"docs/plugins/community/#contributing-a-community-plugin","title":"Contributing a Community Plugin","text":"<p>We encourage the community to develop and share plugins that extend ASH's capabilities.</p> <p>To contribute a community plugin:</p> <ol> <li>Develop your plugin following the Plugin Development Guide</li> <li>Host your plugin in a public repository</li> <li>Open a pull request to add your plugin to this documentation</li> <li>Include the following information in your PR:</li> <li>Plugin name and description</li> <li>Link to the repository</li> <li>Installation instructions</li> <li>Configuration options</li> <li>Example usage</li> </ol>"},{"location":"docs/plugins/community/#plugin-submission-guidelines","title":"Plugin Submission Guidelines","text":"<p>To ensure quality and security, community plugins should:</p> <ul> <li>Be open source with a compatible license</li> <li>Include comprehensive documentation</li> <li>Follow ASH's plugin development best practices</li> <li>Include tests and examples</li> <li>Be actively maintained</li> </ul>"},{"location":"docs/plugins/community/#plugin-review-process","title":"Plugin Review Process","text":"<p>When you submit a PR to add your plugin to this documentation, the ASH team will:</p> <ol> <li>Review the plugin code for security and quality</li> <li>Test the plugin functionality</li> <li>Provide feedback on any necessary changes</li> <li>Merge the documentation PR once the plugin meets the guidelines</li> </ol>"},{"location":"docs/plugins/community/#example-plugin-documentation-template","title":"Example Plugin Documentation Template","text":"<p>Replace <code>&lt;my-plugin-package-name&gt;</code> with the actual name of the plugin package you are creating.</p> <pre><code>## Plugin Name\n\n**Description**: Brief description of what the plugin does and its key features.\n\\*Description\\*\\*: Brief description of what the plugin does and its key features.\n\n**Repository**: [Link to GitHub/GitLab/etc. repository](https://github.com/username/plugin-repo)\n\n**Author**: Your Name or Organization\n\n**License**: License type (e.g., Apache 2.0, MIT)\n\n### Installation\n\n```bash\n# Installation instructions\npip install &lt;my-plugin-package-name&gt;\n```\n</code></pre>"},{"location":"docs/plugins/community/#configuration","title":"Configuration","text":"<pre><code># Example configuration in .ash.yaml\nplugins:\n  my-plugin:\n    enabled: true\n    options:\n      option1: value1\n      option2: value2\n</code></pre>"},{"location":"docs/plugins/community/#features","title":"Features","text":"<ul> <li>Feature 1: Description</li> <li>Feature 2: Description</li> </ul>"},{"location":"docs/plugins/community/#example-usage","title":"Example Usage","text":"<pre><code># Example command line usage\nash --plugins my-plugin\n</code></pre> <p>[!CAUTION] &gt; Community Plugin Security: Community plugins are third-party packages that you install separately (e.g., via <code>pip install</code>). Always verify the source and trustworthiness of these packages before installation. ASH's built-in plugins are included directly in the ASH repository under <code>automated_security_helper/plugin_modules</code> and don't require separate package installation.</p>"},{"location":"docs/plugins/community/ferret-scan-plugin/","title":"ASH Ferret Scan Plugin","text":"<p>This plugin integrates Ferret Scan with the Automated Security Helper (ASH) to provide comprehensive sensitive data detection for source code and documents.</p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#overview","title":"Overview","text":"<p>The Ferret Scan plugin enables ASH to leverage Ferret's powerful sensitive data detection capabilities for:</p> <ul> <li>Credit Card Detection: 15+ card brands with mathematical validation (Luhn algorithm)</li> <li>Passport Numbers: Multi-country formats including US, UK, Canada, EU, and MRZ</li> <li>Social Security Numbers: Domain-aware validation with HR/Tax/Healthcare context</li> <li>API Keys &amp; Secrets: 40+ patterns including AWS, GitHub, Google Cloud, Stripe, and more</li> <li>Email Addresses: RFC-compliant with domain validation</li> <li>Phone Numbers: International and domestic formats</li> <li>IP Addresses: IPv4 and IPv6 with network context</li> <li>Social Media Profiles: LinkedIn, Twitter/X, Facebook, GitHub, Instagram, TikTok</li> <li>Intellectual Property: Patents, trademarks, copyrights, trade secrets</li> <li>Document Metadata: EXIF and document metadata extraction</li> </ul>"},{"location":"docs/plugins/community/ferret-scan-plugin/#ash-convention-compliance","title":"ASH Convention Compliance","text":"<p>This plugin follows ASH conventions for consistent behavior across all scanners:</p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#hardcoded-by-plugin-do-not-configure-at-plugin-level","title":"Hardcoded by Plugin (Do NOT configure at plugin level)","text":"Feature ASH Convention Notes Output Format Always SARIF Required by ASH for result aggregation Color Output Always <code>--no-color</code> ASH handles formatting"},{"location":"docs/plugins/community/ferret-scan-plugin/#ferret-scan-log-controls","title":"Ferret-Scan Log Controls","text":"Option Description <code>ferret_debug: true</code> Enables ferret-scan's own debug output (preprocessing/validation flow) <code>ferret_verbose: true</code> Enables ferret-scan's own verbose output (detailed finding info) <p>Note: Bare <code>debug</code> and <code>verbose</code> are blocked to avoid confusion with ASH's <code>--debug</code>/<code>--verbose</code> flags.</p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#managed-by-ash-framework","title":"Managed by ASH Framework","text":"Feature ASH Convention Notes Output Directory <code>.ash/ash_output/scanners/ferret-scan/</code> Follows ASH conventions Offline Mode <code>ASH_OFFLINE=true</code> Respects ASH offline mode Suppressions <code>.ash/suppressions.yaml</code> Managed centrally by ASH"},{"location":"docs/plugins/community/ferret-scan-plugin/#unsupported-options","title":"Unsupported Options","text":"<p>The following ferret-scan CLI options are NOT supported and will raise an error:</p> Option Reason <code>format</code>, <code>output_format</code> ASH requires SARIF format <code>debug</code>, <code>verbose</code> Use <code>ferret_debug</code>/<code>ferret_verbose</code> instead (avoids confusion with ASH flags) <code>web</code>, <code>port</code> Web server mode not applicable <code>enable_redaction</code>, <code>redaction_*</code>, <code>memory_scrub</code> Post-processing, not scanning <code>generate_suppressions</code>, <code>show_suppressed</code>, <code>suppressions_file</code> ASH manages suppressions <code>extract_text</code> Utility mode, not scanning"},{"location":"docs/plugins/community/ferret-scan-plugin/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/plugins/community/ferret-scan-plugin/#version-compatibility","title":"Version Compatibility","text":"<p>This plugin is tested and compatible with specific ferret-scan versions. Using versions outside the supported range may result in unexpected behavior.</p> <p>Supported Versions: 0.1.0 to 2.0.0 (exclusive)</p> Plugin Version ferret-scan Version Notes Current 0.1.0 - 2.0.0 (exclusive) Initial release with full feature support"},{"location":"docs/plugins/community/ferret-scan-plugin/#install-ferret-scan","title":"Install Ferret Scan","text":"<p>The plugin requires Ferret Scan to be installed and available in your system PATH.</p> <p>pip (Recommended): <pre><code>pip install ferret-scan\n</code></pre></p> <p>Build from Source: <pre><code>git clone https://github.com/awslabs/ferret-scan.git\ncd ferret-scan\nmake build\n</code></pre></p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#verify-installation","title":"Verify Installation","text":"<pre><code>ferret-scan --version\nferret-scan --help\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#quick-start","title":"Quick Start","text":""},{"location":"docs/plugins/community/ferret-scan-plugin/#enable-the-plugin","title":"Enable the Plugin","text":"<p>The ferret-scan plugin is a community plugin that must be explicitly enabled. There are two ways to do this:</p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#option-1-add-to-ash-configuration-file-recommended","title":"Option 1: Add to ASH Configuration File (Recommended)","text":"<p>Include the plugin module in your <code>.ash/.ash.yaml</code> configuration file:</p> <pre><code>ash_plugin_modules:\n  - automated_security_helper.plugin_modules.ash_ferret_plugins\n\nscanners:\n  ferret-scan:\n    enabled: true\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#option-2-command-line-flag","title":"Option 2: Command Line Flag","text":"<p>Use the <code>--ash-plugin-modules</code> flag when running ASH:</p> <pre><code>uv run ash scan --source-dir /path/to/code \\\n    --ash-plugin-modules automated_security_helper.plugin_modules.ash_ferret_plugins\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#verify-plugin-is-loaded","title":"Verify Plugin is Loaded","text":"<pre><code># With config file\nuv run ash plugin list | grep -i ferret\n\n# Or with command line flag\nuv run ash plugin list --ash-plugin-modules automated_security_helper.plugin_modules.ash_ferret_plugins | grep -i ferret\n</code></pre> <p>You should see <code>ferret-scan</code> in the list of scanners.</p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#basic-configuration","title":"Basic Configuration","text":"<p>Add scanner configuration to your <code>.ash/.ash.yaml</code>:</p> <pre><code>scanners:\n  ferret-scan:\n    enabled: true\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#run-ferret-scan","title":"Run Ferret Scan","text":"<pre><code># Scan current directory\nuv run ash --scanners ferret-scan\n\n# Scan specific directory\nuv run ash --source-dir /path/to/project --scanners ferret-scan\n\n# Run with ferret-scan's own debug output\nuv run ash --scanners ferret-scan -o ferret_debug=true\n\n# Run with ferret-scan's own verbose output\nuv run ash --scanners ferret-scan -o ferret_verbose=true\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#run-without-configuration-file","title":"Run Without Configuration File","text":"<p>If you want to run Ferret Scan without saving a configuration file:</p> <pre><code># Scan current directory only with ferret-scan\nuv run ash --scanners ferret-scan \\\n  --ash-plugin-modules automated_security_helper.plugin_modules.ash_ferret_plugins\n\n# Scan with all available scanners (including ferret-scan)\nuv run ash --ash-plugin-modules automated_security_helper.plugin_modules.ash_ferret_plugins\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#configuration-options","title":"Configuration Options","text":""},{"location":"docs/plugins/community/ferret-scan-plugin/#basic-options","title":"Basic Options","text":"<pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      confidence_levels: \"all\"    # high, medium, low, or combinations\n      checks: \"all\"               # Specific checks to run\n      recursive: true             # Recursively scan directories\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#configuration-options-reference","title":"Configuration Options Reference","text":"Option Type Default Description <code>confidence_levels</code> string <code>\"all\"</code> Confidence levels: <code>high</code>, <code>medium</code>, <code>low</code>, or combinations like <code>high,medium</code> <code>checks</code> string <code>\"all\"</code> Specific checks to run (comma-separated) <code>recursive</code> bool <code>true</code> Recursively scan directories <code>config_file</code> string <code>null</code> Path to custom Ferret YAML config file <code>use_default_config</code> bool <code>true</code> Use the default config bundled with this plugin <code>profile</code> string <code>null</code> Profile name from config file <code>exclude_patterns</code> list <code>[]</code> Glob patterns to exclude <code>show_match</code> bool <code>false</code> \u26a0\ufe0f Display matched text in findings (see security warning below) <code>enable_preprocessors</code> bool <code>true</code> Enable text extraction from documents (PDF, Office) <code>ferret_debug</code> bool <code>false</code> Enable ferret-scan's own debug logging (preprocessing/validation flow) <code>ferret_verbose</code> bool <code>false</code> Enable ferret-scan's own verbose output (detailed finding info) <code>tool_version</code> string <code>null</code> Version constraint for ferret-scan (e.g., <code>&gt;=1.0.0,&lt;2.0.0</code>, <code>==1.2.0</code>) <code>skip_version_check</code> bool <code>false</code> Skip version compatibility check (use with caution)"},{"location":"docs/plugins/community/ferret-scan-plugin/#options-not-supported-will-raise-error","title":"Options NOT Supported (Will Raise Error)","text":"<p>The following options are not supported because they conflict with ASH conventions:</p> Option Error Message <code>format</code>, <code>output_format</code> \"ASH requires SARIF format for result aggregation\" <code>debug</code> \"Use 'ferret_debug: true' instead. Bare 'debug' is blocked to avoid confusion with ASH's --debug flag.\" <code>verbose</code> \"Use 'ferret_verbose: true' instead. Bare 'verbose' is blocked to avoid confusion with ASH's --verbose flag.\" <code>web</code>, <code>port</code> \"Web server mode is not supported in ASH integration\" <code>enable_redaction</code>, <code>redaction_output_dir</code>, <code>redaction_strategy</code>, <code>redaction_audit_log</code>, <code>memory_scrub</code> \"Redaction is not supported in ASH integration\" <code>generate_suppressions</code>, <code>show_suppressed</code>, <code>suppressions_file</code> \"ASH manages suppressions centrally\" <code>extract_text</code> \"Text extraction mode is not supported\""},{"location":"docs/plugins/community/ferret-scan-plugin/#security-warning-show_match-option","title":"Security Warning: show_match Option","text":"<p>\u26a0\ufe0f Data Exfiltration Risk: The <code>show_match</code> option causes ferret-scan to include the actual matched sensitive data (credit card numbers, SSNs, API keys, etc.) in the scan output. This creates significant security risks:</p> <ul> <li>Log file exposure: Matched sensitive data will appear in SARIF reports, CI/CD logs, and ASH output files</li> <li>Accidental data leakage: Reports shared with team members or uploaded to security dashboards may contain real sensitive data</li> <li>Compliance violations: Logging actual PII/PCI data may violate data protection regulations (GDPR, PCI-DSS, HIPAA)</li> </ul> <p>Recommendation: Keep <code>show_match: false</code> (the default) in production environments. Only enable it temporarily for debugging in isolated, secure environments where log files are properly protected and purged.</p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#available-checks","title":"Available Checks","text":"<ul> <li><code>CREDIT_CARD</code> - Credit card numbers (Visa, MasterCard, Amex, etc.)</li> <li><code>EMAIL</code> - Email addresses</li> <li><code>INTELLECTUAL_PROPERTY</code> - Patents, trademarks, copyrights</li> <li><code>IP_ADDRESS</code> - IPv4 and IPv6 addresses</li> <li><code>METADATA</code> - Document and image metadata</li> <li><code>PASSPORT</code> - Passport numbers (multi-country)</li> <li><code>PERSON_NAME</code> - Person names</li> <li><code>PHONE</code> - Phone numbers</li> <li><code>SECRETS</code> - API keys, tokens, passwords</li> <li><code>SOCIAL_MEDIA</code> - Social media profiles</li> <li><code>SSN</code> - Social Security Numbers</li> </ul>"},{"location":"docs/plugins/community/ferret-scan-plugin/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      confidence_levels: \"high,medium\"\n      checks: \"CREDIT_CARD,SECRETS,SSN,PASSPORT\"\n      recursive: true\n      profile: \"security-audit\"\n      config_file: \"my-ferret-config.yaml\"\n      # show_match: false  # Keep disabled to avoid logging sensitive data\n      enable_preprocessors: true  # Extract text from PDFs, Office docs\n      exclude_patterns:\n        - \"*.log\"\n        - \"node_modules/**\"\n        - \"vendor/**\"\n        - \".git/**\"\n</code></pre> <p>Note: Do NOT configure bare <code>debug</code>, <code>verbose</code>, <code>format</code>, or suppression options at the plugin level. Use <code>ferret_debug</code>/<code>ferret_verbose</code> for ferret-scan's own log output. Other options are managed by ASH globally.</p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#version-pinning","title":"Version Pinning","text":"<p>Pin to a specific ferret-scan version for reproducible builds:</p> <pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      tool_version: \"==1.2.0\"  # Exact version\n</code></pre> <p>Or use a version range:</p> <pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      tool_version: \"&gt;=1.0.0,&lt;1.5.0\"  # Compatible range\n</code></pre> <p>To bypass version checks (not recommended for production):</p> <pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      skip_version_check: true  # Use with caution\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#default-configuration","title":"Default Configuration","text":"<p>This plugin includes a bundled <code>ferret-config.yaml</code> that is automatically used when no custom config is specified. The default config includes:</p> <ul> <li>Comprehensive validator patterns for intellectual property detection</li> <li>Social media platform detection patterns (LinkedIn, Twitter, Facebook, etc.)</li> <li>Pre-configured profiles for common use cases</li> <li>AWS/Amazon internal URL patterns for IP detection</li> </ul>"},{"location":"docs/plugins/community/ferret-scan-plugin/#available-profiles","title":"Available Profiles","text":"<p>The default configuration includes these pre-defined profiles:</p> Profile Description <code>quick</code> Fast security check (high confidence only) <code>thorough</code> All confidence levels with text extraction <code>ci</code> CI/CD integration (JUnit XML output) <code>security-audit</code> Security team scanning (JSON output) <code>comprehensive</code> Complete analysis (YAML output) <code>credit-card</code> PCI compliance focused <code>passport</code> Identity verification focused <code>intellectual-property</code> IP detection focused <code>json-api</code> Structured JSON for APIs <code>csv-export</code> CSV for spreadsheet analysis <code>silent</code> Minimal output for automation"},{"location":"docs/plugins/community/ferret-scan-plugin/#using-profiles","title":"Using Profiles","text":"<pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      profile: \"security-audit\"\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#overriding-default-configuration","title":"Overriding Default Configuration","text":""},{"location":"docs/plugins/community/ferret-scan-plugin/#option-1-custom-config-file-path","title":"Option 1: Custom Config File Path","text":"<pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      config_file: \"/path/to/your/custom-ferret.yaml\"\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#option-2-config-file-in-source-directory","title":"Option 2: Config File in Source Directory","text":"<p>The plugin searches for config files in this order: 1. Explicitly specified <code>config_file</code> path 2. <code>ferret.yaml</code> or <code>.ferret.yaml</code> in the source directory 3. <code>.ash/ferret.yaml</code> or <code>.ash/ferret-scan.yaml</code> in the source directory 4. Default config bundled with this plugin</p> <p>Simply create a <code>ferret.yaml</code> or <code>.ferret.yaml</code> in your project root to override the defaults.</p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#option-3-disable-default-config","title":"Option 3: Disable Default Config","text":"<pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      use_default_config: false  # Use ferret-scan's built-in defaults only\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#creating-a-custom-config-file","title":"Creating a Custom Config File","text":"<p>Copy the default config from the plugin:</p> <pre><code>cp $(python -c \"import automated_security_helper.plugin_modules.ash_ferret_plugins as p; print(p.__path__[0])\")/ferret-config.yaml ./my-ferret-config.yaml\n</code></pre> <p>Or create a minimal config that only overrides what you need:</p> <pre><code># my-ferret-config.yaml\ndefaults:\n  confidence_levels: high,medium\n  checks: SECRETS,CREDIT_CARD,SSN\n\nvalidators:\n  intellectual_property:\n    internal_urls:\n      - \"http[s]?:\\\\/\\\\/.*\\\\.mycompany\\\\.com\"\n      - \"http[s]?:\\\\/\\\\/internal\\\\..*\"\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#usage-examples","title":"Usage Examples","text":""},{"location":"docs/plugins/community/ferret-scan-plugin/#high-confidence-findings-only","title":"High Confidence Findings Only","text":"<pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      confidence_levels: \"high\"\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#pci-compliance-scan","title":"PCI Compliance Scan","text":"<pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      checks: \"CREDIT_CARD\"\n      confidence_levels: \"all\"\n      profile: \"credit-card\"\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#secrets-detection-only","title":"Secrets Detection Only","text":"<pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      checks: \"SECRETS\"\n      confidence_levels: \"high,medium\"\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#combined-with-other-scanners","title":"Combined with Other Scanners","text":"<pre><code># Use Ferret Scan alongside other ASH scanners\nuv run ash --scanners ferret-scan,bandit,detect-secrets\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># Run in container mode for CI/CD\nuv run ash --mode container --scanners ferret-scan\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#output-integration","title":"Output Integration","text":"<p>Ferret Scan results are integrated into ASH's unified reporting system:</p> <ul> <li>SARIF Format: Machine-readable results for CI/CD integration</li> <li>HTML Reports: Visual security dashboard with remediation guidance</li> <li>JSON/CSV: Structured data for analysis and tracking</li> <li>Markdown: Human-readable summaries for pull requests</li> </ul> <p>Results are written to: <pre><code>.ash/ash_output/\n\u251c\u2500\u2500 scanners/\n\u2502   \u2514\u2500\u2500 ferret-scan/\n\u2502       \u2514\u2500\u2500 source/\n\u2502           \u2514\u2500\u2500 ferret-scan.sarif\n\u2514\u2500\u2500 reports/\n    \u2514\u2500\u2500 ash.sarif  # Aggregated results\n</code></pre></p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/community/ferret-scan-plugin/#common-issues","title":"Common Issues","text":"<p>Ferret Scan not found: <pre><code># Check if ferret-scan is in PATH\nwhich ferret-scan\n\n# Install if missing\npip install ferret-scan\n</code></pre></p> <p>Unsupported option error: If you see an error like: <pre><code>ValueError: Unsupported option 'debug' in ferret-scan plugin configuration. \nUse 'ferret_debug: true' instead. Bare 'debug' is blocked to avoid confusion with ASH's --debug flag.\n</code></pre></p> <p>This means you used a bare option name that's blocked. Use the <code>ferret_</code> prefixed version instead: <pre><code>scanners:\n  ferret-scan:\n    enabled: true\n    options:\n      ferret_debug: true    # Correct\n      ferret_verbose: true  # Correct\n</code></pre></p> <p>Empty directory warnings: The plugin will skip scanning if the target directory is empty or doesn't exist, logging an appropriate warning message.</p> <p>No findings in output: <pre><code># Check if ferret-scan works directly\nferret-scan --format sarif --file /path/to/test/file\n</code></pre></p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#debug-mode","title":"Debug Mode","text":"<p>Enable ferret-scan's own debug/verbose output to troubleshoot issues:</p> <pre><code># Enable ferret-scan's debug output (shows preprocessing and validation flow)\nuv run ash --scanners ferret-scan -o ferret_debug=true\n\n# Enable ferret-scan's verbose output (shows detailed finding info)\nuv run ash --scanners ferret-scan -o ferret_verbose=true\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#integration-examples","title":"Integration Examples","text":""},{"location":"docs/plugins/community/ferret-scan-plugin/#pre-commit-hook","title":"Pre-commit Hook","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: ash-ferret-scan\n        name: ASH Ferret Scan Sensitive Data Detection\n        entry: uv run ash --scanners ferret-scan --mode precommit\n        language: system\n        pass_filenames: false\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#github-actions","title":"GitHub Actions","text":"<pre><code># .github/workflows/security.yml\nname: Security Scan\non: [push, pull_request]\n\njobs:\n  security:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - name: Install dependencies\n        run: |\n          pip install ferret-scan\n          pip install uv\n      - name: Run ASH with Ferret Scan\n        run: |\n          uv run ash --scanners ferret-scan --output-format sarif \\\n            --config-overrides \"ash_plugin_modules+=[\\\"automated_security_helper.plugin_modules.ash_ferret_plugins\\\"]\"\n      - name: Upload SARIF results\n        uses: github/codeql-action/upload-sarif@v2\n        with:\n          sarif_file: .ash/ash_output/reports/ash.sarif\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#gitlab-ci","title":"GitLab CI","text":"<pre><code># .gitlab-ci.yml\nferret-security-scan:\n  stage: test\n  image: python:3.11\n  before_script:\n    - pip install ferret-scan uv\n  script:\n    - uv run ash --scanners ferret-scan \\\n        --config-overrides \"ash_plugin_modules+=[\\\"automated_security_helper.plugin_modules.ash_ferret_plugins\\\"]\"\n  artifacts:\n    reports:\n      sast: .ash/ash_output/reports/ash.sarif\n</code></pre>"},{"location":"docs/plugins/community/ferret-scan-plugin/#comparison-with-detect-secrets","title":"Comparison with detect-secrets","text":"Feature Ferret Scan detect-secrets Credit Cards \u2705 15+ brands with Luhn \u274c Passports \u2705 Multi-country + MRZ \u274c SSN \u2705 Domain-aware \u274c API Keys \u2705 40+ patterns \u2705 Multiple patterns High Entropy \u274c \u2705 Social Media \u2705 \u274c IP Detection \u2705 \u274c Document Metadata \u2705 EXIF extraction \u274c <p>Consider using both scanners together for comprehensive coverage: <pre><code>uv run ash --scanners ferret-scan,detect-secrets\n</code></pre></p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#documentation","title":"Documentation","text":"<p>For comprehensive documentation and advanced configuration options, see: - Ferret Scan GitHub Repository - ASH Plugin Development Guide - ASH Scanner Plugins</p>"},{"location":"docs/plugins/community/ferret-scan-plugin/#support","title":"Support","text":"<ul> <li>ASH Issues: GitHub Issues</li> <li>Ferret Scan Issues: GitHub Issues</li> <li>Community: ASH Discussions</li> </ul>"},{"location":"docs/plugins/community/snyk-plugin-setup/","title":"Snyk Plugin CI/CD Setup Guide","text":"<p>This guide provides detailed instructions for setting up the Snyk plugin in CI/CD environments, specifically for the GitHub Actions workflow in the automated-security-helper repository.</p>"},{"location":"docs/plugins/community/snyk-plugin-setup/#github-secret-configuration","title":"GitHub Secret Configuration","text":"<p>The Snyk plugin requires a <code>SNYK_TOKEN</code> to authenticate with Snyk's services. This token must be configured as a GitHub repository secret.</p>"},{"location":"docs/plugins/community/snyk-plugin-setup/#setting-up-snyk_token-secret","title":"Setting up SNYK_TOKEN Secret","text":"<ol> <li>Obtain a Snyk API Token:</li> <li>Log in to your Snyk account</li> <li>Navigate to Account Settings \u2192 API Token</li> <li>Generate a new token or copy your existing one</li> <li> <p>The token format will look like: <code>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</code></p> </li> <li> <p>Add Secret to GitHub Repository:</p> </li> <li>Navigate to your GitHub repository</li> <li>Go to Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret</li> <li>Name: <code>SNYK_TOKEN</code></li> <li>Value: Your Snyk API token from step 1</li> <li>Click Add secret</li> </ol>"},{"location":"docs/plugins/community/snyk-plugin-setup/#verification","title":"Verification","text":"<p>The GitHub Actions workflow will automatically use the <code>SNYK_TOKEN</code> secret when running community plugin tests. You can verify the setup by:</p> <ol> <li>Checking that the secret appears in your repository's Actions secrets list</li> <li>Running the workflow and ensuring Snyk CLI authentication succeeds</li> <li>Reviewing the workflow logs for successful Snyk plugin execution</li> </ol>"},{"location":"docs/plugins/community/snyk-plugin-setup/#local-development-setup","title":"Local Development Setup","text":"<p>For local development and testing, you can set up Snyk authentication using one of these methods:</p>"},{"location":"docs/plugins/community/snyk-plugin-setup/#option-1-environment-variable","title":"Option 1: Environment Variable","text":"<pre><code>export SNYK_TOKEN=your-snyk-token-here\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin-setup/#option-2-cli-authentication","title":"Option 2: CLI Authentication","text":"<p><pre><code>snyk auth\n</code></pre> This will open a browser window for authentication and store credentials locally.</p>"},{"location":"docs/plugins/community/snyk-plugin-setup/#option-3-configuration-file","title":"Option 3: Configuration File","text":"<p>Snyk CLI will automatically check for credentials at <code>~/.config/configstore/snyk.json</code>.</p>"},{"location":"docs/plugins/community/snyk-plugin-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/community/snyk-plugin-setup/#common-issues","title":"Common Issues","text":"<p>Authentication Failed: - Verify the <code>SNYK_TOKEN</code> secret is correctly set in GitHub - Ensure the token is valid and hasn't expired - Check that the token has appropriate permissions for Snyk Code</p> <p>Workflow Fails on Community Plugin Tests: - The workflow will skip Snyk tests gracefully if <code>SNYK_TOKEN</code> is not available - Check the workflow logs for specific error messages - Ensure npm is available for Snyk CLI installation</p> <p>Rate Limiting: - Snyk may apply rate limits based on your subscription tier - Consider upgrading your Snyk plan for higher rate limits - Implement retry logic if needed</p>"},{"location":"docs/plugins/community/snyk-plugin-setup/#debug-mode","title":"Debug Mode","text":"<p>To troubleshoot issues in the workflow, you can:</p> <ol> <li>Check the \"Install Community Plugin Tools\" step logs</li> <li>Look for Snyk CLI version and authentication status</li> <li>Review the scan execution logs for detailed error messages</li> </ol>"},{"location":"docs/plugins/community/snyk-plugin-setup/#security-considerations","title":"Security Considerations","text":"<ul> <li>Token Security: Never commit Snyk tokens to version control</li> <li>Token Rotation: Regularly rotate your Snyk API tokens</li> <li>Access Control: Limit repository access to trusted contributors</li> <li>Monitoring: Monitor token usage in your Snyk dashboard</li> </ul>"},{"location":"docs/plugins/community/snyk-plugin-setup/#integration-testing","title":"Integration Testing","text":"<p>The GitHub Actions workflow tests both Snyk and Trivy plugins together to ensure:</p> <ul> <li>Both tools install correctly on all supported platforms</li> <li>Plugins don't conflict with each other</li> <li>Scan results are properly generated and formatted</li> <li>All output formats (SARIF, JSON, HTML, etc.) work correctly</li> </ul>"},{"location":"docs/plugins/community/snyk-plugin-setup/#platform-support","title":"Platform Support","text":"<p>The Snyk plugin is tested on:</p> <ul> <li>Linux: Ubuntu (x86_64 and ARM64)</li> <li>macOS: Latest version (x86_64)</li> <li>Windows: Latest version (x86_64)</li> </ul> <p>All platforms use npm for cross-platform Snyk CLI installation, ensuring consistent behavior across environments.</p>"},{"location":"docs/plugins/community/snyk-plugin/","title":"ASH Snyk Code Plugin","text":"<p>This plugin integrates Snyk Code CLI tool with the Automated Security Helper (ASH) to provide comprehensive static application security testing (SAST) for source code vulnerabilities.</p>"},{"location":"docs/plugins/community/snyk-plugin/#overview","title":"Overview","text":"<p>The Snyk Code plugin enables ASH to leverage Snyk's powerful static analysis capabilities for:</p> <ul> <li>Static Application Security Testing (SAST): Identifies security vulnerabilities in source code</li> <li>Real-time Analysis: Fast scanning with minimal false positives</li> <li>Multi-language Support: Supports JavaScript, TypeScript, Python, Java, C#, PHP, Go, Ruby, Scala, Swift, and mor. See a full list here</li> <li>Developer-friendly Results: Provides actionable remediation guidance with code examples</li> </ul>"},{"location":"docs/plugins/community/snyk-plugin/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/plugins/community/snyk-plugin/#install-snyk-cli","title":"Install Snyk CLI","text":"<p>The plugin requires Snyk CLI to be installed and available in your system PATH.</p> <p>npm (Recommended): <pre><code>npm install -g snyk\n</code></pre></p> <p>Homebrew (macOS): <pre><code>brew install snyk/tap/snyk\n</code></pre></p> <p>Manual Installation: <pre><code># Download and install latest release\ncurl -Lo snyk https://github.com/snyk/cli/releases/latest/download/snyk-linux\nchmod +x snyk\nsudo mv snyk /usr/local/bin/\n</code></pre></p> <p>Other platforms: See Snyk CLI Installation Guide</p>"},{"location":"docs/plugins/community/snyk-plugin/#authentication","title":"Authentication","text":"<p>Snyk Code requires authentication to access the scanning service:</p> <p>Option 1: Environment Variable <pre><code>export SNYK_TOKEN=your-snyk-token\n</code></pre></p> <p>Option 2: CLI Authentication <pre><code>snyk auth\n</code></pre></p> <p>Option 3: Configuration File The plugin will automatically check for credentials at <code>~/.config/configstore/snyk.json</code></p>"},{"location":"docs/plugins/community/snyk-plugin/#verify-installation","title":"Verify Installation","text":"<pre><code>snyk --version\nsnyk code test --help\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#quick-start","title":"Quick Start","text":""},{"location":"docs/plugins/community/snyk-plugin/#basic-configuration","title":"Basic Configuration","text":"<p>Snyk code plugin is not included by default with ASH since it requires authentication. Include Snyk plugin module in your <code>.ash/.ash.yaml</code> configuration file</p> <pre><code>ash_plugin_modules:\n  - automated_security_helper.plugin_modules.ash_snyk_plugins\n</code></pre> <p>Add to your <code>.ash/.ash.yaml</code>:</p> <pre><code>scanners:\n  snyk-code:\n    enabled: true\n    severity_threshold: \"MEDIUM\"\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#run-snyk-code-scan","title":"Run Snyk Code Scan","text":"<pre><code># Scan current directory\nuv run ash --scanners snyk-code\n\n# Scan specific directory\nuv run ash --scanners snyk-code /path/to/project\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#run-snyk-code-scan-without-an-ash-configuration-file","title":"Run Snyk Code Scan without an ASH configuration file","text":"<p>If you want to run Snyk code scan without saving a configuration file for ASH, use the following command to enable the plugin</p> <pre><code># Scan current directory only with snyk-code\nuv run ash --scanners snyk-code --config-overrides \"ash_plugin_modules+=[\\\"automated_security_helper.plugin_modules.ash_snyk_plugins\\\"]\"\n\n# SCan current directory with all available scanners (including snyk-code)\nuv run ash  --config-overrides \"ash_plugin_modules+=[\\\"automated_security_helper.plugin_modules.ash_snyk_plugins\\\"]\"\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#configuration-options","title":"Configuration Options","text":""},{"location":"docs/plugins/community/snyk-plugin/#severity-filtering","title":"Severity Filtering","text":"<p>Configure the minimum severity level for reported vulnerabilities:</p> <pre><code>scanners:\n  snyk-code:\n    enabled: true\n    severity_threshold: \"HIGH\"  # Options: LOW, MEDIUM, HIGH, CRITICAL\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#advanced-options","title":"Advanced Options","text":"<pre><code>scanners:\n  snyk-code:\n    enabled: true\n    severity_threshold: \"MEDIUM\"\n    options:\n      # Additional scanner-specific options can be added here\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#usage-examples","title":"Usage Examples","text":""},{"location":"docs/plugins/community/snyk-plugin/#high-severity-issues-only","title":"High Severity Issues Only","text":"<pre><code>scanners:\n  snyk-code:\n    enabled: true\n    severity_threshold: \"HIGH\"\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#all-severity-levels","title":"All Severity Levels","text":"<pre><code>scanners:\n  snyk-code:\n    enabled: true\n    severity_threshold: \"LOW\"\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#combined-with-other-scanners","title":"Combined with Other Scanners","text":"<pre><code># Use Snyk Code alongside other ASH scanners\nuv run ash --scanners snyk-code,bandit,semgrep\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># Run in container mode for CI/CD\nuv run ash --mode container --scanners snyk-code\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#output-integration","title":"Output Integration","text":"<p>Snyk Code results are integrated into ASH's unified reporting system:</p> <ul> <li>SARIF Format: Machine-readable results for CI/CD integration</li> <li>HTML Reports: Visual security dashboard with remediation guidance</li> <li>JSON/CSV: Structured data for analysis and tracking</li> <li>Markdown: Human-readable summaries for pull requests</li> </ul>"},{"location":"docs/plugins/community/snyk-plugin/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>First Run: May require initial authentication and setup</li> <li>Network Dependency: Requires internet connection for cloud-based analysis</li> <li>Large Codebases: Scanning time scales with codebase size</li> <li>Rate Limits: Snyk may apply rate limits based on your subscription tier</li> </ul>"},{"location":"docs/plugins/community/snyk-plugin/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/community/snyk-plugin/#common-issues","title":"Common Issues","text":"<p>Snyk CLI not found: <pre><code># Check if Snyk is in PATH\nwhich snyk\n\n# Install if missing\nnpm install -g snyk\n</code></pre></p> <p>Authentication errors: <pre><code># Check authentication status\nsnyk auth\n\n# Set token via environment variable\nexport SNYK_TOKEN=your-token-here\n</code></pre></p> <p>Network connectivity issues: <pre><code># Test connectivity to Snyk services\nsnyk test --dry-run\n</code></pre></p> <p>Empty directory warnings: The plugin will skip scanning if the target directory is empty or doesn't exist, logging an appropriate warning message.</p>"},{"location":"docs/plugins/community/snyk-plugin/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose logging to troubleshoot issues:</p> <pre><code>uv run ash --scanners snyk-code --log-level DEBUG\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#offline-mode","title":"Offline Mode","text":"<p>Note: Snyk Code requires internet connectivity and will be automatically disabled in offline mode:</p> <pre><code># Snyk Code will be skipped in offline mode\nuv run ash --offline --scanners snyk-code\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#integration-examples","title":"Integration Examples","text":""},{"location":"docs/plugins/community/snyk-plugin/#pre-commit-hook","title":"Pre-commit Hook","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: ash-snyk-code\n        name: ASH Snyk Code Security Scan\n        entry: uv run ash --scanners snyk-code --mode precommit\n        language: system\n        pass_filenames: false\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#github-actions","title":"GitHub Actions","text":"<pre><code># .github/workflows/security.yml\nname: Security Scan\non: [push, pull_request]\n\njobs:\n  security:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Install Snyk CLI\n        run: npm install -g snyk\n      - name: Run ASH with Snyk Code\n        env:\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n        run: |\n          uv run ash --scanners snyk-code --output-format sarif\n      - name: Upload SARIF results\n        uses: github/codeql-action/upload-sarif@v2\n        with:\n          sarif_file: .ash/ash_output/reports/ash.sarif\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#gitlab-ci","title":"GitLab CI","text":"<pre><code># .gitlab-ci.yml\nsnyk-security-scan:\n  stage: test\n  image: node:18\n  before_script:\n    - npm install -g snyk\n  script:\n    - uv run ash --scanners snyk-code\n  variables:\n    SNYK_TOKEN: $SNYK_TOKEN\n  artifacts:\n    reports:\n      sast: .ash/ash_output/reports/ash.sarif\n</code></pre>"},{"location":"docs/plugins/community/snyk-plugin/#supported-languages","title":"Supported Languages","text":"<p>Snyk Code supports static analysis for:</p> <ul> <li>JavaScript/TypeScript: Node.js, React, Angular, Vue.js</li> <li>Python: Django, Flask, FastAPI</li> <li>Java: Spring, Maven, Gradle projects</li> <li>C#/.NET: .NET Framework, .NET Core</li> <li>PHP: Laravel, Symfony, WordPress</li> <li>Go: Standard library and popular frameworks</li> <li>Ruby: Rails, Sinatra</li> <li>Scala: Play Framework, Akka</li> <li>Swift: iOS/macOS applications</li> </ul> <p>Full list of supported languages is available at the Snyk Website</p>"},{"location":"docs/plugins/community/snyk-plugin/#documentation","title":"Documentation","text":"<p>For comprehensive documentation and advanced configuration options, see: - ASH Community Plugins Documentation - Snyk CLI Documentation - Snyk Code Documentation</p>"},{"location":"docs/plugins/community/snyk-plugin/#support","title":"Support","text":"<ul> <li>ASH Issues: GitHub Issues</li> <li>Snyk Issues: Snyk Support</li> <li>Community: ASH Discussions</li> </ul>"},{"location":"docs/plugins/community/trivy-plugin/","title":"Trivy Plugin","text":"<p>Description: The Trivy plugin integrates Aquasec's Trivy CLI tool to provide comprehensive repository scanning for vulnerabilities, misconfigurations, secrets, and license issues. This plugin extends ASH's security scanning capabilities with Trivy's advanced detection algorithms and extensive vulnerability database.</p> <p>Repository: Built-in ASH plugin (part of core distribution)</p> <p>Author: ASH Development Team</p> <p>License: Apache 2.0</p>"},{"location":"docs/plugins/community/trivy-plugin/#overview","title":"Overview","text":"<p>Trivy is a comprehensive security scanner that can detect: - Vulnerabilities in OS packages and language-specific packages - Misconfigurations in Infrastructure as Code (IaC) files - Secrets and sensitive information in code - License compliance issues</p> <p>The ASH Trivy plugin provides seamless integration with Trivy's repository scanning capabilities, allowing you to incorporate Trivy scans into your ASH security workflows with unified reporting and configuration management.</p>"},{"location":"docs/plugins/community/trivy-plugin/#prerequisites","title":"Prerequisites","text":""},{"location":"docs/plugins/community/trivy-plugin/#trivy-cli-installation","title":"Trivy CLI Installation","text":"<p>The Trivy plugin requires the Trivy CLI to be installed and available in your system PATH.</p>"},{"location":"docs/plugins/community/trivy-plugin/#installation-options","title":"Installation Options","text":"<p>Using Homebrew (macOS/Linux): <pre><code>brew install trivy\n</code></pre></p> <p>Using Package Managers: <pre><code># Ubuntu/Debian\nsudo apt-get install wget apt-transport-https gnupg lsb-release\nwget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -\necho \"deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main\" | sudo tee -a /etc/apt/sources.list.d/trivy.list\nsudo apt-get update\nsudo apt-get install trivy\n\n# RHEL/CentOS\nsudo vim /etc/yum.repos.d/trivy.repo\n# Add repository configuration\nsudo yum -y update\nsudo yum -y install trivy\n</code></pre></p> <p>Using Binary Releases:</p> <p>Linux/macOS: <pre><code># Download and install from GitHub releases\ncurl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin\n</code></pre></p> <p>Windows: <pre><code># Download and install using the official endpoint\ncurl -sfL -o trivy.zip \"https://get.trivy.dev/trivy?type=zip&amp;os=windows&amp;arch=amd64\"\nunzip trivy.zip\n# Move trivy.exe to a directory in your PATH\n</code></pre></p>"},{"location":"docs/plugins/community/trivy-plugin/#verification","title":"Verification","text":"<p>Verify Trivy installation: <pre><code>trivy version\n</code></pre></p>"},{"location":"docs/plugins/community/trivy-plugin/#configuration","title":"Configuration","text":""},{"location":"docs/plugins/community/trivy-plugin/#basic-configuration","title":"Basic Configuration","text":"<p>Add the Trivy plugin to your <code>.ash/.ash.yaml</code> configuration:</p> <pre><code>ash_plugin_modules:\n  - automated_security_helper.plugin_modules.ash_trivy_plugins\n</code></pre> <p>Configure the scanner based on your requirements:</p> <pre><code>scanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"vuln\", \"misconfig\", \"secret\", \"license\"]\n      severity_threshold: \"MEDIUM\"\n      ignore_unfixed: false\n      license_full: false\n      disable_telemetry: true\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#configuration-options","title":"Configuration Options","text":"Option Type Default Description <code>scanners</code> List[str] <code>[\"vuln\", \"misconfig\", \"secret\", \"license\"]</code> Types of scans to perform <code>severity_threshold</code> str <code>\"MEDIUM\"</code> Minimum severity level to report (UNKNOWN, LOW, MEDIUM, HIGH, CRITICAL) <code>ignore_unfixed</code> bool <code>false</code> Ignore vulnerabilities without available fixes <code>license_full</code> bool <code>false</code> Enable full license scanning (more comprehensive but slower) <code>disable_telemetry</code> bool <code>true</code> Disable Trivy telemetry data collection"},{"location":"docs/plugins/community/trivy-plugin/#scanner-types","title":"Scanner Types","text":"<p>The <code>scanners</code> option accepts the following values:</p> <ul> <li><code>vuln</code>: Vulnerability scanning for OS and language packages</li> <li><code>misconfig</code>: Infrastructure as Code misconfiguration detection</li> <li><code>secret</code>: Secret and sensitive information detection</li> <li><code>license</code>: License compliance scanning</li> </ul>"},{"location":"docs/plugins/community/trivy-plugin/#severity-levels","title":"Severity Levels","text":"<p>Trivy supports the following severity levels (from lowest to highest): - <code>UNKNOWN</code>: Unknown severity - <code>LOW</code>: Low severity issues - <code>MEDIUM</code>: Medium severity issues - <code>HIGH</code>: High severity issues - <code>CRITICAL</code>: Critical severity issues</p>"},{"location":"docs/plugins/community/trivy-plugin/#usage-examples","title":"Usage Examples","text":""},{"location":"docs/plugins/community/trivy-plugin/#basic-repository-scan","title":"Basic Repository Scan","text":"<pre><code># Scan current directory with default Trivy settings\nash --scanners trivy-repo\n\n# Scan specific directory\nash --target /path/to/project --scanners trivy-repo\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#vulnerability-only-scan","title":"Vulnerability-Only Scan","text":"<pre><code># .ash/.ash.yaml\nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"vuln\"]\n      severity_threshold: \"HIGH\"\n      ignore_unfixed: true\n</code></pre> <pre><code>ash --scanners trivy-repo\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#comprehensive-security-scan","title":"Comprehensive Security Scan","text":"<pre><code># .ash/.ash.yaml\nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"vuln\", \"misconfig\", \"secret\", \"license\"]\n      severity_threshold: \"LOW\"\n      license_full: true\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#integration-with-other-scanners","title":"Integration with Other Scanners","text":"<pre><code># .ash/.ash.yaml\nscanners:\n  # Combine Trivy with other ASH scanners\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"vuln\", \"misconfig\"]\n      severity_threshold: \"MEDIUM\"\n\n  bandit:\n    enabled: true\n    options:\n      severity_threshold: \"MEDIUM\"\n\n  semgrep:\n    enabled: true\n    options:\n      severity_threshold: \"MEDIUM\"\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#output-and-reporting","title":"Output and Reporting","text":"<p>The Trivy plugin integrates with ASH's unified reporting system, producing results in SARIF format that can be converted to various output formats.</p>"},{"location":"docs/plugins/community/trivy-plugin/#available-output-formats","title":"Available Output Formats","text":"<ul> <li>SARIF: Machine-readable results for CI/CD integration</li> <li>HTML: Human-readable reports with detailed findings</li> <li>Markdown: Documentation-friendly format</li> <li>CSV: Spreadsheet-compatible format</li> <li>JSON: Structured data for custom processing</li> </ul>"},{"location":"docs/plugins/community/trivy-plugin/#example-output-configuration","title":"Example Output Configuration","text":"<pre><code># .ash/.ash.yaml\nreporters:\n  sarif:\n    enabled: true\n    options:\n      output_file: \"trivy-results.sarif\"\n\n  html:\n    enabled: true\n    options:\n      output_file: \"security-report.html\"\n\n  markdown:\n    enabled: true\n    options:\n      output_file: \"SECURITY.md\"\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#performance-considerations","title":"Performance Considerations","text":""},{"location":"docs/plugins/community/trivy-plugin/#resource-requirements","title":"Resource Requirements","text":""},{"location":"docs/plugins/community/trivy-plugin/#memory-usage","title":"Memory Usage","text":"Repository Size Scan Types Expected Memory Peak Memory Small (&lt;100MB) All 150-300MB 500MB Medium (100MB-1GB) All 300-800MB 1.2GB Large (1-5GB) All 800MB-2GB 3GB Very Large (&gt;5GB) Selective 1-3GB 5GB+"},{"location":"docs/plugins/community/trivy-plugin/#disk-space-requirements","title":"Disk Space Requirements","text":"<ul> <li>Vulnerability Database: ~200MB (updated automatically)</li> <li>Cache Directory: 50-500MB depending on scan history</li> <li>Temporary Files: 10-100MB during scan execution</li> <li>Results Storage: 1-50MB per scan depending on findings</li> </ul>"},{"location":"docs/plugins/community/trivy-plugin/#network-requirements","title":"Network Requirements","text":"<ul> <li>Initial Setup: 200MB download for vulnerability database</li> <li>Regular Updates: 10-50MB daily for database updates</li> <li>Bandwidth: Minimal during scanning (only for updates)</li> <li>Offline Support: Full offline scanning supported after initial setup</li> </ul>"},{"location":"docs/plugins/community/trivy-plugin/#cpu-performance","title":"CPU Performance","text":"Repository Type Scan Duration CPU Usage Python project (1000 files) 30-60 seconds 1-2 cores Node.js project (5000 files) 1-3 minutes 2-4 cores Multi-language (10000+ files) 3-10 minutes 4+ cores Monorepo (50000+ files) 10-30 minutes 8+ cores"},{"location":"docs/plugins/community/trivy-plugin/#performance-optimization-strategies","title":"Performance Optimization Strategies","text":""},{"location":"docs/plugins/community/trivy-plugin/#1-scan-type-optimization","title":"1. Scan Type Optimization","text":"<pre><code># Fast vulnerability-only scan\nscanners:\n  trivy-repo:\n    options:\n      scanners: [\"vuln\"]\n      severity_threshold: \"HIGH\"\n      ignore_unfixed: true\n\n# Comprehensive but slower scan\nscanners:\n  trivy-repo:\n    options:\n      scanners: [\"vuln\", \"misconfig\", \"secret\", \"license\"]\n      severity_threshold: \"LOW\"\n      license_full: true\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#2-directory-based-optimization","title":"2. Directory-Based Optimization","text":"<pre><code># Scan critical paths first\nash --target src/ --scanners trivy-repo  # Core application code\nash --target config/ --scanners trivy-repo  # Configuration files\n\n# Skip non-critical directories\n</code></pre> <pre><code>global_settings:\n  ignore_paths:\n    - path: \"node_modules/**\"\n      reason: \"Package manager cache\"\n    - path: \"vendor/**\"\n      reason: \"Third-party dependencies\"\n    - path: \"**/*.min.*\"\n      reason: \"Minified assets\"\n    - path: \"build/**\"\n      reason: \"Build artifacts\"\n    - path: \"dist/**\"\n      reason: \"Distribution files\"\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#3-caching-optimization","title":"3. Caching Optimization","text":"<pre><code># Optimize cache location for performance\nexport TRIVY_CACHE_DIR=/fast-ssd/trivy-cache\n\n# Pre-warm cache for CI/CD\ntrivy image --download-db-only --cache-dir /shared/trivy-cache\n\n# Use shared cache in team environments\nexport TRIVY_CACHE_DIR=/shared/trivy-cache\nchmod -R 755 /shared/trivy-cache\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#4-parallel-processing","title":"4. Parallel Processing","text":"<pre><code>#!/bin/bash\n# parallel-scan.sh\n\n# Define scan targets\nTARGETS=(\"src/\" \"lib/\" \"config/\" \"scripts/\")\n\n# Run parallel scans\nfor target in \"${TARGETS[@]}\"; do\n  ash --target \"$target\" --scanners trivy-repo --config-file \".ash/${target%/}.yaml\" &amp;\ndone\n\n# Wait for all scans to complete\nwait\n\n# Merge results\npython scripts/merge_scan_results.py\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#5-incremental-scanning","title":"5. Incremental Scanning","text":"<pre><code>#!/bin/bash\n# incremental-scan.sh\n\n# Only scan changed files since last scan\nif [ -f .last_trivy_scan ]; then\n  CHANGED_FILES=$(find . -newer .last_trivy_scan -name \"*.py\" -o -name \"*.js\" -o -name \"*.yaml\")\n\n  if [ -n \"$CHANGED_FILES\" ]; then\n    echo \"Scanning changed files: $CHANGED_FILES\"\n    echo \"$CHANGED_FILES\" | xargs -I {} dirname {} | sort -u | while read dir; do\n      ash --target \"$dir\" --scanners trivy-repo\n    done\n  else\n    echo \"No changes detected, skipping scan\"\n  fi\nelse\n  echo \"First scan, scanning entire repository\"\n  ash --scanners trivy-repo\nfi\n\ntouch .last_trivy_scan\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#benchmarking-and-monitoring","title":"Benchmarking and Monitoring","text":""},{"location":"docs/plugins/community/trivy-plugin/#performance-measurement","title":"Performance Measurement","text":"<pre><code>#!/bin/bash\n# benchmark-trivy.sh\n\necho \"Starting Trivy performance benchmark...\"\n\n# Measure scan time\nstart_time=$(date +%s)\nash --scanners trivy-repo --reporters json\nend_time=$(date +%s)\n\nscan_duration=$((end_time - start_time))\necho \"Scan completed in ${scan_duration} seconds\"\n\n# Measure resource usage\necho \"Memory usage during scan:\"\nps aux | grep trivy | awk '{print $6}' | sort -n | tail -1\n\n# Measure result size\nresult_size=$(du -h .ash/ash_output/reports/ash_aggregated_results.json | cut -f1)\necho \"Results file size: $result_size\"\n\n# Count findings\nfinding_count=$(jq '[.runs[].results[]] | length' .ash/ash_output/reports/ash_aggregated_results.json)\necho \"Total findings: $finding_count\"\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#continuous-performance-monitoring","title":"Continuous Performance Monitoring","text":"<pre><code># performance_monitor.py\nimport json\nimport time\nimport psutil\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\n\nclass TrivyPerformanceMonitor:\n    def __init__(self):\n        self.metrics = []\n\n    def run_monitored_scan(self):\n        start_time = time.time()\n        start_memory = psutil.virtual_memory().used\n\n        # Run ASH with Trivy\n        process = subprocess.Popen(\n            [\"ash\", \"--scanners\", \"trivy-repo\", \"--reporters\", \"json\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n\n        # Monitor resource usage\n        max_memory = start_memory\n        while process.poll() is None:\n            current_memory = psutil.virtual_memory().used\n            max_memory = max(max_memory, current_memory)\n            time.sleep(1)\n\n        end_time = time.time()\n\n        # Collect metrics\n        duration = end_time - start_time\n        memory_used = max_memory - start_memory\n\n        # Count results\n        results_file = Path(\".ash/ash_output/reports/ash_aggregated_results.json\")\n        finding_count = 0\n        if results_file.exists():\n            with open(results_file) as f:\n                data = json.load(f)\n                finding_count = sum(\n                    len(run.get(\"results\", []))\n                    for run in data.get(\"runs\", [])\n                )\n\n        # Store metrics\n        metric = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"duration_seconds\": duration,\n            \"memory_mb\": memory_used / (1024 * 1024),\n            \"finding_count\": finding_count,\n            \"exit_code\": process.returncode\n        }\n\n        self.metrics.append(metric)\n        return metric\n\n    def save_metrics(self, filename=\"trivy_performance.json\"):\n        with open(filename, \"w\") as f:\n            json.dump(self.metrics, f, indent=2)\n\nif __name__ == \"__main__\":\n    monitor = TrivyPerformanceMonitor()\n    metric = monitor.run_monitored_scan()\n    monitor.save_metrics()\n\n    print(f\"Scan completed in {metric['duration_seconds']:.1f}s\")\n    print(f\"Memory used: {metric['memory_mb']:.1f}MB\")\n    print(f\"Findings: {metric['finding_count']}\")\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"docs/plugins/community/trivy-plugin/#horizontal-scaling","title":"Horizontal Scaling","text":"<pre><code>#!/bin/bash\n# distributed-scan.sh\n\n# Split repository by language/framework\nfind . -name \"*.py\" | head -1000 | xargs dirname | sort -u &gt; python_dirs.txt\nfind . -name \"*.js\" | head -1000 | xargs dirname | sort -u &gt; javascript_dirs.txt\nfind . -name \"*.yaml\" -o -name \"*.yml\" | head -1000 | xargs dirname | sort -u &gt; yaml_dirs.txt\n\n# Distribute across multiple workers\ncat python_dirs.txt | xargs -P 4 -I {} ash --target {} --scanners trivy-repo &amp;\ncat javascript_dirs.txt | xargs -P 4 -I {} ash --target {} --scanners trivy-repo &amp;\ncat yaml_dirs.txt | xargs -P 4 -I {} ash --target {} --scanners trivy-repo &amp;\n\nwait\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#container-based-scaling","title":"Container-Based Scaling","text":"<pre><code># docker-compose.yml for distributed scanning\nversion: '3.8'\nservices:\n  trivy-scanner-1:\n    image: ash:latest\n    volumes:\n      - ./src:/workspace/src\n      - ./trivy-cache:/root/.cache/trivy\n    command: ash --target /workspace/src --scanners trivy-repo\n\n  trivy-scanner-2:\n    image: ash:latest\n    volumes:\n      - ./lib:/workspace/lib\n      - ./trivy-cache:/root/.cache/trivy\n    command: ash --target /workspace/lib --scanners trivy-repo\n\n  trivy-scanner-3:\n    image: ash:latest\n    volumes:\n      - ./config:/workspace/config\n      - ./trivy-cache:/root/.cache/trivy\n    command: ash --target /workspace/config --scanners trivy-repo\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#resource-limits-and-constraints","title":"Resource Limits and Constraints","text":""},{"location":"docs/plugins/community/trivy-plugin/#memory-limits","title":"Memory Limits","text":"<pre><code># Set memory limits for container mode\nash --mode container --memory 2g --scanners trivy-repo\n\n# Monitor memory usage\nulimit -v 2097152  # 2GB virtual memory limit\nash --scanners trivy-repo\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#time-limits","title":"Time Limits","text":"<pre><code># Set timeout for scans\ntimeout 600 ash --scanners trivy-repo  # 10-minute timeout\n\n# Use with retry logic\nfor i in {1..3}; do\n  if timeout 600 ash --scanners trivy-repo; then\n    break\n  else\n    echo \"Scan attempt $i failed, retrying...\"\n    sleep 30\n  fi\ndone\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#disk-space-management","title":"Disk Space Management","text":"<pre><code># Clean up old results\nfind .ash/ash_output -name \"*.sarif\" -mtime +7 -delete\n\n# Rotate Trivy cache\nif [ $(du -s ~/.cache/trivy | cut -f1) -gt 1048576 ]; then  # 1GB\n  rm -rf ~/.cache/trivy/db\n  trivy image --download-db-only\nfi\n\n# Compress old results\nfind .ash/ash_output -name \"*.json\" -mtime +1 -exec gzip {} \\;\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#scanning-scenarios-and-use-cases","title":"Scanning Scenarios and Use Cases","text":""},{"location":"docs/plugins/community/trivy-plugin/#1-development-workflow-integration","title":"1. Development Workflow Integration","text":""},{"location":"docs/plugins/community/trivy-plugin/#pre-commit-scanning","title":"Pre-commit Scanning","text":"<pre><code>#!/bin/bash\n# .git/hooks/pre-commit\n\n# Quick scan of staged files only\nSTAGED_FILES=$(git diff --cached --name-only)\n\nif [ -n \"$STAGED_FILES\" ]; then\n  echo \"Running security scan on staged files...\"\n\n  # Create temporary directory with staged files\n  TEMP_DIR=$(mktemp -d)\n  echo \"$STAGED_FILES\" | while read file; do\n    if [ -f \"$file\" ]; then\n      mkdir -p \"$TEMP_DIR/$(dirname \"$file\")\"\n      cp \"$file\" \"$TEMP_DIR/$file\"\n    fi\n  done\n\n  # Scan staged files\n  ash --target \"$TEMP_DIR\" --scanners trivy-repo --config-file .ash/precommit.yaml\n  SCAN_RESULT=$?\n\n  # Cleanup\n  rm -rf \"$TEMP_DIR\"\n\n  if [ $SCAN_RESULT -ne 0 ]; then\n    echo \"Security scan failed. Commit aborted.\"\n    exit 1\n  fi\nfi\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#pull-request-scanning","title":"Pull Request Scanning","text":"<pre><code># .github/workflows/pr-security-scan.yml\nname: PR Security Scan\non:\n  pull_request:\n    types: [opened, synchronize]\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: Get changed files\n        id: changed-files\n        run: |\n          git diff --name-only origin/${{ github.base_ref }}..HEAD &gt; changed_files.txt\n          echo \"files=$(cat changed_files.txt | tr '\\n' ' ')\" &gt;&gt; $GITHUB_OUTPUT\n\n      - name: Scan changed files\n        run: |\n          # Create directory structure for changed files\n          mkdir -p scan_target\n          while read file; do\n            if [ -f \"$file\" ]; then\n              mkdir -p \"scan_target/$(dirname \"$file\")\"\n              cp \"$file\" \"scan_target/$file\"\n            fi\n          done &lt; changed_files.txt\n\n          # Run security scan\n          ash --target scan_target --scanners trivy-repo --reporters sarif,markdown\n\n      - name: Comment PR\n        uses: actions/github-script@v6\n        with:\n          script: |\n            const fs = require('fs');\n            if (fs.existsSync('.ash/ash_output/reports/security-report.md')) {\n              const report = fs.readFileSync('.ash/ash_output/reports/security-report.md', 'utf8');\n              github.rest.issues.createComment({\n                issue_number: context.issue.number,\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                body: '## Security Scan Results\\n\\n' + report\n              });\n            }\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#2-multi-language-project-scanning","title":"2. Multi-Language Project Scanning","text":""},{"location":"docs/plugins/community/trivy-plugin/#monorepo-configuration","title":"Monorepo Configuration","text":"<pre><code># .ash/monorepo.yaml\nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"vuln\", \"secret\", \"misconfig\"]\n      severity_threshold: \"MEDIUM\"\n\nglobal_settings:\n  ignore_paths:\n    # Language-specific ignores\n    - path: \"**/node_modules/**\"\n      reason: \"Node.js dependencies\"\n    - path: \"**/vendor/**\"\n      reason: \"PHP/Go dependencies\"\n    - path: \"**/__pycache__/**\"\n      reason: \"Python cache\"\n    - path: \"**/target/**\"\n      reason: \"Rust/Java build output\"\n    - path: \"**/build/**\"\n      reason: \"Build artifacts\"\n</code></pre> <pre><code>#!/bin/bash\n# scan-monorepo.sh\n\n# Define language-specific directories\nPYTHON_DIRS=$(find . -name \"*.py\" -not -path \"*/.*\" | head -100 | xargs dirname | sort -u)\nNODE_DIRS=$(find . -name \"package.json\" -not -path \"*/node_modules/*\" | xargs dirname)\nGO_DIRS=$(find . -name \"go.mod\" | xargs dirname)\nRUST_DIRS=$(find . -name \"Cargo.toml\" | xargs dirname)\n\n# Scan each language ecosystem\necho \"Scanning Python projects...\"\necho \"$PYTHON_DIRS\" | while read dir; do\n  [ -n \"$dir\" ] &amp;&amp; ash --target \"$dir\" --scanners trivy-repo --config-file .ash/python.yaml\ndone\n\necho \"Scanning Node.js projects...\"\necho \"$NODE_DIRS\" | while read dir; do\n  [ -n \"$dir\" ] &amp;&amp; ash --target \"$dir\" --scanners trivy-repo --config-file .ash/nodejs.yaml\ndone\n\necho \"Scanning Go projects...\"\necho \"$GO_DIRS\" | while read dir; do\n  [ -n \"$dir\" ] &amp;&amp; ash --target \"$dir\" --scanners trivy-repo --config-file .ash/golang.yaml\ndone\n\necho \"Scanning Rust projects...\"\necho \"$RUST_DIRS\" | while read dir; do\n  [ -n \"$dir\" ] &amp;&amp; ash --target \"$dir\" --scanners trivy-repo --config-file .ash/rust.yaml\ndone\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#3-infrastructure-as-code-scanning","title":"3. Infrastructure as Code Scanning","text":""},{"location":"docs/plugins/community/trivy-plugin/#terraform-projects","title":"Terraform Projects","text":"<pre><code># .ash/terraform.yaml\nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"misconfig\", \"secret\"]\n      severity_threshold: \"HIGH\"\n\nglobal_settings:\n  ignore_paths:\n    - path: \"**/.terraform/**\"\n      reason: \"Terraform cache\"\n    - path: \"**/*.tfstate*\"\n      reason: \"Terraform state files\"\n</code></pre> <pre><code># Scan Terraform modules\nfind . -name \"*.tf\" | xargs dirname | sort -u | while read tf_dir; do\n  echo \"Scanning Terraform directory: $tf_dir\"\n  ash --target \"$tf_dir\" --scanners trivy-repo --config-file .ash/terraform.yaml\ndone\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#kubernetes-manifests","title":"Kubernetes Manifests","text":"<pre><code># .ash/kubernetes.yaml\nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"misconfig\", \"secret\"]\n      severity_threshold: \"MEDIUM\"\n</code></pre> <pre><code># Scan Kubernetes manifests\nfind . -name \"*.yaml\" -o -name \"*.yml\" | grep -E \"(k8s|kubernetes|manifests)\" | xargs dirname | sort -u | while read k8s_dir; do\n  echo \"Scanning Kubernetes directory: $k8s_dir\"\n  ash --target \"$k8s_dir\" --scanners trivy-repo --config-file .ash/kubernetes.yaml\ndone\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#4-container-and-docker-scanning","title":"4. Container and Docker Scanning","text":""},{"location":"docs/plugins/community/trivy-plugin/#dockerfile-analysis","title":"Dockerfile Analysis","text":"<pre><code>#!/bin/bash\n# scan-dockerfiles.sh\n\n# Find all Dockerfiles\nfind . -name \"Dockerfile*\" -o -name \"*.dockerfile\" | while read dockerfile; do\n  dir=$(dirname \"$dockerfile\")\n  echo \"Scanning Docker context: $dir\"\n\n  # Scan the directory containing Dockerfile\n  ash --target \"$dir\" --scanners trivy-repo --config-file .ash/docker.yaml\n\n  # Also scan the built image if available\n  image_name=$(grep -E \"^FROM\" \"$dockerfile\" | tail -1 | awk '{print $2}')\n  if [ -n \"$image_name\" ] &amp;&amp; [ \"$image_name\" != \"scratch\" ]; then\n    echo \"Scanning base image: $image_name\"\n    trivy image --format sarif --output \"${dir}/base-image.sarif\" \"$image_name\" || true\n  fi\ndone\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#docker-compose-projects","title":"Docker Compose Projects","text":"<pre><code>#!/bin/bash\n# scan-compose-projects.sh\n\nfind . -name \"docker-compose*.yml\" -o -name \"docker-compose*.yaml\" | while read compose_file; do\n  dir=$(dirname \"$compose_file\")\n  echo \"Scanning Docker Compose project: $dir\"\n\n  # Scan the compose directory\n  ash --target \"$dir\" --scanners trivy-repo --config-file .ash/docker-compose.yaml\n\n  # Extract and scan referenced images\n  grep -E \"^\\s*image:\" \"$compose_file\" | awk '{print $2}' | tr -d '\"' | while read image; do\n    if [ \"$image\" != \"scratch\" ] &amp;&amp; [[ \"$image\" != *\"${\"* ]]; then\n      echo \"Scanning compose image: $image\"\n      trivy image --format sarif --output \"${dir}/compose-${image//\\//_}.sarif\" \"$image\" || true\n    fi\n  done\ndone\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#5-license-compliance-scanning","title":"5. License Compliance Scanning","text":""},{"location":"docs/plugins/community/trivy-plugin/#open-source-license-audit","title":"Open Source License Audit","text":"<pre><code># .ash/license-audit.yaml\nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"license\"]\n      license_full: true\n      severity_threshold: \"LOW\"\n\nreporters:\n  csv:\n    enabled: true\n    options:\n      output_file: \"license-report.csv\"\n  html:\n    enabled: true\n    options:\n      output_file: \"license-report.html\"\n</code></pre> <pre><code>#!/bin/bash\n# license-compliance-check.sh\n\necho \"Running comprehensive license scan...\"\nash --scanners trivy-repo --config-file .ash/license-audit.yaml\n\n# Process license results\npython3 &lt;&lt; 'EOF'\nimport json\nimport csv\nfrom collections import defaultdict\n\n# Load scan results\nwith open('.ash/ash_output/reports/ash_aggregated_results.json') as f:\n    data = json.load(f)\n\n# Extract license information\nlicenses = defaultdict(list)\nfor run in data.get('runs', []):\n    for result in run.get('results', []):\n        if 'license' in result.get('ruleId', '').lower():\n            license_name = result.get('message', {}).get('text', 'Unknown')\n            file_path = result.get('locations', [{}])[0].get('physicalLocation', {}).get('artifactLocation', {}).get('uri', 'Unknown')\n            licenses[license_name].append(file_path)\n\n# Generate license summary\nprint(\"License Summary:\")\nprint(\"================\")\nfor license_name, files in licenses.items():\n    print(f\"{license_name}: {len(files)} files\")\n    for file_path in files[:5]:  # Show first 5 files\n        print(f\"  - {file_path}\")\n    if len(files) &gt; 5:\n        print(f\"  ... and {len(files) - 5} more files\")\n    print()\n\n# Check for problematic licenses\nproblematic_licenses = ['GPL-3.0', 'AGPL-3.0', 'SSPL-1.0']\nissues = []\nfor license_name in licenses:\n    if any(prob in license_name for prob in problematic_licenses):\n        issues.append(license_name)\n\nif issues:\n    print(\"\u26a0\ufe0f  Potentially problematic licenses found:\")\n    for issue in issues:\n        print(f\"  - {issue}\")\nelse:\n    print(\"\u2705 No known problematic licenses detected\")\nEOF\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#6-security-baseline-and-regression-testing","title":"6. Security Baseline and Regression Testing","text":""},{"location":"docs/plugins/community/trivy-plugin/#establish-security-baseline","title":"Establish Security Baseline","text":"<pre><code>#!/bin/bash\n# establish-security-baseline.sh\n\necho \"Establishing security baseline...\"\n\n# Run comprehensive scan\nash --scanners trivy-repo --reporters json --config-file .ash/baseline.yaml\n\n# Store baseline\nmkdir -p .security-baseline\ncp .ash/ash_output/reports/ash_aggregated_results.json .security-baseline/baseline-$(date +%Y%m%d).json\n\n# Generate baseline suppressions\npython3 &lt;&lt; 'EOF'\nimport json\nfrom datetime import datetime, timedelta\n\nwith open('.ash/ash_output/reports/ash_aggregated_results.json') as f:\n    data = json.load(f)\n\nsuppressions = []\nexpiration = (datetime.now() + timedelta(days=90)).strftime('%Y-%m-%d')\n\nfor run in data.get('runs', []):\n    for result in run.get('results', []):\n        rule_id = result.get('ruleId')\n        file_path = result.get('locations', [{}])[0].get('physicalLocation', {}).get('artifactLocation', {}).get('uri')\n\n        if rule_id and file_path:\n            suppression = {\n                'rule_id': rule_id,\n                'path': file_path,\n                'reason': 'Baseline finding - review and update',\n                'expiration': expiration\n            }\n            suppressions.append(suppression)\n\n# Write suppressions to YAML\nimport yaml\nwith open('.security-baseline/baseline-suppressions.yaml', 'w') as f:\n    yaml.dump({'global_settings': {'suppressions': suppressions}}, f, default_flow_style=False)\n\nprint(f\"Generated {len(suppressions)} baseline suppressions\")\nprint(\"Review .security-baseline/baseline-suppressions.yaml and merge approved suppressions into .ash/.ash.yaml\")\nEOF\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#regression-detection","title":"Regression Detection","text":"<pre><code>#!/bin/bash\n# detect-security-regressions.sh\n\necho \"Checking for security regressions...\"\n\n# Run current scan\nash --scanners trivy-repo --reporters json\n\n# Compare with baseline\npython3 &lt;&lt; 'EOF'\nimport json\nfrom pathlib import Path\n\n# Load current results\nwith open('.ash/ash_output/reports/ash_aggregated_results.json') as f:\n    current_data = json.load(f)\n\n# Load baseline (most recent)\nbaseline_files = sorted(Path('.security-baseline').glob('baseline-*.json'))\nif not baseline_files:\n    print(\"No baseline found. Run establish-security-baseline.sh first.\")\n    exit(1)\n\nwith open(baseline_files[-1]) as f:\n    baseline_data = json.load(f)\n\n# Extract findings\ndef extract_findings(data):\n    findings = set()\n    for run in data.get('runs', []):\n        for result in run.get('results', []):\n            rule_id = result.get('ruleId')\n            file_path = result.get('locations', [{}])[0].get('physicalLocation', {}).get('artifactLocation', {}).get('uri')\n            if rule_id and file_path:\n                findings.add(f\"{rule_id}:{file_path}\")\n    return findings\n\ncurrent_findings = extract_findings(current_data)\nbaseline_findings = extract_findings(baseline_data)\n\n# Detect regressions (new findings)\nregressions = current_findings - baseline_findings\nimprovements = baseline_findings - current_findings\n\nprint(f\"Security Status:\")\nprint(f\"  Current findings: {len(current_findings)}\")\nprint(f\"  Baseline findings: {len(baseline_findings)}\")\nprint(f\"  New issues (regressions): {len(regressions)}\")\nprint(f\"  Fixed issues: {len(improvements)}\")\n\nif regressions:\n    print(\"\\n\ud83d\udea8 Security Regressions Detected:\")\n    for regression in sorted(regressions):\n        rule_id, file_path = regression.split(':', 1)\n        print(f\"  - {rule_id} in {file_path}\")\n    exit(1)\nelse:\n    print(\"\\n\u2705 No security regressions detected\")\n\nif improvements:\n    print(\"\\n\ud83c\udf89 Security Improvements:\")\n    for improvement in sorted(improvements):\n        rule_id, file_path = improvement.split(':', 1)\n        print(f\"  - Fixed {rule_id} in {file_path}\")\nEOF\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/plugins/community/trivy-plugin/#common-issues","title":"Common Issues","text":""},{"location":"docs/plugins/community/trivy-plugin/#trivy-cli-not-found","title":"Trivy CLI Not Found","text":"<p>Error: <code>Trivy CLI not found in PATH</code></p> <p>Symptoms: - ASH reports \"Dependencies not satisfied\" for trivy-repo scanner - Scanner is skipped during execution - No Trivy results in output</p> <p>Solutions: 1. Verify Installation: Check if Trivy is installed    <pre><code>trivy version\nwhich trivy\n</code></pre></p> <ol> <li> <p>PATH Configuration: Ensure Trivy is in your PATH    <pre><code>echo $PATH\nexport PATH=$PATH:/path/to/trivy/bin\n</code></pre></p> </li> <li> <p>Reinstall Trivy: Use your preferred installation method    <pre><code># Homebrew\nbrew install trivy\n\n# Direct download\ncurl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin\n</code></pre></p> </li> <li> <p>Container Mode Alternative: Use ASH container mode    <pre><code>ash --mode container --scanners trivy-repo\n</code></pre></p> </li> </ol>"},{"location":"docs/plugins/community/trivy-plugin/#database-update-failures","title":"Database Update Failures","text":"<p>Error: <code>Failed to update vulnerability database</code></p> <p>Symptoms: - Trivy hangs during database download - Network timeout errors - Outdated vulnerability data</p> <p>Solutions: 1. Check Connectivity: Verify internet access    <pre><code>curl -I https://github.com/aquasecurity/trivy-db/releases\n</code></pre></p> <ol> <li> <p>Clear Cache: Remove corrupted database cache    <pre><code>rm -rf ~/.cache/trivy/\ntrivy image --download-db-only\n</code></pre></p> </li> <li> <p>Proxy Configuration: Configure proxy settings    <pre><code>export HTTP_PROXY=http://proxy.company.com:8080\nexport HTTPS_PROXY=http://proxy.company.com:8080\ntrivy image --download-db-only\n</code></pre></p> </li> <li> <p>Offline Mode: Use pre-downloaded database    <pre><code># Download database separately\ntrivy image --download-db-only --cache-dir /shared/trivy-cache\n\n# Use cached database\nexport TRIVY_CACHE_DIR=/shared/trivy-cache\nash --scanners trivy-repo\n</code></pre></p> </li> </ol>"},{"location":"docs/plugins/community/trivy-plugin/#permission-issues","title":"Permission Issues","text":"<p>Error: <code>Permission denied accessing scan target</code></p> <p>Symptoms: - Scanner fails to read files - Empty scan results - Access denied errors in logs</p> <p>Solutions: 1. Check Permissions: Verify directory access    <pre><code>ls -la /path/to/target\nfind /path/to/target -type f ! -readable\n</code></pre></p> <ol> <li> <p>Fix Ownership: Correct file ownership    <pre><code>sudo chown -R $USER:$USER /path/to/target\nchmod -R u+r /path/to/target\n</code></pre></p> </li> <li> <p>Run with Sudo: Use elevated permissions (not recommended)    <pre><code>sudo ash --scanners trivy-repo --target /path/to/target\n</code></pre></p> </li> <li> <p>Container Mode: Use container with proper volume mounts    <pre><code>ash --mode container --target /path/to/target --scanners trivy-repo\n</code></pre></p> </li> </ol>"},{"location":"docs/plugins/community/trivy-plugin/#large-repository-performance","title":"Large Repository Performance","text":"<p>Issue: Slow scanning on large repositories (&gt;1GB or &gt;10k files)</p> <p>Symptoms: - Scan takes &gt;10 minutes - High memory usage - Timeout errors</p> <p>Performance Solutions:</p> <ol> <li> <p>Selective Scanning: Target specific scan types    <pre><code>scanners:\n  trivy-repo:\n    options:\n      scanners: [\"vuln\"]  # Only vulnerabilities\n      severity_threshold: \"HIGH\"  # Reduce noise\n</code></pre></p> </li> <li> <p>Directory Filtering: Scan specific subdirectories    <pre><code># Scan only source code directories\nash --target src/ --scanners trivy-repo\nash --target lib/ --scanners trivy-repo\n</code></pre></p> </li> <li> <p>Ignore Patterns: Use ASH ignore patterns    <pre><code>global_settings:\n  ignore_paths:\n    - path: \"node_modules/**\"\n      reason: \"Third-party dependencies\"\n    - path: \"vendor/**\"\n      reason: \"Vendor libraries\"\n    - path: \"**/*.min.js\"\n      reason: \"Minified files\"\n</code></pre></p> </li> <li> <p>Parallel Processing: Split large repositories    <pre><code># Process in parallel\nash --target frontend/ --scanners trivy-repo &amp;\nash --target backend/ --scanners trivy-repo &amp;\nwait\n</code></pre></p> </li> </ol>"},{"location":"docs/plugins/community/trivy-plugin/#memory-issues","title":"Memory Issues","text":"<p>Error: <code>Out of memory</code> or system becomes unresponsive</p> <p>Solutions: 1. Increase Memory Limits: For container mode    <pre><code>docker run --memory=2g --scanners trivy-repo\n</code></pre></p> <ol> <li> <p>Process Smaller Chunks: Break down large scans    <pre><code>find . -maxdepth 1 -type d | xargs -I {} ash --target {} --scanners trivy-repo\n</code></pre></p> </li> <li> <p>Optimize Configuration: Reduce memory usage    <pre><code>scanners:\n  trivy-repo:\n    options:\n      ignore_unfixed: true  # Reduce result set\n      severity_threshold: \"MEDIUM\"  # Filter low-priority issues\n</code></pre></p> </li> </ol>"},{"location":"docs/plugins/community/trivy-plugin/#network-and-firewall-issues","title":"Network and Firewall Issues","text":"<p>Error: <code>Connection timeout</code> or <code>SSL certificate verification failed</code></p> <p>Solutions: 1. Corporate Firewall: Configure proxy and certificates    <pre><code>export HTTP_PROXY=http://proxy.company.com:8080\nexport HTTPS_PROXY=http://proxy.company.com:8080\nexport SSL_CERT_FILE=/path/to/corporate-ca.crt\n</code></pre></p> <ol> <li> <p>Disable SSL Verification (not recommended for production):    <pre><code>export TRIVY_INSECURE=true\n</code></pre></p> </li> <li> <p>Use Internal Mirror: Configure custom database repository    <pre><code>export TRIVY_DB_REPOSITORY=internal-mirror.company.com/trivy-db\n</code></pre></p> </li> </ol>"},{"location":"docs/plugins/community/trivy-plugin/#false-positives-and-noise","title":"False Positives and Noise","text":"<p>Issue: Too many irrelevant findings</p> <p>Solutions: 1. Severity Filtering: Adjust threshold    <pre><code>scanners:\n  trivy-repo:\n    options:\n      severity_threshold: \"HIGH\"\n      ignore_unfixed: true\n</code></pre></p> <ol> <li> <p>Targeted Suppressions: Use ASH suppression system    <pre><code>global_settings:\n  suppressions:\n    - rule_id: \"CVE-2023-12345\"\n      reason: \"Not applicable to our use case\"\n    - path: \"test/**\"\n      reason: \"Test files only\"\n    - rule_id: \"GHSA-*\"\n      path: \"vendor/**\"\n      reason: \"Third-party code\"\n</code></pre></p> </li> <li> <p>Custom Ignore Files: Use Trivy's ignore functionality    <pre><code># Create .trivyignore file\necho \"CVE-2023-12345\" &gt; .trivyignore\necho \"GHSA-abcd-1234\" &gt;&gt; .trivyignore\n</code></pre></p> </li> </ol>"},{"location":"docs/plugins/community/trivy-plugin/#debug-mode-and-logging","title":"Debug Mode and Logging","text":""},{"location":"docs/plugins/community/trivy-plugin/#enable-detailed-logging","title":"Enable Detailed Logging","text":"<pre><code># ASH debug mode\nash --scanners trivy-repo --log-level DEBUG\n\n# Trivy debug mode\nexport TRIVY_DEBUG=true\nash --scanners trivy-repo\n\n# Combined debugging\nTRIVY_DEBUG=true ash --scanners trivy-repo --log-level DEBUG\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#log-analysis","title":"Log Analysis","text":"<pre><code># Check ASH logs\ntail -f .ash/ash_output/scanners/trivy-repo/*/ash.log\n\n# Check Trivy-specific logs\ngrep -i trivy .ash/ash_output/scanners/trivy-repo/*/ash.log\n\n# Check for errors\ngrep -i error .ash/ash_output/scanners/trivy-repo/*/ash.log\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#container-mode-troubleshooting","title":"Container Mode Troubleshooting","text":""},{"location":"docs/plugins/community/trivy-plugin/#volume-mount-issues","title":"Volume Mount Issues","text":"<p>Error: <code>No such file or directory</code> in container mode</p> <p>Solution: <pre><code># Ensure proper volume mounting\nash --mode container --target $(pwd) --scanners trivy-repo\n\n# Check container logs\ndocker logs $(docker ps -q --filter ancestor=ash)\n</code></pre></p>"},{"location":"docs/plugins/community/trivy-plugin/#container-network-issues","title":"Container Network Issues","text":"<p>Error: Database download fails in container</p> <p>Solution: <pre><code># Use host network\nash --mode container --network host --scanners trivy-repo\n\n# Pre-download database\ntrivy image --download-db-only\nash --mode container --scanners trivy-repo\n</code></pre></p>"},{"location":"docs/plugins/community/trivy-plugin/#environment-specific-issues","title":"Environment-Specific Issues","text":""},{"location":"docs/plugins/community/trivy-plugin/#cicd-pipeline-issues","title":"CI/CD Pipeline Issues","text":"<p>Common Problems: - Network restrictions - Limited disk space - Time constraints</p> <p>Solutions: <pre><code># GitHub Actions example\n- name: Setup Trivy Cache\n  uses: actions/cache@v3\n  with:\n    path: ~/.cache/trivy\n    key: trivy-db-${{ github.run_id }}\n    restore-keys: trivy-db-\n\n- name: Pre-download Trivy DB\n  run: trivy image --download-db-only\n\n- name: Run ASH with Trivy\n  run: ash --scanners trivy-repo --reporters sarif\n  timeout-minutes: 10\n</code></pre></p>"},{"location":"docs/plugins/community/trivy-plugin/#windows-specific-issues","title":"Windows-Specific Issues","text":"<p>Path Separator Issues: <pre><code># Use forward slashes or escape backslashes\nash --target \"C:/projects/myapp\" --scanners trivy-repo\n\n# Or use PowerShell-style paths\nash --target $PWD --scanners trivy-repo\n</code></pre></p>"},{"location":"docs/plugins/community/trivy-plugin/#getting-additional-help","title":"Getting Additional Help","text":""},{"location":"docs/plugins/community/trivy-plugin/#diagnostic-information-collection","title":"Diagnostic Information Collection","text":"<p>When reporting issues, include:</p> <pre><code># System information\nuv --version\npython --version\ntrivy version\nash --version\n\n# Configuration\ncat .ash/.ash.yaml\n\n# Recent logs\ntail -50 .ash/ash_output/scanners/trivy-repo/*/ash.log\n\n# Environment variables\nenv | grep -i trivy\nenv | grep -i ash\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#support-channels","title":"Support Channels","text":"<ol> <li>ASH Issues: GitHub Issues</li> <li>Trivy Issues: Trivy GitHub</li> <li>Community Discussions: ASH Discussions</li> </ol>"},{"location":"docs/plugins/community/trivy-plugin/#integration-examples","title":"Integration Examples","text":""},{"location":"docs/plugins/community/trivy-plugin/#cicd-pipeline-integration","title":"CI/CD Pipeline Integration","text":""},{"location":"docs/plugins/community/trivy-plugin/#github-actions","title":"GitHub Actions","text":"<pre><code>name: Security Scan\non: [push, pull_request]\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install ASH\n        run: pip install git+https://github.com/awslabs/automated-security-helper.git@v3.1.2\n\n      - name: Run Trivy Security Scan\n        run: |\n          ash --scanners trivy-repo --reporters sarif,html\n\n      - name: Upload Results\n        uses: actions/upload-artifact@v3\n        with:\n          name: security-results\n          path: .ash/ash_output/\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#gitlab-ci","title":"GitLab CI","text":"<pre><code>security-scan:\n  stage: test\n  image: python:3.10\n  script:\n    - pip install git+https://github.com/awslabs/automated-security-helper.git@v3.1.2\n    - ash --scanners trivy-repo --reporters sarif,markdown\n  artifacts:\n    reports:\n      sast: .ash/ash_output/reports/ash_aggregated_results.sarif\n    paths:\n      - .ash/ash_output/\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#pre-commit-hook","title":"Pre-commit Hook","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: ash-trivy-scan\n        name: ASH Trivy Security Scan\n        entry: ash --mode precommit --scanners trivy-repo\n        language: system\n        pass_filenames: false\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"docs/plugins/community/trivy-plugin/#environment-variables","title":"Environment Variables","text":"<p>Trivy supports extensive configuration through environment variables:</p> <pre><code># Database and caching\nexport TRIVY_CACHE_DIR=/custom/cache/path\nexport TRIVY_DB_REPOSITORY=custom-db-repo\nexport TRIVY_TIMEOUT=10m\nexport TRIVY_OFFLINE_SCAN=true\n\n# Network and proxy\nexport HTTP_PROXY=http://proxy.company.com:8080\nexport HTTPS_PROXY=http://proxy.company.com:8080\nexport TRIVY_INSECURE=false\n\n# Output and formatting\nexport TRIVY_DEBUG=true\nexport TRIVY_QUIET=false\n\n# Run ASH with custom Trivy settings\nash --scanners trivy-repo\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#custom-database-configuration","title":"Custom Database Configuration","text":""},{"location":"docs/plugins/community/trivy-plugin/#using-private-vulnerability-database","title":"Using Private Vulnerability Database","text":"<pre><code># Set custom database repository\nexport TRIVY_DB_REPOSITORY=registry.company.com/security/trivy-db\n\n# Use custom OCI registry\nexport TRIVY_USERNAME=myuser\nexport TRIVY_PASSWORD=mypass\nexport TRIVY_DB_REPOSITORY=myregistry.azurecr.io/trivy-db\n\nash --scanners trivy-repo\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#offline-scanning-setup","title":"Offline Scanning Setup","text":"<pre><code># 1. Download database on internet-connected machine\ntrivy image --download-db-only --cache-dir /shared/trivy-cache\n\n# 2. Copy cache to air-gapped environment\nrsync -av /shared/trivy-cache/ airgapped-server:/opt/trivy-cache/\n\n# 3. Configure offline scanning\nexport TRIVY_CACHE_DIR=/opt/trivy-cache\nexport TRIVY_OFFLINE_SCAN=true\nash --scanners trivy-repo\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#advanced-suppression-strategies","title":"Advanced Suppression Strategies","text":""},{"location":"docs/plugins/community/trivy-plugin/#rule-based-suppressions","title":"Rule-Based Suppressions","text":"<pre><code># .ash/.ash.yaml\nglobal_settings:\n  suppressions:\n    # Suppress specific CVEs\n    - rule_id: \"CVE-2023-12345\"\n      reason: \"Patched in our custom build\"\n      expiration: \"2024-12-31\"\n\n    # Suppress by severity and path\n    - rule_id: \"GHSA-*\"\n      path: \"vendor/**\"\n      reason: \"Third-party dependencies managed separately\"\n\n    # Suppress license issues in test files\n    - rule_id: \"LICENSE-*\"\n      path: \"test/**\"\n      reason: \"Test files don't affect production licensing\"\n\n    # Suppress secrets in documentation\n    - rule_id: \"SECRET-*\"\n      path: \"docs/**\"\n      reason: \"Documentation examples only\"\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#dynamic-suppressions","title":"Dynamic Suppressions","text":"<pre><code># Generate suppressions from previous scan\nash --scanners trivy-repo --reporters json &gt; results.json\npython scripts/generate_suppressions.py results.json &gt; suppressions.yaml\n\n# Apply generated suppressions\ncat suppressions.yaml &gt;&gt; .ash/.ash.yaml\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#multi-environment-configuration","title":"Multi-Environment Configuration","text":""},{"location":"docs/plugins/community/trivy-plugin/#environment-specific-configurations","title":"Environment-Specific Configurations","text":"<pre><code># .ash/production.yaml\nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"vuln\", \"secret\"]\n      severity_threshold: \"MEDIUM\"\n      ignore_unfixed: false\n      disable_telemetry: true\n\n# .ash/development.yaml  \nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"vuln\", \"misconfig\", \"secret\", \"license\"]\n      severity_threshold: \"HIGH\"\n      ignore_unfixed: true\n      license_full: false\n</code></pre> <pre><code># Use environment-specific configs\nash --config-file .ash/production.yaml --scanners trivy-repo\nash --config-file .ash/development.yaml --scanners trivy-repo\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#multi-target-scanning-strategies","title":"Multi-Target Scanning Strategies","text":"<pre><code># Parallel scanning of different components\nash --target frontend/ --scanners trivy-repo --config-file .ash/frontend.yaml &amp;\nash --target backend/ --scanners trivy-repo --config-file .ash/backend.yaml &amp;\nash --target infrastructure/ --scanners trivy-repo --config-file .ash/infra.yaml &amp;\nwait\n\n# Sequential scanning with different thresholds\nfor dir in src/ lib/ config/; do\n  ash --target \"$dir\" --scanners trivy-repo --config-file \".ash/${dir%/}.yaml\"\ndone\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#performance-optimization","title":"Performance Optimization","text":""},{"location":"docs/plugins/community/trivy-plugin/#caching-strategies","title":"Caching Strategies","text":"<pre><code># Shared cache for team environments\nexport TRIVY_CACHE_DIR=/shared/trivy-cache\nmkdir -p /shared/trivy-cache\nchmod 755 /shared/trivy-cache\n\n# Pre-warm cache for CI/CD\ntrivy image --download-db-only --cache-dir /shared/trivy-cache\n\n# Use cached database\nash --scanners trivy-repo\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#incremental-scanning","title":"Incremental Scanning","text":"<pre><code># Scan only changed files (requires Git)\ngit diff --name-only HEAD~1 | while read file; do\n  if [[ -f \"$file\" ]]; then\n    ash --target \"$(dirname \"$file\")\" --scanners trivy-repo\n  fi\ndone\n\n# Scan based on file types\nfind . -name \"*.py\" -newer .last_scan | xargs -I {} dirname {} | sort -u | while read dir; do\n  ash --target \"$dir\" --scanners trivy-repo\ndone\ntouch .last_scan\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#resource-management","title":"Resource Management","text":"<pre><code># .ash/.ash.yaml - Optimized for large repositories\nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"vuln\"]  # Most critical findings only\n      severity_threshold: \"HIGH\"  # Reduce noise\n      ignore_unfixed: true  # Skip unfixable issues\n      disable_telemetry: true  # Reduce network calls\n\nglobal_settings:\n  ignore_paths:\n    - path: \"node_modules/**\"\n      reason: \"Package manager dependencies\"\n    - path: \"vendor/**\"\n      reason: \"Third-party code\"\n    - path: \"**/*.min.*\"\n      reason: \"Minified files\"\n    - path: \"**/test_data/**\"\n      reason: \"Test fixtures\"\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#integration-patterns","title":"Integration Patterns","text":""},{"location":"docs/plugins/community/trivy-plugin/#custom-reporting-pipeline","title":"Custom Reporting Pipeline","text":"<pre><code>#!/bin/bash\n# custom-trivy-scan.sh\n\n# Run Trivy scan with custom processing\nash --scanners trivy-repo --reporters sarif,json\n\n# Process results\npython scripts/process_trivy_results.py .ash/ash_output/reports/\n\n# Generate custom reports\npython scripts/generate_security_dashboard.py\n\n# Send notifications\npython scripts/notify_security_team.py\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#webhook-integration","title":"Webhook Integration","text":"<pre><code># webhook_handler.py\nimport json\nimport requests\nfrom pathlib import Path\n\ndef send_trivy_results():\n    results_file = Path(\".ash/ash_output/reports/ash_aggregated_results.json\")\n\n    if results_file.exists():\n        with open(results_file) as f:\n            results = json.load(f)\n\n        # Filter Trivy results\n        trivy_results = [\n            r for r in results.get(\"runs\", [])\n            if r.get(\"tool\", {}).get(\"driver\", {}).get(\"name\") == \"trivy-repo\"\n        ]\n\n        # Send to webhook\n        webhook_url = \"https://security-dashboard.company.com/webhook\"\n        requests.post(webhook_url, json={\"trivy_results\": trivy_results})\n\nif __name__ == \"__main__\":\n    send_trivy_results()\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#database-integration","title":"Database Integration","text":"<pre><code># store_results.py\nimport json\nimport sqlite3\nfrom datetime import datetime\nfrom pathlib import Path\n\ndef store_trivy_results():\n    conn = sqlite3.connect(\"security_results.db\")\n\n    # Create table if not exists\n    conn.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS trivy_findings (\n            id INTEGER PRIMARY KEY,\n            scan_date TEXT,\n            rule_id TEXT,\n            severity TEXT,\n            file_path TEXT,\n            message TEXT,\n            fixed_version TEXT\n        )\n    \"\"\")\n\n    # Load and store results\n    results_file = Path(\".ash/ash_output/reports/ash_aggregated_results.json\")\n    if results_file.exists():\n        with open(results_file) as f:\n            data = json.load(f)\n\n        scan_date = datetime.now().isoformat()\n\n        for run in data.get(\"runs\", []):\n            if run.get(\"tool\", {}).get(\"driver\", {}).get(\"name\") == \"trivy-repo\":\n                for result in run.get(\"results\", []):\n                    conn.execute(\"\"\"\n                        INSERT INTO trivy_findings \n                        (scan_date, rule_id, severity, file_path, message, fixed_version)\n                        VALUES (?, ?, ?, ?, ?, ?)\n                    \"\"\", (\n                        scan_date,\n                        result.get(\"ruleId\"),\n                        result.get(\"level\"),\n                        result.get(\"locations\", [{}])[0].get(\"physicalLocation\", {}).get(\"artifactLocation\", {}).get(\"uri\"),\n                        result.get(\"message\", {}).get(\"text\"),\n                        result.get(\"fixes\", [{}])[0].get(\"description\", {}).get(\"text\")\n                    ))\n\n    conn.commit()\n    conn.close()\n\nif __name__ == \"__main__\":\n    store_trivy_results()\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#specialized-use-cases","title":"Specialized Use Cases","text":""},{"location":"docs/plugins/community/trivy-plugin/#container-image-scanning-integration","title":"Container Image Scanning Integration","text":"<pre><code># .ash/container-scan.yaml\nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"vuln\", \"secret\"]\n      severity_threshold: \"HIGH\"\n      ignore_unfixed: false\n\n# Custom script for container + repo scanning\n</code></pre> <pre><code>#!/bin/bash\n# container-and-repo-scan.sh\n\n# Scan repository\nash --config-file .ash/container-scan.yaml --scanners trivy-repo\n\n# Scan container images referenced in repo\nfind . -name \"Dockerfile*\" -o -name \"docker-compose*.yml\" | while read file; do\n  echo \"Found container definition: $file\"\n  # Extract image names and scan them with Trivy directly\n  grep -E \"FROM|image:\" \"$file\" | while read line; do\n    image=$(echo \"$line\" | awk '{print $NF}' | tr -d '\"')\n    if [[ \"$image\" != \"scratch\" &amp;&amp; \"$image\" != *\"$\"* ]]; then\n      echo \"Scanning image: $image\"\n      trivy image --format sarif --output \"trivy-image-${image//\\//_}.sarif\" \"$image\"\n    fi\n  done\ndone\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#license-compliance-workflow","title":"License Compliance Workflow","text":"<pre><code># .ash/license-compliance.yaml\nscanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"license\"]\n      license_full: true\n      severity_threshold: \"LOW\"\n\nreporters:\n  csv:\n    enabled: true\n    options:\n      output_file: \"license-report.csv\"\n</code></pre> <pre><code># Generate license compliance report\nash --config-file .ash/license-compliance.yaml --scanners trivy-repo\n\n# Process license data\npython scripts/license_compliance_check.py license-report.csv\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#security-baseline-establishment","title":"Security Baseline Establishment","text":"<pre><code>#!/bin/bash\n# establish-baseline.sh\n\n# Initial comprehensive scan\nash --scanners trivy-repo --reporters json --config-file .ash/baseline.yaml\n\n# Store baseline\ncp .ash/ash_output/reports/ash_aggregated_results.json security-baseline.json\n\n# Generate initial suppressions for existing issues\npython scripts/generate_baseline_suppressions.py security-baseline.json &gt; .ash/baseline-suppressions.yaml\n\necho \"Security baseline established. Review and approve suppressions in .ash/baseline-suppressions.yaml\"\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"docs/plugins/community/trivy-plugin/#trend-analysis","title":"Trend Analysis","text":"<pre><code># trend_analysis.py\nimport json\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\ndef analyze_security_trends():\n    # Load historical data\n    history_dir = Path(\"security-history\")\n\n    dates = []\n    vuln_counts = []\n\n    for result_file in sorted(history_dir.glob(\"*.json\")):\n        with open(result_file) as f:\n            data = json.load(f)\n\n        date = datetime.fromisoformat(result_file.stem)\n        dates.append(date)\n\n        # Count vulnerabilities by severity\n        vuln_count = sum(\n            1 for run in data.get(\"runs\", [])\n            for result in run.get(\"results\", [])\n            if result.get(\"level\") in [\"error\", \"warning\"]\n        )\n        vuln_counts.append(vuln_count)\n\n    # Generate trend chart\n    plt.figure(figsize=(12, 6))\n    plt.plot(dates, vuln_counts, marker='o')\n    plt.title(\"Security Findings Trend\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Number of Findings\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(\"security-trend.png\")\n\n    return dates, vuln_counts\n\nif __name__ == \"__main__\":\n    analyze_security_trends()\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#automated-alerting","title":"Automated Alerting","text":"<pre><code>#!/bin/bash\n# security-alert.sh\n\n# Run scan\nash --scanners trivy-repo --reporters json\n\n# Check for critical issues\nCRITICAL_COUNT=$(jq '[.runs[].results[] | select(.level == \"error\")] | length' .ash/ash_output/reports/ash_aggregated_results.json)\n\nif [ \"$CRITICAL_COUNT\" -gt 0 ]; then\n  echo \"ALERT: $CRITICAL_COUNT critical security issues found!\"\n\n  # Send Slack notification\n  curl -X POST -H 'Content-type: application/json' \\\n    --data \"{\\\"text\\\":\\\"\ud83d\udea8 Security Alert: $CRITICAL_COUNT critical issues found in $(pwd)\\\"}\" \\\n    \"$SLACK_WEBHOOK_URL\"\n\n  # Send email\n  echo \"Critical security issues detected. See attached report.\" | \\\n    mail -s \"Security Alert: $(basename $(pwd))\" -a .ash/ash_output/reports/security-report.html security-team@company.com\nfi\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#comparison-with-other-scanners","title":"Comparison with Other Scanners","text":"Feature Trivy Bandit Semgrep Checkov Vulnerability Detection \u2705 Excellent \u274c No \u274c No \u274c No IaC Misconfiguration \u2705 Good \u274c No \u2705 Limited \u2705 Excellent Secret Detection \u2705 Good \u274c No \u2705 Good \u2705 Limited License Scanning \u2705 Excellent \u274c No \u274c No \u274c No Language Support \u2705 Multi-language \ud83d\udfe1 Python only \u2705 Multi-language \u2705 Multi-language Performance \ud83d\udfe1 Moderate \u2705 Fast \ud83d\udfe1 Moderate \ud83d\udfe1 Moderate Database Updates \u2705 Automatic N/A \u2705 Automatic \u2705 Automatic"},{"location":"docs/plugins/community/trivy-plugin/#best-practices","title":"Best Practices","text":""},{"location":"docs/plugins/community/trivy-plugin/#1-layered-security-approach","title":"1. Layered Security Approach","text":"<p>Combine Trivy with other ASH scanners for comprehensive coverage:</p> <pre><code>scanners:\n  trivy-repo:\n    enabled: true\n    options:\n      scanners: [\"vuln\", \"misconfig\"]\n\n  bandit:\n    enabled: true  # Python-specific SAST\n\n  semgrep:\n    enabled: true  # General SAST rules\n\n  checkov:\n    enabled: true  # IaC best practices\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#2-severity-based-workflows","title":"2. Severity-based Workflows","text":"<p>Configure different severity thresholds for different environments:</p> <pre><code># Production pipeline - strict\nscanners:\n  trivy-repo:\n    options:\n      severity_threshold: \"MEDIUM\"\n      ignore_unfixed: false\n\n# Development pipeline - permissive  \nscanners:\n  trivy-repo:\n    options:\n      severity_threshold: \"HIGH\"\n      ignore_unfixed: true\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#3-targeted-scanning","title":"3. Targeted Scanning","text":"<p>Use specific scanner types based on your needs:</p> <pre><code># For container-focused projects\nscanners:\n  trivy-repo:\n    options:\n      scanners: [\"vuln\", \"secret\"]\n\n# For infrastructure projects\nscanners:\n  trivy-repo:\n    options:\n      scanners: [\"misconfig\", \"secret\"]\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#4-regular-database-updates","title":"4. Regular Database Updates","text":"<p>Ensure Trivy's vulnerability database stays current:</p> <pre><code># Manual database update\ntrivy image --download-db-only\n\n# Automated in CI/CD\n- name: Update Trivy DB\n  run: trivy image --download-db-only\n</code></pre>"},{"location":"docs/plugins/community/trivy-plugin/#support-and-resources","title":"Support and Resources","text":""},{"location":"docs/plugins/community/trivy-plugin/#documentation","title":"Documentation","text":"<ul> <li>Trivy Official Documentation</li> <li>ASH Plugin Development Guide</li> <li>ASH Configuration Reference</li> </ul>"},{"location":"docs/plugins/community/trivy-plugin/#community","title":"Community","text":"<ul> <li>ASH GitHub Issues</li> <li>Trivy GitHub Repository</li> <li>ASH Discussions</li> </ul>"},{"location":"docs/plugins/community/trivy-plugin/#getting-help","title":"Getting Help","text":"<p>If you encounter issues with the Trivy plugin:</p> <ol> <li>Check the troubleshooting section above</li> <li>Review Trivy's official documentation</li> <li>Search existing GitHub issues</li> <li>Open a new issue with detailed information:</li> <li>ASH version</li> <li>Trivy version</li> <li>Configuration used</li> <li>Error messages</li> <li>Steps to reproduce</li> </ol>"},{"location":"docs/testing/","title":"ASH Testing Framework Documentation","text":""},{"location":"docs/testing/#overview","title":"Overview","text":"<p>This document provides comprehensive guidance on using the ASH testing framework. The framework is designed to make writing and maintaining tests easier, more consistent, and more effective. It includes utilities for test organization, fixtures, mocking, test data management, and integration testing.</p>"},{"location":"docs/testing/#test-organization","title":"Test Organization","text":""},{"location":"docs/testing/#directory-structure","title":"Directory Structure","text":"<p>The test directory structure mirrors the main codebase structure to make it easier to locate tests for specific components:</p> <pre><code>tests/\n\u251c\u2500\u2500 unit/                  # Unit tests that test individual components in isolation\n\u2502   \u251c\u2500\u2500 core/              # Tests for core functionality\n\u2502   \u251c\u2500\u2500 scanners/          # Tests for scanner components\n\u2502   \u251c\u2500\u2500 reporters/         # Tests for reporter components\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 integration/           # Integration tests that test component interactions\n\u2502   \u251c\u2500\u2500 scanners/          # Integration tests for scanner components\n\u2502   \u251c\u2500\u2500 reporters/         # Integration tests for reporter components\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 fixtures/              # Common test fixtures\n\u2502   \u251c\u2500\u2500 config/            # Configuration fixtures\n\u2502   \u251c\u2500\u2500 models/            # Model fixtures\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 utils/                 # Test utilities\n\u2502   \u251c\u2500\u2500 assertions.py      # Custom assertions\n\u2502   \u251c\u2500\u2500 mocks.py           # Mock objects and factories\n\u2502   \u251c\u2500\u2500 test_data.py       # Test data utilities\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 conftest.py            # Pytest configuration and shared fixtures\n</code></pre>"},{"location":"docs/testing/#naming-conventions","title":"Naming Conventions","text":"<p>Test files and functions follow these naming conventions:</p> <ul> <li>Test files: <code>test_&lt;module_name&gt;.py</code></li> <li>Test classes: <code>Test&lt;ComponentName&gt;</code></li> <li>Test functions: <code>test_&lt;functionality_being_tested&gt;</code></li> </ul> <p>Example: <pre><code># tests/unit/scanners/test_bandit_scanner.py\nclass TestBanditScanner:\n    def test_scan_python_file(self):\n        # Test code here\n        pass\n\n    def test_scan_with_custom_config(self):\n        # Test code here\n        pass\n</code></pre></p>"},{"location":"docs/testing/#test-categories-and-markers","title":"Test Categories and Markers","text":"<p>Tests are categorized using pytest markers to allow selective execution:</p> <ul> <li><code>@pytest.mark.unit</code>: Unit tests that test individual components in isolation</li> <li><code>@pytest.mark.integration</code>: Integration tests that test component interactions</li> <li><code>@pytest.mark.slow</code>: Tests that take a long time to run</li> <li><code>@pytest.mark.scanner</code>: Tests related to scanner functionality</li> <li><code>@pytest.mark.reporter</code>: Tests related to reporter functionality</li> <li><code>@pytest.mark.config</code>: Tests related to configuration functionality</li> <li><code>@pytest.mark.model</code>: Tests related to data models</li> <li><code>@pytest.mark.serial</code>: Tests that should not run in parallel</li> </ul>"},{"location":"docs/testing/#uv-migration-tests","title":"UV Migration Tests","text":"<p>The project includes comprehensive integration tests for the Poetry to UV migration, including:</p> <ul> <li>UV tool scanner execution tests</li> <li>UV dependency resolution and installation tests</li> <li>UV build system functionality tests</li> <li>Performance comparison tests between Poetry and UV</li> <li>Cross-platform compatibility tests</li> </ul>"},{"location":"docs/testing/#migration-validation","title":"Migration Validation","text":"<p>The project includes a migration validator that can be used in tests and CI:</p> <pre><code># Validate migration status in tests\npython -m automated_security_helper.utils.migration_validator\n\n# Use in CI with JSON output\npython -m automated_security_helper.utils.migration_validator --json\n</code></pre> <p>Example: <pre><code>import pytest\n\n@pytest.mark.unit\n@pytest.mark.scanner\ndef test_bandit_scanner_initialization():\n    # Test code here\n    pass\n\n@pytest.mark.integration\n@pytest.mark.slow\ndef test_end_to_end_scan():\n    # Test code here\n    pass\n</code></pre></p>"},{"location":"docs/testing/#test-fixtures","title":"Test Fixtures","text":""},{"location":"docs/testing/#common-fixtures","title":"Common Fixtures","text":"<p>The framework provides several common fixtures to simplify test setup:</p> <ul> <li><code>temp_config_dir</code>: Creates a temporary directory for configuration files</li> <li><code>temp_output_dir</code>: Creates a temporary directory for output files</li> <li><code>temp_project_dir</code>: Creates a temporary directory with a basic project structure</li> <li><code>temp_env_vars</code>: Sets environment variables for the duration of a test</li> </ul> <p>Example: <pre><code>def test_scanner_with_config(temp_config_dir):\n    config_file = temp_config_dir / \"config.yaml\"\n    config_file.write_text(\"scanners:\\n  bandit:\\n    enabled: true\")\n\n    scanner = BanditScanner(config_file=config_file)\n    assert scanner.is_enabled()\n</code></pre></p>"},{"location":"docs/testing/#custom-fixtures","title":"Custom Fixtures","text":"<p>You can create custom fixtures in <code>conftest.py</code> or in test modules:</p> <pre><code>@pytest.fixture\ndef mock_bandit_scanner():\n    scanner = MockBanditScanner()\n    scanner.add_finding(\"test.py\", \"Test finding\", \"HIGH\")\n    return scanner\n</code></pre>"},{"location":"docs/testing/#test-utilities","title":"Test Utilities","text":""},{"location":"docs/testing/#assertions","title":"Assertions","text":"<p>Custom assertions are available in <code>tests.utils.assertions</code>:</p> <pre><code>from tests.utils.assertions import assert_sarif_report_valid, assert_has_finding\n\ndef test_scanner_output(scanner_result):\n    assert_sarif_report_valid(scanner_result.sarif_report)\n    assert_has_finding(scanner_result.sarif_report, \"test.py\", \"Test finding\")\n</code></pre>"},{"location":"docs/testing/#mocking","title":"Mocking","text":"<p>Mocking utilities are available in <code>tests.utils.mocks</code>:</p> <pre><code>from tests.utils.mocks import create_mock_sarif_report, create_mock_scanner\n\ndef test_reporter_with_mock_scanner():\n    mock_scanner = create_mock_scanner(\"bandit\", findings=[\n        {\"file\": \"test.py\", \"message\": \"Test finding\", \"severity\": \"HIGH\"}\n    ])\n\n    reporter = SarifReporter()\n    report = reporter.generate_report(mock_scanner.scan())\n\n    assert \"test.py\" in report\n    assert \"Test finding\" in report\n</code></pre>"},{"location":"docs/testing/#test-data-management","title":"Test Data Management","text":"<p>Test data utilities are available in <code>tests.utils.test_data</code>:</p> <pre><code>from tests.utils.test_data import load_test_data, create_test_file\n\ndef test_scanner_with_test_data():\n    test_data = load_test_data(\"scanners/bandit/vulnerable_code.py\")\n    test_file = create_test_file(\"test.py\", test_data)\n\n    scanner = BanditScanner()\n    result = scanner.scan_file(test_file)\n\n    assert len(result.findings) &gt; 0\n</code></pre>"},{"location":"docs/testing/#integration-testing","title":"Integration Testing","text":""},{"location":"docs/testing/#integration-test-environment","title":"Integration Test Environment","text":"<p>The <code>IntegrationTestEnvironment</code> class provides utilities for setting up integration test environments:</p> <pre><code>from tests.utils.integration_test_utils import integration_test_environment\n\ndef test_end_to_end_scan():\n    with integration_test_environment() as env:\n        env.create_config_file({\"scanners\": {\"bandit\": {\"enabled\": True}}})\n        env.create_source_file(\"src/main.py\", \"import pickle\\npickle.loads(b'')\")\n\n        result = env.run_ash([\"scan\"])\n\n        assert result.returncode == 0\n        assert \"pickle.loads\" in env.read_output_file(\"bandit_report.txt\")\n</code></pre>"},{"location":"docs/testing/#component-interaction-testing","title":"Component Interaction Testing","text":"<p>The <code>ComponentInteractionTester</code> class provides utilities for testing interactions between components:</p> <pre><code>from tests.utils.integration_test_utils import component_interaction_tester\n\ndef test_scanner_reporter_interaction():\n    with component_interaction_tester() as tester:\n        scanner = tester.register_component(\"scanner\", BanditScanner)\n        reporter = tester.register_component(\"reporter\", SarifReporter)\n\n        scanner.scan()\n        reporter.report(scanner.results)\n\n        assert tester.verify_interaction(\"scanner\", \"reporter\", \"report\")\n</code></pre>"},{"location":"docs/testing/#resource-management","title":"Resource Management","text":"<p>Resource management utilities are available in <code>tests.utils.resource_management</code>:</p> <pre><code>from tests.utils.resource_management import temp_directory, managed_process\n\ndef test_with_external_process():\n    with temp_directory() as temp_dir:\n        config_file = temp_dir / \"config.yaml\"\n        config_file.write_text(\"scanners:\\n  bandit:\\n    enabled: true\")\n\n        with managed_process([\"python\", \"-m\", \"http.server\"], cwd=temp_dir) as process:\n            # Test code that interacts with the HTTP server\n            pass\n</code></pre>"},{"location":"docs/testing/#external-service-mocks","title":"External Service Mocks","text":"<p>Mock external services are available in <code>tests.utils.external_service_mocks</code>:</p> <pre><code>from tests.utils.external_service_mocks import mock_http_server, mock_api_server\n\ndef test_with_mock_http_server():\n    with mock_http_server() as server:\n        server.add_file(\"test.json\", {\"key\": \"value\"})\n        url = server.get_url(\"test.json\")\n\n        # Test code that interacts with the HTTP server\n        response = requests.get(url)\n        assert response.json() == {\"key\": \"value\"}\n\ndef test_with_mock_api_server():\n    with mock_api_server() as server:\n        def handle_hello(method, path, query, headers, body):\n            return 200, {\"Content-Type\": \"application/json\"}, {\"message\": \"Hello, world!\"}\n\n        server.add_route(\"/hello\", handle_hello)\n        url = server.get_url(\"hello\")\n\n        # Test code that interacts with the API server\n        response = requests.get(url)\n        assert response.json() == {\"message\": \"Hello, world!\"}\n</code></pre>"},{"location":"docs/testing/#coverage-reporting","title":"Coverage Reporting","text":""},{"location":"docs/testing/#configuration","title":"Configuration","text":"<p>Coverage reporting is configured in <code>.coveragerc</code>:</p> <pre><code>[run]\nsource = automated_security_helper\nomit =\n    */tests/*\n    */venv/*\n    */site-packages/*\n\n[report]\nexclude_lines =\n    pragma: no cover\n    def __repr__\n    raise NotImplementedError\n    if __name__ == .__main__.:\n    pass\n    raise ImportError\n</code></pre>"},{"location":"docs/testing/#running-coverage-reports","title":"Running Coverage Reports","text":"<p>To run tests with coverage reporting:</p> <pre><code>pytest --cov=automated_security_helper\n</code></pre> <p>To generate an HTML coverage report:</p> <pre><code>pytest --cov=automated_security_helper --cov-report=html\n</code></pre>"},{"location":"docs/testing/#coverage-enforcement","title":"Coverage Enforcement","text":"<p>Coverage thresholds are enforced in CI pipelines. The minimum coverage threshold is 80% for the overall codebase, with higher thresholds for critical components.</p>"},{"location":"docs/testing/#parallel-test-execution","title":"Parallel Test Execution","text":""},{"location":"docs/testing/#configuration_1","title":"Configuration","text":"<p>Parallel test execution is configured in <code>pytest.ini</code>:</p> <pre><code>[pytest]\naddopts = -xvs\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\nmarkers =\n    unit: Unit tests that test individual components in isolation\n    integration: Integration tests that test component interactions\n    slow: Tests that take a long time to run\n    scanner: Tests related to scanner functionality\n    reporter: Tests related to reporter functionality\n    config: Tests related to configuration functionality\n    model: Tests related to data models\n    serial: Tests that should not run in parallel\n</code></pre>"},{"location":"docs/testing/#running-tests-in-parallel","title":"Running Tests in Parallel","text":"<p>To run tests in parallel:</p> <pre><code>pytest -xvs -n auto\n</code></pre> <p>Tests marked with <code>@pytest.mark.serial</code> will not run in parallel.</p>"},{"location":"docs/testing/#test-selection-and-filtering","title":"Test Selection and Filtering","text":""},{"location":"docs/testing/#command-line-options","title":"Command-Line Options","text":"<p>The framework provides several command-line options for selective test execution:</p> <ul> <li><code>--run-slow</code>: Run slow tests</li> <li><code>--run-integration</code>: Run integration tests</li> <li><code>--run-changed-only</code>: Run only tests for changed files</li> <li><code>--base-branch</code>: Base branch for <code>--run-changed-only</code> option (default: <code>main</code>)</li> </ul> <p>Example: <pre><code>pytest --run-integration --run-slow\n</code></pre></p>"},{"location":"docs/testing/#test-selection-utilities","title":"Test Selection Utilities","text":"<p>Test selection utilities are available in <code>tests.utils.test_selection</code>:</p> <pre><code>from tests.utils.test_selection import get_changed_files, get_related_test_files\n\ndef test_selection():\n    changed_files = get_changed_files(\"main\")\n    related_test_files = get_related_test_files(changed_files)\n\n    # Run only the related tests\n    for test_file in related_test_files:\n        pytest.main([test_file])\n</code></pre>"},{"location":"docs/testing/#best-practices","title":"Best Practices","text":""},{"location":"docs/testing/#writing-effective-tests","title":"Writing Effective Tests","text":"<ol> <li>Test one thing per test: Each test should focus on testing a single functionality or behavior.</li> <li>Use descriptive test names: Test names should clearly describe what is being tested.</li> <li>Follow the AAA pattern: Arrange, Act, Assert.</li> <li>Use fixtures for setup and teardown: Use fixtures to set up test environments and clean up after tests.</li> <li>Mock external dependencies: Use mocks to isolate the code being tested from external dependencies.</li> <li>Test edge cases: Test boundary conditions and error cases.</li> <li>Keep tests independent: Tests should not depend on the state created by other tests.</li> <li>Use parameterized tests: Use <code>@pytest.mark.parametrize</code> to test multiple inputs with the same test function.</li> </ol>"},{"location":"docs/testing/#example","title":"Example:","text":"<pre><code>import pytest\nfrom automated_security_helper.scanners.bandit_scanner import BanditScanner\n\n@pytest.mark.parametrize(\"code,expected_findings\", [\n    (\"import pickle\\npickle.loads(b'')\", 1),  # Unsafe pickle usage\n    (\"import hashlib\\nhashlib.md5(b'')\", 1),  # Weak hash algorithm\n    (\"print('Hello, world!')\", 0),  # No security issues\n])\ndef test_bandit_scanner_findings(temp_project_dir, code, expected_findings):\n    # Arrange\n    test_file = temp_project_dir / \"test.py\"\n    test_file.write_text(code)\n    scanner = BanditScanner()\n\n    # Act\n    result = scanner.scan_file(test_file)\n\n    # Assert\n    assert len(result.findings) == expected_findings\n</code></pre>"},{"location":"docs/testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/testing/#common-issues","title":"Common Issues","text":"<ol> <li>Tests fail in CI but pass locally: Check for environment differences, file path issues, or timing issues.</li> <li>Tests interfere with each other: Check for shared state or resources that are not properly isolated.</li> <li>Slow tests: Use profiling to identify bottlenecks, consider marking slow tests with <code>@pytest.mark.slow</code>.</li> <li>Flaky tests: Check for race conditions, timing issues, or external dependencies.</li> </ol>"},{"location":"docs/testing/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Use <code>pytest -v</code>: Run tests with verbose output to see more details.</li> <li>Use <code>pytest --pdb</code>: Drop into the debugger on test failures.</li> <li>Use <code>print</code> statements: Add print statements to see what's happening during test execution.</li> <li>Check test isolation: Make sure tests don't depend on the state created by other tests.</li> <li>Check resource cleanup: Make sure resources are properly cleaned up after tests.</li> </ol>"},{"location":"docs/testing/#contributing","title":"Contributing","text":"<p>When adding new tests or test utilities, please follow these guidelines:</p> <ol> <li>Follow naming conventions: Use the naming conventions described in this document.</li> <li>Add appropriate markers: Add markers to categorize tests appropriately.</li> <li>Document test utilities: Add docstrings to test utilities to explain how to use them.</li> <li>Keep tests fast: Optimize tests to run quickly, mark slow tests with <code>@pytest.mark.slow</code>.</li> <li>Keep tests independent: Tests should not depend on the state created by other tests.</li> <li>Add examples: Add examples to show how to use new test utilities.</li> <li>Update documentation: Update this document when adding new test utilities or patterns.</li> </ol>"},{"location":"docs/testing/parallel_testing/","title":"Parallel Test Execution Guide","text":"<p>This guide explains how to use and configure parallel test execution in the ASH testing framework.</p>"},{"location":"docs/testing/parallel_testing/#overview","title":"Overview","text":"<p>Parallel test execution allows tests to run simultaneously, significantly reducing the time required to run the test suite. However, it requires careful consideration to ensure tests don't interfere with each other when running in parallel.</p>"},{"location":"docs/testing/parallel_testing/#configuration","title":"Configuration","text":"<p>Parallel test execution is enabled by default in the pytest configuration. The following settings in <code>pytest.ini</code> control parallel execution:</p> <pre><code>addopts =\n    # Other options...\n    -n auto\n</code></pre> <p>The <code>-n auto</code> option tells pytest to automatically determine the number of workers based on the number of available CPU cores.</p>"},{"location":"docs/testing/parallel_testing/#writing-parallel-safe-tests","title":"Writing Parallel-Safe Tests","text":"<p>To ensure your tests can run safely in parallel, follow these guidelines:</p>"},{"location":"docs/testing/parallel_testing/#1-use-isolated-resources","title":"1. Use Isolated Resources","text":"<p>Always use isolated resources (files, directories, environment variables) that won't conflict with other tests running in parallel.</p> <pre><code># BAD: Using a fixed file path\ndef test_scanner_output():\n    output_file = Path(\"/tmp/scanner_output.json\")\n    # This could conflict with other tests using the same path\n\n# GOOD: Using an isolated file path\ndef test_scanner_output(ash_temp_path):\n    output_file = ash_temp_path / \"scanner_output.json\"\n    # This is isolated to this test\n</code></pre>"},{"location":"docs/testing/parallel_testing/#2-use-the-provided-test-utilities","title":"2. Use the Provided Test Utilities","text":"<p>The testing framework provides utilities to help write parallel-safe tests:</p> <pre><code>from tests.utils.parallel_test_utils import isolated_test_context, ParallelTestHelper\n\ndef test_with_isolation():\n    with isolated_test_context() as temp_dir:\n        # Use temp_dir for test files\n        input_file = temp_dir / \"input.txt\"\n        input_file.write_text(\"test content\")\n\n        # Run your test code\n        result = process_file(input_file)\n        assert result == \"expected output\"\n</code></pre>"},{"location":"docs/testing/parallel_testing/#3-avoid-modifying-global-state","title":"3. Avoid Modifying Global State","text":"<p>Tests should not modify global state that could affect other tests:</p> <pre><code># BAD: Modifying global configuration\ndef test_with_global_config():\n    set_global_config({\"key\": \"value\"})  # This affects other tests\n\n# GOOD: Using context manager for isolated configuration\ndef test_with_isolated_config():\n    with mock_config({\"key\": \"value\"}):  # This is isolated to this test\n        # Test code here\n</code></pre>"},{"location":"docs/testing/parallel_testing/#4-use-test-specific-environment-variables","title":"4. Use Test-Specific Environment Variables","text":"<p>When tests need environment variables, use isolated names:</p> <pre><code>from tests.utils.parallel_test_utils import get_isolated_env_var_name\n\ndef test_with_env_vars():\n    env_var_name = get_isolated_env_var_name(\"API_KEY\")\n    with environment_variables(**{env_var_name: \"test_value\"}):\n        # Test code here\n</code></pre>"},{"location":"docs/testing/parallel_testing/#5-use-pytest-fixtures-for-resource-management","title":"5. Use Pytest Fixtures for Resource Management","text":"<p>Pytest fixtures provide a clean way to set up and tear down resources:</p> <pre><code>@pytest.fixture\ndef isolated_config_file(ash_temp_path):\n    config_file = ash_temp_path / \"config.yaml\"\n    config_file.write_text(\"key: value\")\n    return config_file\n\ndef test_with_config(isolated_config_file):\n    # Use isolated_config_file in your test\n</code></pre>"},{"location":"docs/testing/parallel_testing/#marking-tests-as-non-parallel","title":"Marking Tests as Non-Parallel","text":"<p>Some tests may not be suitable for parallel execution. Mark these tests with the <code>pytest.mark.serial</code> decorator:</p> <pre><code>@pytest.mark.serial\ndef test_that_must_run_serially():\n    # This test will not run in parallel with other tests\n</code></pre>"},{"location":"docs/testing/parallel_testing/#troubleshooting-parallel-test-issues","title":"Troubleshooting Parallel Test Issues","text":"<p>If you encounter issues with parallel test execution, consider these common problems:</p> <ol> <li>Resource Conflicts: Tests might be using the same files, directories, or environment variables.</li> <li> <p>Solution: Use the <code>isolated_test_context</code> or pytest's <code>ash_temp_path</code> fixture.</p> </li> <li> <p>Database Conflicts: Tests might be using the same database tables.</p> </li> <li> <p>Solution: Use separate database schemas or in-memory databases for testing.</p> </li> <li> <p>Global State Modifications: Tests might be modifying global state.</p> </li> <li> <p>Solution: Use context managers to isolate changes to the test scope.</p> </li> <li> <p>Order Dependencies: Tests might depend on running in a specific order.</p> </li> <li>Solution: Make tests independent of each other.</li> </ol>"},{"location":"docs/testing/parallel_testing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Test Isolation: More isolated tests run better in parallel but may have more setup/teardown overhead.</li> <li>Resource Usage: Running tests in parallel increases CPU and memory usage.</li> <li>CI Environment: Consider setting a specific number of workers in CI environments with limited resources:   <pre><code>pytest -n 2  # Use 2 workers instead of auto-detection\n</code></pre></li> </ul>"},{"location":"docs/testing/parallel_testing/#advanced-configuration","title":"Advanced Configuration","text":"<p>For more advanced parallel test configuration, you can create a <code>conftest.py</code> file with custom settings:</p> <pre><code>def pytest_xdist_make_scheduler(config, log):\n    \"\"\"Custom test scheduler for parallel execution.\"\"\"\n    from xdist.scheduler import LoadScheduling\n    return LoadScheduling(config, log)\n</code></pre> <p>This allows for customizing how tests are distributed among workers.</p>"},{"location":"docs/testing/test_organization/","title":"Test Organization Guide","text":""},{"location":"docs/testing/test_organization/#overview","title":"Overview","text":"<p>This document provides guidelines for organizing tests in the ASH project. Proper test organization makes tests easier to find, understand, and maintain.</p>"},{"location":"docs/testing/test_organization/#directory-structure","title":"Directory Structure","text":"<p>The test directory structure mirrors the main codebase structure to make it easier to locate tests for specific components:</p> <pre><code>tests/\n\u251c\u2500\u2500 unit/                  # Unit tests that test individual components in isolation\n\u2502   \u251c\u2500\u2500 core/              # Tests for core functionality\n\u2502   \u251c\u2500\u2500 scanners/          # Tests for scanner components\n\u2502   \u251c\u2500\u2500 reporters/         # Tests for reporter components\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 integration/           # Integration tests that test component interactions\n\u2502   \u251c\u2500\u2500 scanners/          # Integration tests for scanner components\n\u2502   \u251c\u2500\u2500 reporters/         # Integration tests for reporter components\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 fixtures/              # Common test fixtures\n\u2502   \u251c\u2500\u2500 config/            # Configuration fixtures\n\u2502   \u251c\u2500\u2500 models/            # Model fixtures\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 utils/                 # Test utilities\n\u2502   \u251c\u2500\u2500 assertions.py      # Custom assertions\n\u2502   \u251c\u2500\u2500 mocks.py           # Mock objects and factories\n\u2502   \u251c\u2500\u2500 test_data.py       # Test data utilities\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 conftest.py            # Pytest configuration and shared fixtures\n\u2514\u2500\u2500 docs/                  # Test documentation\n    \u251c\u2500\u2500 testing_framework.md  # Main documentation\n    \u251c\u2500\u2500 test_organization.md  # This document\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"docs/testing/test_organization/#test-types","title":"Test Types","text":""},{"location":"docs/testing/test_organization/#unit-tests","title":"Unit Tests","text":"<p>Unit tests focus on testing individual components in isolation. They should be fast, reliable, and independent of external dependencies.</p> <ul> <li>Location: <code>tests/unit/&lt;module_path&gt;/</code></li> <li>Naming: <code>test_&lt;module_name&gt;.py</code></li> <li>Marker: <code>@pytest.mark.unit</code></li> </ul> <p>Example: <pre><code># tests/unit/scanners/test_bandit_scanner.py\nimport pytest\n\n@pytest.mark.unit\n@pytest.mark.scanner\ndef test_bandit_scanner_initialization():\n    # Test code here\n    pass\n</code></pre></p>"},{"location":"docs/testing/test_organization/#integration-tests","title":"Integration Tests","text":"<p>Integration tests focus on testing interactions between components. They verify that components work together correctly.</p> <ul> <li>Location: <code>tests/integration/&lt;module_path&gt;/</code></li> <li>Naming: <code>test_&lt;component1&gt;_&lt;component2&gt;_integration.py</code></li> <li>Marker: <code>@pytest.mark.integration</code></li> </ul> <p>Example: <pre><code># tests/integration/scanners/test_scanner_reporter_integration.py\nimport pytest\n\n@pytest.mark.integration\n@pytest.mark.scanner\n@pytest.mark.reporter\ndef test_scanner_reporter_integration():\n    # Test code here\n    pass\n</code></pre></p>"},{"location":"docs/testing/test_organization/#naming-conventions","title":"Naming Conventions","text":""},{"location":"docs/testing/test_organization/#test-files","title":"Test Files","text":"<p>Test files should be named according to the component they are testing:</p> <ul> <li><code>test_&lt;module_name&gt;.py</code></li> </ul> <p>Examples: - <code>test_bandit_scanner.py</code> - <code>test_sarif_reporter.py</code> - <code>test_config_loader.py</code></p>"},{"location":"docs/testing/test_organization/#test-classes","title":"Test Classes","text":"<p>Test classes should be named according to the component they are testing:</p> <ul> <li><code>Test&lt;ComponentName&gt;</code></li> </ul> <p>Examples: - <code>TestBanditScanner</code> - <code>TestSarifReporter</code> - <code>TestConfigLoader</code></p>"},{"location":"docs/testing/test_organization/#test-functions","title":"Test Functions","text":"<p>Test functions should be named according to the functionality they are testing:</p> <ul> <li><code>test_&lt;functionality_being_tested&gt;</code></li> </ul> <p>Examples: - <code>test_scan_python_file</code> - <code>test_generate_sarif_report</code> - <code>test_load_config_from_file</code></p> <p>For parameterized tests, include the parameter in the name:</p> <ul> <li><code>test_&lt;functionality&gt;_with_&lt;parameter&gt;</code></li> </ul> <p>Examples: - <code>test_scan_with_custom_config</code> - <code>test_report_with_multiple_findings</code></p>"},{"location":"docs/testing/test_organization/#test-categories-and-markers","title":"Test Categories and Markers","text":"<p>Tests are categorized using pytest markers to allow selective execution:</p> <ul> <li><code>@pytest.mark.unit</code>: Unit tests that test individual components in isolation</li> <li><code>@pytest.mark.integration</code>: Integration tests that test component interactions</li> <li><code>@pytest.mark.slow</code>: Tests that take a long time to run</li> <li><code>@pytest.mark.scanner</code>: Tests related to scanner functionality</li> <li><code>@pytest.mark.reporter</code>: Tests related to reporter functionality</li> <li><code>@pytest.mark.config</code>: Tests related to configuration functionality</li> <li><code>@pytest.mark.model</code>: Tests related to data models</li> <li><code>@pytest.mark.serial</code>: Tests that should not run in parallel</li> </ul> <p>Example: <pre><code>import pytest\n\n@pytest.mark.unit\n@pytest.mark.scanner\ndef test_bandit_scanner_initialization():\n    # Test code here\n    pass\n\n@pytest.mark.integration\n@pytest.mark.slow\ndef test_end_to_end_scan():\n    # Test code here\n    pass\n</code></pre></p>"},{"location":"docs/testing/test_organization/#test-structure","title":"Test Structure","text":"<p>Tests should follow the Arrange-Act-Assert (AAA) pattern:</p> <ol> <li>Arrange: Set up the test environment and inputs</li> <li>Act: Execute the code being tested</li> <li>Assert: Verify the results</li> </ol> <p>Example: <pre><code>def test_bandit_scanner_findings(temp_project_dir):\n    # Arrange\n    test_file = temp_project_dir / \"test.py\"\n    test_file.write_text(\"import pickle\\npickle.loads(b'')\")\n    scanner = BanditScanner()\n\n    # Act\n    result = scanner.scan_file(test_file)\n\n    # Assert\n    assert len(result.findings) == 1\n    assert \"pickle.loads\" in result.findings[0].message\n</code></pre></p>"},{"location":"docs/testing/test_organization/#test-independence","title":"Test Independence","text":"<p>Tests should be independent of each other. They should not depend on the state created by other tests.</p> <ul> <li>Use fixtures for setup and teardown</li> <li>Avoid global state</li> <li>Clean up resources after tests</li> </ul> <p>Example: <pre><code>@pytest.fixture\ndef temp_config():\n    config_file = Path(tempfile.mktemp())\n    config_file.write_text(\"scanners:\\n  bandit:\\n    enabled: true\")\n    yield config_file\n    config_file.unlink()\n\ndef test_with_config(temp_config):\n    # Test code here\n    pass\n</code></pre></p>"},{"location":"docs/testing/test_organization/#test-data","title":"Test Data","text":"<p>Test data should be stored in a consistent location:</p> <ul> <li>Small test data can be included directly in the test file</li> <li>Larger test data should be stored in <code>tests/fixtures/data/</code></li> <li>Test data should be versioned with the code</li> </ul> <p>Example: <pre><code>def test_with_test_data():\n    test_data_path = Path(__file__).parent / \"../fixtures/data/vulnerable_code.py\"\n    with open(test_data_path, \"r\") as f:\n        test_data = f.read()\n\n    # Test code here\n    pass\n</code></pre></p>"},{"location":"docs/testing/test_organization/#best-practices","title":"Best Practices","text":"<ol> <li>Test one thing per test: Each test should focus on testing a single functionality or behavior.</li> <li>Use descriptive test names: Test names should clearly describe what is being tested.</li> <li>Follow the AAA pattern: Arrange, Act, Assert.</li> <li>Use fixtures for setup and teardown: Use fixtures to set up test environments and clean up after tests.</li> <li>Mock external dependencies: Use mocks to isolate the code being tested from external dependencies.</li> <li>Test edge cases: Test boundary conditions and error cases.</li> <li>Keep tests independent: Tests should not depend on the state created by other tests.</li> <li>Use parameterized tests: Use <code>@pytest.mark.parametrize</code> to test multiple inputs with the same test function.</li> </ol>"},{"location":"docs/testing/test_organization/#example-test-file","title":"Example Test File","text":"<pre><code># tests/unit/scanners/test_bandit_scanner.py\nimport pytest\nfrom pathlib import Path\nfrom automated_security_helper.scanners.bandit_scanner import BanditScanner\n\n@pytest.fixture\ndef temp_python_file(temp_project_dir):\n    file_path = temp_project_dir / \"test.py\"\n    return file_path\n\n@pytest.mark.unit\n@pytest.mark.scanner\nclass TestBanditScanner:\n    def test_initialization(self):\n        scanner = BanditScanner()\n        assert scanner.name == \"bandit\"\n        assert scanner.is_enabled()\n\n    def test_scan_python_file(self, temp_python_file):\n        # Arrange\n        temp_python_file.write_text(\"import pickle\\npickle.loads(b'')\")\n        scanner = BanditScanner()\n\n        # Act\n        result = scanner.scan_file(temp_python_file)\n\n        # Assert\n        assert len(result.findings) == 1\n        assert \"pickle.loads\" in result.findings[0].message\n\n    @pytest.mark.parametrize(\"code,expected_findings\", [\n        (\"import pickle\\npickle.loads(b'')\", 1),  # Unsafe pickle usage\n        (\"import hashlib\\nhashlib.md5(b'')\", 1),  # Weak hash algorithm\n        (\"print('Hello, world!')\", 0),  # No security issues\n    ])\n    def test_findings_with_different_code(self, temp_python_file, code, expected_findings):\n        # Arrange\n        temp_python_file.write_text(code)\n        scanner = BanditScanner()\n\n        # Act\n        result = scanner.scan_file(temp_python_file)\n\n        # Assert\n        assert len(result.findings) == expected_findings\n</code></pre>"},{"location":"docs/testing/test_selection/","title":"Test Selection and Filtering Guide","text":"<p>This guide explains how to use the test selection and filtering capabilities in the ASH testing framework.</p>"},{"location":"docs/testing/test_selection/#overview","title":"Overview","text":"<p>The ASH testing framework provides several ways to select and filter tests, allowing you to run specific subsets of tests based on various criteria such as:</p> <ul> <li>Test markers</li> <li>Keywords in test names</li> <li>File paths</li> <li>Related code changes</li> <li>Test categories</li> </ul>"},{"location":"docs/testing/test_selection/#using-test-markers","title":"Using Test Markers","text":"<p>Test markers allow you to categorize tests and run specific categories.</p>"},{"location":"docs/testing/test_selection/#available-markers","title":"Available Markers","text":"<p>The following markers are available in the ASH testing framework:</p> <ul> <li><code>unit</code>: Unit tests that test individual components in isolation</li> <li><code>integration</code>: Integration tests that test component interactions</li> <li><code>slow</code>: Tests that take a long time to run</li> <li><code>scanner</code>: Tests related to scanner functionality</li> <li><code>reporter</code>: Tests related to reporter functionality</li> <li><code>config</code>: Tests related to configuration functionality</li> <li><code>model</code>: Tests related to data models</li> </ul>"},{"location":"docs/testing/test_selection/#running-tests-with-specific-markers","title":"Running Tests with Specific Markers","text":"<p>To run tests with a specific marker:</p> <pre><code># Run all unit tests\npytest -m unit\n\n# Run all integration tests\npytest -m integration\n\n# Run all scanner tests\npytest -m scanner\n\n# Run tests with multiple markers (OR logic)\npytest -m \"unit or integration\"\n\n# Run tests with multiple markers (AND logic)\npytest -m \"scanner and not slow\"\n</code></pre>"},{"location":"docs/testing/test_selection/#using-keywords","title":"Using Keywords","text":"<p>You can run tests that match specific keywords in their names:</p> <pre><code># Run all tests with \"config\" in their name\npytest -k config\n\n# Run all tests with \"parse\" or \"validate\" in their name\npytest -k \"parse or validate\"\n\n# Run all tests with \"config\" in their name but not \"error\"\npytest -k \"config and not error\"\n</code></pre>"},{"location":"docs/testing/test_selection/#running-tests-for-changed-files","title":"Running Tests for Changed Files","text":"<p>The testing framework provides a utility to run tests related to changed files:</p> <pre><code># Run tests for files changed compared to the main branch\npython -m tests.utils.test_selection --changed\n\n# Run tests for files changed compared to a specific branch\npython -m tests.utils.test_selection --changed --base-branch develop\n\n# Include related test files\npython -m tests.utils.test_selection --changed --include-related\n</code></pre>"},{"location":"docs/testing/test_selection/#command-line-interface","title":"Command-Line Interface","text":"<p>The <code>test_selection.py</code> module provides a command-line interface for running selected tests:</p> <pre><code># Run tests with specific markers\npython -m tests.utils.test_selection --marker unit --marker config\n\n# Run tests with specific keywords\npython -m tests.utils.test_selection --keyword parse --keyword validate\n\n# Exclude tests with specific markers\npython -m tests.utils.test_selection --exclude-marker slow\n\n# Exclude tests with specific keywords\npython -m tests.utils.test_selection --exclude-keyword error\n\n# Run specific test files\npython -m tests.utils.test_selection tests/unit/test_config.py tests/unit/test_parser.py\n\n# Pass additional arguments to pytest\npython -m tests.utils.test_selection --marker unit -- -v --no-header\n</code></pre>"},{"location":"docs/testing/test_selection/#programmatic-usage","title":"Programmatic Usage","text":"<p>You can also use the test selection utilities programmatically in your scripts:</p> <pre><code>from tests.utils.test_selection import run_selected_tests, run_tests_for_changed_files\n\n# Run tests with specific markers\nrun_selected_tests(markers=[\"unit\", \"config\"])\n\n# Run tests with specific keywords\nrun_selected_tests(keywords=[\"parse\", \"validate\"])\n\n# Run tests for changed files\nrun_tests_for_changed_files(base_branch=\"main\", include_related=True)\n</code></pre>"},{"location":"docs/testing/test_selection/#utility-functions","title":"Utility Functions","text":"<p>The <code>test_selection.py</code> module provides several utility functions:</p> <ul> <li><code>get_changed_files(base_branch)</code>: Get a list of files changed compared to the base branch</li> <li><code>get_related_test_files(changed_files)</code>: Get a list of test files related to the changed files</li> <li><code>get_tests_by_marker(marker)</code>: Get a list of test files that have the specified marker</li> <li><code>get_tests_by_keyword(keyword)</code>: Get a list of test files that match the specified keyword</li> <li><code>get_slow_tests(threshold_seconds)</code>: Get a list of slow tests based on previous test runs</li> <li><code>create_test_selection_args(...)</code>: Create pytest command-line arguments for test selection</li> <li><code>run_selected_tests(...)</code>: Run selected tests based on the specified criteria</li> <li><code>run_tests_for_changed_files(...)</code>: Run tests for changed files compared to the base branch</li> </ul>"},{"location":"docs/testing/test_selection/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Markers Consistently: Apply markers consistently to ensure tests can be properly categorized and selected.</p> </li> <li> <p>Name Tests Descriptively: Use descriptive test names that include relevant keywords for easy filtering.</p> </li> <li> <p>Group Related Tests: Keep related tests in the same file or directory to make it easier to run them together.</p> </li> <li> <p>Mark Slow Tests: Always mark tests that take a long time to run with the <code>@pytest.mark.slow</code> decorator.</p> </li> <li> <p>Run Changed Tests First: When making changes, run the tests related to those changes first to get quick feedback.</p> </li> <li> <p>Use Test Categories: Use test categories (unit, integration, etc.) to organize and run tests at different levels of granularity.</p> </li> </ol>"},{"location":"docs/testing/test_selection/#examples","title":"Examples","text":""},{"location":"docs/testing/test_selection/#example-1-running-unit-tests-for-a-specific-module","title":"Example 1: Running Unit Tests for a Specific Module","text":"<pre><code># Run all unit tests for the config module\npytest -m unit tests/unit/config/\n</code></pre>"},{"location":"docs/testing/test_selection/#example-2-running-tests-related-to-a-feature","title":"Example 2: Running Tests Related to a Feature","text":"<pre><code># Run all tests related to the SARIF reporter\npytest -k sarif_reporter\n</code></pre>"},{"location":"docs/testing/test_selection/#example-3-running-tests-for-changed-files-in-ci","title":"Example 3: Running Tests for Changed Files in CI","text":"<pre><code># In a CI pipeline, run tests for files changed in the pull request\npython -m tests.utils.test_selection --changed --base-branch main --include-related\n</code></pre>"},{"location":"docs/testing/test_selection/#example-4-excluding-slow-tests-during-development","title":"Example 4: Excluding Slow Tests During Development","text":"<pre><code># Run all tests except slow tests\npytest -m \"not slow\"\n</code></pre>"},{"location":"docs/testing/test_selection/#example-5-running-tests-with-multiple-criteria","title":"Example 5: Running Tests with Multiple Criteria","text":"<pre><code># Run unit tests for the scanner module that are not slow\npytest -m \"unit and scanner and not slow\"\n</code></pre>"},{"location":"docs/testing/test_utilities/","title":"Test Utilities Guide","text":""},{"location":"docs/testing/test_utilities/#overview","title":"Overview","text":"<p>This document provides guidance on using the test utilities available in the ASH testing framework. These utilities are designed to make writing and maintaining tests easier, more consistent, and more effective.</p>"},{"location":"docs/testing/test_utilities/#assertion-utilities","title":"Assertion Utilities","text":"<p>Custom assertions are available in <code>tests.utils.assertions</code> to simplify common validation tasks.</p>"},{"location":"docs/testing/test_utilities/#sarif-report-assertions","title":"SARIF Report Assertions","text":"<pre><code>from tests.utils.assertions import assert_sarif_report_valid, assert_has_finding\n\ndef test_scanner_output(scanner_result):\n    # Validate that the SARIF report is well-formed\n    assert_sarif_report_valid(scanner_result.sarif_report)\n\n    # Check for specific findings\n    assert_has_finding(scanner_result.sarif_report,\n                      file_path=\"test.py\",\n                      message_pattern=\"Unsafe pickle usage\")\n\n    # Check for findings with specific properties\n    assert_has_finding(scanner_result.sarif_report,\n                      severity=\"HIGH\",\n                      rule_id=\"B301\")\n</code></pre>"},{"location":"docs/testing/test_utilities/#suppression-assertions","title":"Suppression Assertions","text":"<pre><code>from tests.utils.assertions import assert_finding_suppressed\n\ndef test_suppression(scanner_result, suppression_config):\n    # Check that a specific finding is suppressed\n    assert_finding_suppressed(scanner_result.sarif_report,\n                             file_path=\"test.py\",\n                             rule_id=\"B301\",\n                             suppression_config=suppression_config)\n</code></pre>"},{"location":"docs/testing/test_utilities/#custom-matchers","title":"Custom Matchers","text":"<pre><code>from tests.utils.assertions import assert_matches_pattern, assert_dict_contains\n\ndef test_with_pattern_matching():\n    # Check that a string matches a pattern\n    assert_matches_pattern(\"Error: File not found\", r\"Error: .* not found\")\n\n    # Check that a dictionary contains specific keys and values\n    assert_dict_contains({\"name\": \"bandit\", \"enabled\": True, \"options\": {\"level\": \"HIGH\"}},\n                        {\"name\": \"bandit\", \"enabled\": True})\n</code></pre>"},{"location":"docs/testing/test_utilities/#mocking-utilities","title":"Mocking Utilities","text":"<p>Mocking utilities are available in <code>tests.utils.mocks</code> to simplify creating mock objects for testing.</p>"},{"location":"docs/testing/test_utilities/#mock-sarif-reports","title":"Mock SARIF Reports","text":"<pre><code>from tests.utils.mocks import create_mock_sarif_report\n\ndef test_with_mock_sarif():\n    # Create a mock SARIF report with specific findings\n    mock_sarif = create_mock_sarif_report(\n        findings=[\n            {\n                \"file_path\": \"test.py\",\n                \"line\": 10,\n                \"message\": \"Unsafe pickle usage\",\n                \"severity\": \"HIGH\",\n                \"rule_id\": \"B301\"\n            },\n            {\n                \"file_path\": \"other.py\",\n                \"line\": 5,\n                \"message\": \"Weak hash algorithm\",\n                \"severity\": \"MEDIUM\",\n                \"rule_id\": \"B303\"\n            }\n        ]\n    )\n\n    # Use the mock SARIF report in tests\n    reporter = SarifReporter()\n    report = reporter.process_report(mock_sarif)\n\n    assert len(report.findings) == 2\n</code></pre>"},{"location":"docs/testing/test_utilities/#mock-scanner-plugins","title":"Mock Scanner Plugins","text":"<pre><code>from tests.utils.mocks import create_mock_scanner\n\ndef test_with_mock_scanner():\n    # Create a mock scanner with specific findings\n    mock_scanner = create_mock_scanner(\n        name=\"bandit\",\n        findings=[\n            {\n                \"file_path\": \"test.py\",\n                \"line\": 10,\n                \"message\": \"Unsafe pickle usage\",\n                \"severity\": \"HIGH\",\n                \"rule_id\": \"B301\"\n            }\n        ]\n    )\n\n    # Use the mock scanner in tests\n    result = mock_scanner.scan()\n    assert len(result.findings) == 1\n    assert result.findings[0].rule_id == \"B301\"\n</code></pre>"},{"location":"docs/testing/test_utilities/#mock-context-generators","title":"Mock Context Generators","text":"<pre><code>from tests.utils.mocks import create_mock_context\n\ndef test_with_mock_context():\n    # Create a mock context with specific properties\n    mock_context = create_mock_context(\n        config={\"scanners\": {\"bandit\": {\"enabled\": True}}},\n        work_dir=\"/tmp/test\",\n        output_dir=\"/tmp/test/output\"\n    )\n\n    # Use the mock context in tests\n    scanner = BanditScanner(context=mock_context)\n    assert scanner.is_enabled()\n</code></pre>"},{"location":"docs/testing/test_utilities/#test-data-utilities","title":"Test Data Utilities","text":"<p>Test data utilities are available in <code>tests.utils.test_data</code> to simplify managing test data.</p>"},{"location":"docs/testing/test_utilities/#test-data-factories","title":"Test Data Factories","text":"<pre><code>from tests.utils.test_data_factories import create_test_file, create_test_config\n\ndef test_with_generated_data():\n    # Create a test file with specific content\n    test_file = create_test_file(\n        file_path=\"test.py\",\n        content=\"import pickle\\npickle.loads(b'')\"\n    )\n\n    # Create a test configuration\n    test_config = create_test_config(\n        scanners={\"bandit\": {\"enabled\": True}}\n    )\n\n    # Use the test data in tests\n    scanner = BanditScanner(config=test_config)\n    result = scanner.scan_file(test_file)\n\n    assert len(result.findings) == 1\n</code></pre>"},{"location":"docs/testing/test_utilities/#test-data-loaders","title":"Test Data Loaders","text":"<pre><code>from tests.utils.test_data_loaders import load_test_data, load_test_config\n\ndef test_with_loaded_data():\n    # Load test data from a file\n    test_data = load_test_data(\"scanners/bandit/vulnerable_code.py\")\n\n    # Load a test configuration\n    test_config = load_test_config(\"scanners/bandit/config.yaml\")\n\n    # Use the loaded data in tests\n    test_file = create_test_file(\"test.py\", test_data)\n    scanner = BanditScanner(config=test_config)\n    result = scanner.scan_file(test_file)\n\n    assert len(result.findings) &gt; 0\n</code></pre>"},{"location":"docs/testing/test_utilities/#context-managers","title":"Context Managers","text":"<p>Context managers are available in <code>tests.utils.context_managers</code> to simplify managing test resources.</p>"},{"location":"docs/testing/test_utilities/#environment-variables","title":"Environment Variables","text":"<pre><code>from tests.utils.context_managers import environment_variable\n\ndef test_with_env_var():\n    # Set an environment variable for the duration of the test\n    with environment_variable(\"ASH_CONFIG_PATH\", \"/tmp/test/config.yaml\"):\n        # Code that uses the environment variable\n        config_path = os.environ.get(\"ASH_CONFIG_PATH\")\n        assert config_path == \"/tmp/test/config.yaml\"\n\n    # The environment variable is restored to its original value\n    assert \"ASH_CONFIG_PATH\" not in os.environ\n</code></pre>"},{"location":"docs/testing/test_utilities/#temporary-files-and-directories","title":"Temporary Files and Directories","text":"<pre><code>from tests.utils.context_managers import temp_file, temp_directory\n\ndef test_with_temp_file():\n    # Create a temporary file for the duration of the test\n    with temp_file(content=\"test content\") as file_path:\n        # Code that uses the temporary file\n        assert file_path.read_text() == \"test content\"\n\n    # The file is automatically deleted\n    assert not file_path.exists()\n\ndef test_with_temp_directory():\n    # Create a temporary directory for the duration of the test\n    with temp_directory() as dir_path:\n        # Code that uses the temporary directory\n        (dir_path / \"test.txt\").write_text(\"test content\")\n        assert (dir_path / \"test.txt\").exists()\n\n    # The directory is automatically deleted\n    assert not dir_path.exists()\n</code></pre>"},{"location":"docs/testing/test_utilities/#mocking-external-services","title":"Mocking External Services","text":"<pre><code>from tests.utils.context_managers import mock_subprocess_run\n\ndef test_with_mock_subprocess():\n    # Mock subprocess.run for the duration of the test\n    with mock_subprocess_run(return_value=subprocess.CompletedProcess(\n        args=[\"bandit\", \"-r\", \"test.py\"],\n        returncode=0,\n        stdout=\"No issues found.\",\n        stderr=\"\"\n    )):\n        # Code that calls subprocess.run\n        result = subprocess.run([\"bandit\", \"-r\", \"test.py\"], capture_output=True, text=True)\n        assert result.returncode == 0\n        assert result.stdout == \"No issues found.\"\n</code></pre>"},{"location":"docs/testing/test_utilities/#integration-test-utilities","title":"Integration Test Utilities","text":"<p>Integration test utilities are available in <code>tests.utils.integration_test_utils</code> to simplify setting up integration tests.</p>"},{"location":"docs/testing/test_utilities/#integration-test-environment","title":"Integration Test Environment","text":"<pre><code>from tests.utils.integration_test_utils import integration_test_environment\n\ndef test_end_to_end_scan():\n    with integration_test_environment() as env:\n        # Set up the test environment\n        env.create_config_file({\"scanners\": {\"bandit\": {\"enabled\": True}}})\n        env.create_source_file(\"src/main.py\", \"import pickle\\npickle.loads(b'')\")\n\n        # Run the command being tested\n        result = env.run_ash([\"scan\"])\n\n        # Verify the results\n        assert result.returncode == 0\n        assert \"pickle.loads\" in env.read_output_file(\"bandit_report.txt\")\n</code></pre>"},{"location":"docs/testing/test_utilities/#component-interaction-testing","title":"Component Interaction Testing","text":"<pre><code>from tests.utils.integration_test_utils import component_interaction_tester\n\ndef test_scanner_reporter_interaction():\n    with component_interaction_tester() as tester:\n        # Register components for testing\n        scanner = tester.register_component(\"scanner\", BanditScanner)\n        reporter = tester.register_component(\"reporter\", SarifReporter)\n\n        # Execute the interaction\n        scanner.scan()\n        reporter.report(scanner.results)\n\n        # Verify the interaction\n        assert tester.verify_interaction(\"scanner\", \"reporter\", \"report\")\n</code></pre>"},{"location":"docs/testing/test_utilities/#integration-point-verification","title":"Integration Point Verification","text":"<pre><code>from tests.utils.integration_test_utils import integration_test_verifier\n\ndef test_integration_points():\n    with integration_test_verifier() as verifier:\n        # Register integration points to verify\n        verifier.register_integration_point(\n            name=\"scan-report\",\n            source=\"scanner\",\n            target=\"reporter\",\n            interface=[\"report\"]\n        )\n\n        # Set up the test\n        with component_interaction_tester() as tester:\n            scanner = tester.register_component(\"scanner\", BanditScanner)\n            reporter = tester.register_component(\"reporter\", SarifReporter)\n\n            # Execute the interaction\n            scanner.scan()\n            reporter.report(scanner.results)\n\n            # Verify all integration points\n            assert verifier.verify_all(tester)\n</code></pre>"},{"location":"docs/testing/test_utilities/#resource-management","title":"Resource Management","text":"<p>Resource management utilities are available in <code>tests.utils.resource_management</code> to simplify managing test resources.</p>"},{"location":"docs/testing/test_utilities/#temporary-resources","title":"Temporary Resources","text":"<pre><code>from tests.utils.resource_management import temp_directory, temp_file\n\ndef test_with_temp_resources():\n    with temp_directory() as temp_dir:\n        # Use the temporary directory\n        config_file = temp_dir / \"config.yaml\"\n        config_file.write_text(\"scanners:\\n  bandit:\\n    enabled: true\")\n\n        with temp_file(suffix=\".py\", content=\"import pickle\\npickle.loads(b'')\") as temp_file_path:\n            # Use the temporary file\n            scanner = BanditScanner(config_file=config_file)\n            result = scanner.scan_file(temp_file_path)\n\n            assert len(result.findings) == 1\n</code></pre>"},{"location":"docs/testing/test_utilities/#process-management","title":"Process Management","text":"<pre><code>from tests.utils.resource_management import managed_process\n\ndef test_with_external_process():\n    with temp_directory() as temp_dir:\n        # Set up the test environment\n        config_file = temp_dir / \"config.yaml\"\n        config_file.write_text(\"scanners:\\n  bandit:\\n    enabled: true\")\n\n        # Start a process for the duration of the test\n        with managed_process([\"python\", \"-m\", \"http.server\"], cwd=temp_dir) as process:\n            # Test code that interacts with the HTTP server\n            # The process will be automatically terminated when the context exits\n            pass\n</code></pre>"},{"location":"docs/testing/test_utilities/#service-management","title":"Service Management","text":"<pre><code>from tests.utils.resource_management import managed_service\n\ndef test_with_external_service():\n    # Define a function to check if the service is ready\n    def is_ready():\n        try:\n            with socket.create_connection((\"localhost\", 8000), timeout=1):\n                return True\n        except:\n            return False\n\n    # Start a service for the duration of the test\n    with managed_service(\n        name=\"http-server\",\n        command=[\"python\", \"-m\", \"http.server\"],\n        ready_check=is_ready\n    ) as process:\n        # Test code that interacts with the service\n        # The service will be automatically stopped when the context exits\n        pass\n</code></pre>"},{"location":"docs/testing/test_utilities/#external-service-mocks","title":"External Service Mocks","text":"<p>Mock external services are available in <code>tests.utils.external_service_mocks</code> to simplify testing code that interacts with external services.</p>"},{"location":"docs/testing/test_utilities/#mock-http-server","title":"Mock HTTP Server","text":"<pre><code>from tests.utils.external_service_mocks import mock_http_server\n\ndef test_with_mock_http_server():\n    with mock_http_server() as server:\n        # Add files to the server\n        server.add_file(\"test.json\", {\"key\": \"value\"})\n\n        # Get the URL for a file\n        url = server.get_url(\"test.json\")\n\n        # Test code that interacts with the HTTP server\n        response = requests.get(url)\n        assert response.json() == {\"key\": \"value\"}\n</code></pre>"},{"location":"docs/testing/test_utilities/#mock-api-server","title":"Mock API Server","text":"<pre><code>from tests.utils.external_service_mocks import mock_api_server\n\ndef test_with_mock_api_server():\n    with mock_api_server() as server:\n        # Define a route handler\n        def handle_hello(method, path, query, headers, body):\n            return 200, {\"Content-Type\": \"application/json\"}, {\"message\": \"Hello, world!\"}\n\n        # Add a route to the server\n        server.add_route(\"/hello\", handle_hello)\n\n        # Get the URL for the route\n        url = server.get_url(\"hello\")\n\n        # Test code that interacts with the API server\n        response = requests.get(url)\n        assert response.json() == {\"message\": \"Hello, world!\"}\n</code></pre>"},{"location":"docs/testing/test_utilities/#mock-file-server","title":"Mock File Server","text":"<pre><code>from tests.utils.external_service_mocks import mock_file_server\n\ndef test_with_mock_file_server():\n    with mock_file_server() as server:\n        # Add files to the server\n        server.add_file(\"test.json\", {\"key\": \"value\"})\n\n        # Get the path to a file\n        path = server.get_file_path(\"test.json\")\n\n        # Test code that interacts with the file server\n        with open(path, \"r\") as f:\n            data = json.load(f)\n            assert data == {\"key\": \"value\"}\n</code></pre>"},{"location":"docs/testing/test_utilities/#best-practices","title":"Best Practices","text":"<ol> <li>Use the right utility for the job: Choose the appropriate utility based on what you're testing.</li> <li>Clean up resources: Use context managers to ensure resources are cleaned up properly.</li> <li>Isolate tests: Use mocks and fixtures to isolate tests from external dependencies.</li> <li>Keep tests fast: Use mocks instead of real external services when possible.</li> <li>Make tests readable: Use descriptive variable names and comments to explain what the test is doing.</li> <li>Test edge cases: Use utilities to create test data that covers edge cases.</li> <li>Reuse test code: Create helper functions for common test patterns.</li> <li>Document utilities: Add docstrings to explain how to use utilities.</li> </ol>"},{"location":"docs/testing/test_utilities/#example-comprehensive-test","title":"Example: Comprehensive Test","text":"<pre><code>import pytest\nfrom pathlib import Path\nfrom tests.utils.assertions import assert_sarif_report_valid, assert_has_finding\nfrom tests.utils.mocks import create_mock_scanner\nfrom tests.utils.test_data_factories import create_test_file\nfrom tests.utils.context_managers import environment_variable\nfrom tests.utils.integration_test_utils import integration_test_environment\n\n# Unit test with mocks\n@pytest.mark.unit\n@pytest.mark.reporter\ndef test_reporter_with_mock_scanner():\n    # Create a mock scanner with specific findings\n    mock_scanner = create_mock_scanner(\n        name=\"bandit\",\n        findings=[\n            {\n                \"file_path\": \"test.py\",\n                \"line\": 10,\n                \"message\": \"Unsafe pickle usage\",\n                \"severity\": \"HIGH\",\n                \"rule_id\": \"B301\"\n            }\n        ]\n    )\n\n    # Use the mock scanner in tests\n    reporter = SarifReporter()\n    report = reporter.generate_report(mock_scanner.scan())\n\n    # Verify the report\n    assert_sarif_report_valid(report)\n    assert_has_finding(report, file_path=\"test.py\", rule_id=\"B301\")\n\n# Integration test with environment\n@pytest.mark.integration\n@pytest.mark.scanner\n@pytest.mark.reporter\ndef test_end_to_end_scan():\n    with integration_test_environment() as env:\n        # Set up the test environment\n        env.create_config_file({\"scanners\": {\"bandit\": {\"enabled\": True}}})\n        env.create_source_file(\"src/main.py\", \"import pickle\\npickle.loads(b'')\")\n\n        # Set environment variables\n        with environment_variable(\"ASH_DEBUG\", \"true\"):\n            # Run the command being tested\n            result = env.run_ash([\"scan\"])\n\n            # Verify the results\n            assert result.returncode == 0\n            assert \"pickle.loads\" in env.read_output_file(\"bandit_report.txt\")\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/","title":"Writing Effective Tests Guide","text":""},{"location":"docs/testing/writing_effective_tests/#overview","title":"Overview","text":"<p>This document provides guidelines for writing effective tests for the ASH project. Following these guidelines will help ensure that tests are reliable, maintainable, and provide good coverage of the codebase.</p>"},{"location":"docs/testing/writing_effective_tests/#principles-of-effective-testing","title":"Principles of Effective Testing","text":""},{"location":"docs/testing/writing_effective_tests/#1-test-one-thing-at-a-time","title":"1. Test One Thing at a Time","text":"<p>Each test should focus on testing a single functionality or behavior. This makes tests easier to understand, maintain, and debug.</p> <p>Good Example: <pre><code>def test_bandit_scanner_initialization():\n    scanner = BanditScanner()\n    assert scanner.name == \"bandit\"\n    assert scanner.is_enabled()\n\ndef test_bandit_scanner_scan_python_file(temp_python_file):\n    temp_python_file.write_text(\"import pickle\\npickle.loads(b'')\")\n    scanner = BanditScanner()\n    result = scanner.scan_file(temp_python_file)\n    assert len(result.findings) == 1\n</code></pre></p> <p>Bad Example: <pre><code>def test_bandit_scanner():\n    # Tests too many things in one test\n    scanner = BanditScanner()\n    assert scanner.name == \"bandit\"\n    assert scanner.is_enabled()\n\n    temp_file = Path(\"/tmp/test.py\")\n    temp_file.write_text(\"import pickle\\npickle.loads(b'')\")\n    result = scanner.scan_file(temp_file)\n    assert len(result.findings) == 1\n\n    # More tests...\n</code></pre></p>"},{"location":"docs/testing/writing_effective_tests/#2-use-descriptive-test-names","title":"2. Use Descriptive Test Names","text":"<p>Test names should clearly describe what is being tested. This makes it easier to understand what a test is doing and what failed when a test fails.</p> <p>Good Example: <pre><code>def test_bandit_scanner_finds_unsafe_pickle_usage():\n    # Test code here\n    pass\n\ndef test_bandit_scanner_ignores_safe_code():\n    # Test code here\n    pass\n</code></pre></p> <p>Bad Example: <pre><code>def test_scanner_1():\n    # Test code here\n    pass\n\ndef test_scanner_2():\n    # Test code here\n    pass\n</code></pre></p>"},{"location":"docs/testing/writing_effective_tests/#3-follow-the-aaa-pattern","title":"3. Follow the AAA Pattern","text":"<p>Tests should follow the Arrange-Act-Assert (AAA) pattern:</p> <ol> <li>Arrange: Set up the test environment and inputs</li> <li>Act: Execute the code being tested</li> <li>Assert: Verify the results</li> </ol> <p>This makes tests easier to read and understand.</p> <p>Good Example: <pre><code>def test_bandit_scanner_findings(temp_project_dir):\n    # Arrange\n    test_file = temp_project_dir / \"test.py\"\n    test_file.write_text(\"import pickle\\npickle.loads(b'')\")\n    scanner = BanditScanner()\n\n    # Act\n    result = scanner.scan_file(test_file)\n\n    # Assert\n    assert len(result.findings) == 1\n    assert \"pickle.loads\" in result.findings[0].message\n</code></pre></p>"},{"location":"docs/testing/writing_effective_tests/#4-use-fixtures-for-setup-and-teardown","title":"4. Use Fixtures for Setup and Teardown","text":"<p>Use fixtures to set up test environments and clean up after tests. This reduces code duplication and ensures proper cleanup.</p> <p>Good Example: <pre><code>@pytest.fixture\ndef temp_config():\n    config_file = Path(tempfile.mktemp())\n    config_file.write_text(\"scanners:\\n  bandit:\\n    enabled: true\")\n    yield config_file\n    config_file.unlink()\n\ndef test_with_config(temp_config):\n    scanner = BanditScanner(config_file=temp_config)\n    assert scanner.is_enabled()\n</code></pre></p>"},{"location":"docs/testing/writing_effective_tests/#5-mock-external-dependencies","title":"5. Mock External Dependencies","text":"<p>Use mocks to isolate the code being tested from external dependencies. This makes tests faster, more reliable, and focused on the code being tested.</p> <p>Good Example: <pre><code>def test_scanner_with_mock_subprocess(mocker):\n    # Mock subprocess.run to return a predefined result\n    mock_run = mocker.patch(\"subprocess.run\")\n    mock_run.return_value = subprocess.CompletedProcess(\n        args=[\"bandit\", \"-r\", \"test.py\"],\n        returncode=0,\n        stdout=\"No issues found.\",\n        stderr=\"\"\n    )\n\n    scanner = BanditScanner()\n    result = scanner.scan_file(Path(\"test.py\"))\n\n    assert len(result.findings) == 0\n    mock_run.assert_called_once()\n</code></pre></p>"},{"location":"docs/testing/writing_effective_tests/#6-test-edge-cases","title":"6. Test Edge Cases","text":"<p>Test boundary conditions and error cases to ensure the code handles them correctly.</p> <p>Good Example: <pre><code>@pytest.mark.parametrize(\"input_value,expected_error\", [\n    (None, TypeError),\n    (\"\", ValueError),\n    (\"/nonexistent/file.py\", FileNotFoundError),\n])\ndef test_scanner_with_invalid_input(input_value, expected_error):\n    scanner = BanditScanner()\n    with pytest.raises(expected_error):\n        scanner.scan_file(input_value)\n</code></pre></p>"},{"location":"docs/testing/writing_effective_tests/#7-keep-tests-independent","title":"7. Keep Tests Independent","text":"<p>Tests should not depend on the state created by other tests. Each test should be able to run independently.</p> <p>Good Example: <pre><code>def test_scanner_1(temp_project_dir):\n    # Test code here using temp_project_dir\n    pass\n\ndef test_scanner_2(temp_project_dir):\n    # Test code here using a fresh temp_project_dir\n    pass\n</code></pre></p> <p>Bad Example: <pre><code># Global state that tests depend on\nTEMP_DIR = Path(\"/tmp/test\")\nTEMP_DIR.mkdir(exist_ok=True)\n\ndef test_scanner_1():\n    # Creates files that test_scanner_2 depends on\n    (TEMP_DIR / \"test.py\").write_text(\"import pickle\\npickle.loads(b'')\")\n    # Test code here\n    pass\n\ndef test_scanner_2():\n    # Depends on files created by test_scanner_1\n    # This test will fail if test_scanner_1 is not run first\n    assert (TEMP_DIR / \"test.py\").exists()\n    # Test code here\n    pass\n</code></pre></p>"},{"location":"docs/testing/writing_effective_tests/#8-use-parameterized-tests","title":"8. Use Parameterized Tests","text":"<p>Use <code>@pytest.mark.parametrize</code> to test multiple inputs with the same test function. This reduces code duplication and ensures consistent testing across different inputs.</p> <p>Good Example: <pre><code>@pytest.mark.parametrize(\"code,expected_findings\", [\n    (\"import pickle\\npickle.loads(b'')\", 1),  # Unsafe pickle usage\n    (\"import hashlib\\nhashlib.md5(b'')\", 1),  # Weak hash algorithm\n    (\"print('Hello, world!')\", 0),  # No security issues\n])\ndef test_bandit_scanner_findings(temp_python_file, code, expected_findings):\n    # Arrange\n    temp_python_file.write_text(code)\n    scanner = BanditScanner()\n\n    # Act\n    result = scanner.scan_file(temp_python_file)\n\n    # Assert\n    assert len(result.findings) == expected_findings\n</code></pre></p>"},{"location":"docs/testing/writing_effective_tests/#test-structure","title":"Test Structure","text":""},{"location":"docs/testing/writing_effective_tests/#unit-tests","title":"Unit Tests","text":"<p>Unit tests should focus on testing a single unit of code in isolation. They should be fast, reliable, and independent of external dependencies.</p> <pre><code>import pytest\nfrom automated_security_helper.scanners.bandit_scanner import BanditScanner\n\n@pytest.mark.unit\n@pytest.mark.scanner\nclass TestBanditScanner:\n    def test_initialization(self):\n        scanner = BanditScanner()\n        assert scanner.name == \"bandit\"\n        assert scanner.is_enabled()\n\n    def test_scan_python_file(self, temp_python_file, mocker):\n        # Mock subprocess.run to return a predefined result\n        mock_run = mocker.patch(\"subprocess.run\")\n        mock_run.return_value = subprocess.CompletedProcess(\n            args=[\"bandit\", \"-r\", \"test.py\"],\n            returncode=0,\n            stdout=json.dumps({\n                \"results\": [\n                    {\n                        \"filename\": \"test.py\",\n                        \"line\": 1,\n                        \"issue_text\": \"Unsafe pickle usage\",\n                        \"issue_severity\": \"HIGH\",\n                        \"issue_confidence\": \"HIGH\",\n                        \"issue_cwe\": \"CWE-502\",\n                        \"test_id\": \"B301\"\n                    }\n                ]\n            }),\n            stderr=\"\"\n        )\n\n        # Arrange\n        temp_python_file.write_text(\"import pickle\\npickle.loads(b'')\")\n        scanner = BanditScanner()\n\n        # Act\n        result = scanner.scan_file(temp_python_file)\n\n        # Assert\n        assert len(result.findings) == 1\n        assert result.findings[0].file_path == \"test.py\"\n        assert result.findings[0].line == 1\n        assert \"Unsafe pickle usage\" in result.findings[0].message\n        assert result.findings[0].severity == \"HIGH\"\n        assert result.findings[0].rule_id == \"B301\"\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#integration-tests","title":"Integration Tests","text":"<p>Integration tests should focus on testing interactions between components. They verify that components work together correctly.</p> <pre><code>import pytest\nfrom automated_security_helper.scanners.bandit_scanner import BanditScanner\nfrom automated_security_helper.reporters.sarif_reporter import SarifReporter\n\n@pytest.mark.integration\n@pytest.mark.scanner\n@pytest.mark.reporter\ndef test_scanner_reporter_integration(temp_project_dir):\n    # Arrange\n    test_file = temp_project_dir / \"test.py\"\n    test_file.write_text(\"import pickle\\npickle.loads(b'')\")\n\n    scanner = BanditScanner()\n    reporter = SarifReporter()\n\n    # Act\n    scan_result = scanner.scan_file(test_file)\n    report = reporter.generate_report(scan_result)\n\n    # Assert\n    assert len(report[\"runs\"][0][\"results\"]) == 1\n    assert report[\"runs\"][0][\"results\"][0][\"locations\"][0][\"physicalLocation\"][\"artifactLocation\"][\"uri\"] == \"test.py\"\n    assert \"Unsafe pickle usage\" in report[\"runs\"][0][\"results\"][0][\"message\"][\"text\"]\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#end-to-end-tests","title":"End-to-End Tests","text":"<p>End-to-end tests should focus on testing complete workflows from start to finish. They verify that the system works correctly as a whole.</p> <pre><code>import pytest\nfrom tests.utils.integration_test_utils import integration_test_environment\n\n@pytest.mark.integration\n@pytest.mark.slow\ndef test_end_to_end_scan():\n    with integration_test_environment() as env:\n        # Set up the test environment\n        env.create_config_file({\"scanners\": {\"bandit\": {\"enabled\": True}}})\n        env.create_source_file(\"src/main.py\", \"import pickle\\npickle.loads(b'')\")\n\n        # Run the command being tested\n        result = env.run_ash([\"scan\"])\n\n        # Verify the results\n        assert result.returncode == 0\n        assert \"pickle.loads\" in env.read_output_file(\"bandit_report.txt\")\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#test-coverage","title":"Test Coverage","text":""},{"location":"docs/testing/writing_effective_tests/#what-to-test","title":"What to Test","text":"<ol> <li>Public API: Test all public methods and functions.</li> <li>Edge Cases: Test boundary conditions and error cases.</li> <li>Complex Logic: Test complex logic with multiple paths.</li> <li>Bug Fixes: Write tests for bug fixes to prevent regressions.</li> </ol>"},{"location":"docs/testing/writing_effective_tests/#what-not-to-test","title":"What Not to Test","text":"<ol> <li>Private Methods: Focus on testing the public API, not implementation details.</li> <li>External Libraries: Assume external libraries work correctly.</li> <li>Simple Getters/Setters: Don't test trivial code.</li> <li>Generated Code: Don't test code that is generated by tools.</li> </ol>"},{"location":"docs/testing/writing_effective_tests/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Line Coverage: Aim for at least 80% line coverage.</li> <li>Branch Coverage: Aim for at least 80% branch coverage.</li> <li>Critical Components: Aim for 100% coverage of critical components.</li> </ul>"},{"location":"docs/testing/writing_effective_tests/#common-testing-patterns","title":"Common Testing Patterns","text":""},{"location":"docs/testing/writing_effective_tests/#testing-functions","title":"Testing Functions","text":"<pre><code>def test_function_name():\n    # Arrange\n    input_value = \"test input\"\n    expected_output = \"expected output\"\n\n    # Act\n    actual_output = function_name(input_value)\n\n    # Assert\n    assert actual_output == expected_output\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#testing-classes","title":"Testing Classes","text":"<pre><code>class TestClassName:\n    def test_initialization(self):\n        # Test initialization\n        instance = ClassName(param1=\"value1\")\n        assert instance.param1 == \"value1\"\n\n    def test_method_name(self):\n        # Test a method\n        instance = ClassName()\n        result = instance.method_name(\"input\")\n        assert result == \"expected output\"\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#testing-exceptions","title":"Testing Exceptions","text":"<pre><code>def test_function_raises_exception():\n    with pytest.raises(ValueError) as excinfo:\n        function_that_raises()\n\n    assert \"Expected error message\" in str(excinfo.value)\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#testing-asynchronous-code","title":"Testing Asynchronous Code","text":"<pre><code>@pytest.mark.asyncio\nasync def test_async_function():\n    # Arrange\n    input_value = \"test input\"\n    expected_output = \"expected output\"\n\n    # Act\n    actual_output = await async_function(input_value)\n\n    # Assert\n    assert actual_output == expected_output\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#testing-anti-patterns","title":"Testing Anti-Patterns","text":""},{"location":"docs/testing/writing_effective_tests/#1-slow-tests","title":"1. Slow Tests","text":"<p>Slow tests discourage frequent testing and slow down development. Keep tests fast by:</p> <ul> <li>Mocking external dependencies</li> <li>Using in-memory databases instead of real databases</li> <li>Focusing on unit tests over integration tests</li> <li>Marking slow tests with <code>@pytest.mark.slow</code></li> </ul>"},{"location":"docs/testing/writing_effective_tests/#2-flaky-tests","title":"2. Flaky Tests","text":"<p>Flaky tests that sometimes pass and sometimes fail reduce confidence in the test suite. Avoid flaky tests by:</p> <ul> <li>Avoiding race conditions</li> <li>Not depending on timing</li> <li>Not depending on external services</li> <li>Using deterministic test data</li> <li>Isolating tests from each other</li> </ul>"},{"location":"docs/testing/writing_effective_tests/#3-overspecified-tests","title":"3. Overspecified Tests","text":"<p>Tests that are too tightly coupled to implementation details make refactoring difficult. Avoid overspecified tests by:</p> <ul> <li>Testing behavior, not implementation</li> <li>Using black-box testing</li> <li>Focusing on inputs and outputs</li> <li>Not testing private methods directly</li> </ul>"},{"location":"docs/testing/writing_effective_tests/#4-incomplete-tests","title":"4. Incomplete Tests","text":"<p>Tests that don't cover all important cases can give a false sense of security. Avoid incomplete tests by:</p> <ul> <li>Testing edge cases</li> <li>Testing error cases</li> <li>Using parameterized tests</li> <li>Checking coverage reports</li> </ul>"},{"location":"docs/testing/writing_effective_tests/#debugging-tests","title":"Debugging Tests","text":""},{"location":"docs/testing/writing_effective_tests/#1-use-verbose-output","title":"1. Use Verbose Output","text":"<p>Run tests with verbose output to see more details:</p> <pre><code>pytest -v\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#2-use-the-debugger","title":"2. Use the Debugger","text":"<p>Drop into the debugger on test failures:</p> <pre><code>pytest --pdb\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#3-use-print-statements","title":"3. Use Print Statements","text":"<p>Add print statements to see what's happening during test execution:</p> <pre><code>def test_function():\n    result = function_being_tested()\n    print(f\"Result: {result}\")\n    assert result == expected_result\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#4-isolate-the-problem","title":"4. Isolate the Problem","text":"<p>Run only the failing test to isolate the problem:</p> <pre><code>pytest path/to/test_file.py::test_function -v\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#5-check-test-dependencies","title":"5. Check Test Dependencies","text":"<p>Make sure tests don't depend on each other:</p> <pre><code>pytest --random-order\n</code></pre>"},{"location":"docs/testing/writing_effective_tests/#continuous-integration","title":"Continuous Integration","text":""},{"location":"docs/testing/writing_effective_tests/#1-run-tests-on-every-commit","title":"1. Run Tests on Every Commit","text":"<p>Configure CI to run tests on every commit to catch issues early.</p>"},{"location":"docs/testing/writing_effective_tests/#2-run-all-tests","title":"2. Run All Tests","text":"<p>Run all tests, including slow and integration tests, in CI.</p>"},{"location":"docs/testing/writing_effective_tests/#3-check-coverage","title":"3. Check Coverage","text":"<p>Generate coverage reports in CI to ensure coverage doesn't decrease.</p>"},{"location":"docs/testing/writing_effective_tests/#4-fail-fast","title":"4. Fail Fast","text":"<p>Configure CI to fail as soon as a test fails to get faster feedback.</p>"},{"location":"docs/testing/writing_effective_tests/#conclusion","title":"Conclusion","text":"<p>Writing effective tests is an investment in the quality and maintainability of the codebase. By following these guidelines, you can create tests that are reliable, maintainable, and provide good coverage of the codebase.</p> <p>Remember that the goal of testing is not just to catch bugs, but also to:</p> <ul> <li>Document how the code is supposed to work</li> <li>Make it safer to refactor code</li> <li>Provide confidence that changes don't break existing functionality</li> <li>Help design better code by making it testable</li> </ul> <p>By writing effective tests, you contribute to the long-term health and success of the project.</p>"},{"location":"tutorials/mcp-streaming-guide/","title":"ASH MCP Streaming Results Guide","text":"<p>This guide explains how to use ASH's streaming functionality through the Model Context Protocol (MCP) server for real-time progress tracking during security scans.</p>"},{"location":"tutorials/mcp-streaming-guide/#overview","title":"Overview","text":"<p>The streaming functionality provides real-time progress updates during ASH security scans. Instead of waiting for a complete scan to finish, you can now:</p> <ul> <li>Start scans in the background</li> <li>Monitor progress in real-time</li> <li>Get partial results as they become available</li> <li>Manage multiple concurrent scans</li> <li>Cancel running scans</li> </ul> <p>This is especially useful for large codebases where scans can take several minutes to complete.</p>"},{"location":"tutorials/mcp-streaming-guide/#how-it-works","title":"How It Works","text":"<p>ASH's MCP server uses an event-driven system to track scan progress. When you start a scan with progress tracking, ASH:</p> <ol> <li>Starts the scan in the background</li> <li>Returns a unique scan ID immediately</li> <li>Publishes progress events as the scan proceeds</li> <li>Allows you to check progress and get partial results</li> <li>Provides final results when the scan completes</li> </ol>"},{"location":"tutorials/mcp-streaming-guide/#available-commands","title":"Available Commands","text":"<p>When using ASH through an MCP-compatible AI assistant, you can use these streaming capabilities:</p>"},{"location":"tutorials/mcp-streaming-guide/#starting-a-streaming-scan","title":"Starting a Streaming Scan","text":"<pre><code>\"Start a security scan with progress tracking on this directory\"\n\"Begin a background security scan and show me updates as it runs\"\n</code></pre> <p>This uses the <code>scan_directory_with_progress</code> tool, which returns immediately with a scan ID.</p>"},{"location":"tutorials/mcp-streaming-guide/#monitoring-progress","title":"Monitoring Progress","text":"<pre><code>\"Show me the progress of my security scan\"\n\"What's the current status of scan [scan-id]?\"\n\"How many security issues have been found so far?\"\n</code></pre> <p>This uses the <code>get_scan_progress</code> tool to show: - Overall progress percentage - Current scanning phase - Which scanner is currently running - Number of findings discovered so far - Estimated time remaining</p>"},{"location":"tutorials/mcp-streaming-guide/#managing-multiple-scans","title":"Managing Multiple Scans","text":"<pre><code>\"Show me all my active security scans\"\n\"List the status of all running scans\"\n</code></pre> <p>This uses the <code>list_active_scans</code> tool to display all concurrent scans and their status.</p>"},{"location":"tutorials/mcp-streaming-guide/#canceling-scans","title":"Canceling Scans","text":"<pre><code>\"Cancel the security scan on the backend directory\"\n\"Stop scan [scan-id]\"\n</code></pre> <p>This uses the <code>cancel_scan</code> tool to stop a running scan and clean up resources.</p>"},{"location":"tutorials/mcp-streaming-guide/#getting-final-results","title":"Getting Final Results","text":"<pre><code>\"Get the final results for my completed security scan\"\n\"Show me the detailed findings from scan [scan-id]\"\n</code></pre> <p>This uses the <code>get_scan_results</code> tool to retrieve comprehensive results once a scan completes.</p>"},{"location":"tutorials/mcp-streaming-guide/#progress-phases","title":"Progress Phases","text":"<p>Your security scan progresses through these phases:</p> <ol> <li>Initializing (0%): Setting up the scan and validating parameters</li> <li>Running (0-25%): Starting the scan execution</li> <li>Scanning (25-85%): Individual security scanners are running</li> <li>detect-secrets (finding hardcoded secrets)</li> <li>bandit (Python security issues)</li> <li>semgrep (code patterns and vulnerabilities)</li> <li>checkov (infrastructure security)</li> <li>And others based on your project type</li> <li>Generating Reports (85-95%): Creating output files and summaries</li> <li>Finished (100%): Scan completed successfully</li> </ol>"},{"location":"tutorials/mcp-streaming-guide/#real-time-updates","title":"Real-Time Updates","text":"<p>During the scanning phase, you'll see updates like:</p> <ul> <li>\"Running bandit scanner...\" (Python security analysis)</li> <li>\"Running semgrep scanner...\" (Pattern-based vulnerability detection)</li> <li>\"Running detect-secrets scanner...\" (Credential scanning)</li> <li>\"Found 3 new security issues in current file...\"</li> <li>\"Completed infrastructure security checks...\"</li> </ul>"},{"location":"tutorials/mcp-streaming-guide/#example-workflow","title":"Example Workflow","text":"<p>Here's a typical workflow using streaming scans:</p> <ol> <li> <p>Start the scan: <pre><code>\"Start a security scan with progress tracking on my project\"\n</code></pre>    Response: \"Started security scan with ID abc-123. I'll monitor the progress for you.\"</p> </li> <li> <p>Monitor progress: <pre><code>\"How is my security scan progressing?\"\n</code></pre>    Response: \"Your scan is 45% complete. Currently running semgrep scanner. Found 2 security issues so far.\"</p> </li> <li> <p>Check multiple scans: <pre><code>\"Show me all my active scans\"\n</code></pre>    Response: \"You have 2 active scans: Project A (75% complete), Project B (30% complete).\"</p> </li> <li> <p>Get final results: <pre><code>\"My scan finished - show me the results\"\n</code></pre>    Response: \"Scan completed! Found 5 security issues: 2 high severity, 3 medium severity. Here are the details...\"</p> </li> </ol>"},{"location":"tutorials/mcp-streaming-guide/#benefits","title":"Benefits","text":""},{"location":"tutorials/mcp-streaming-guide/#for-large-projects","title":"For Large Projects","text":"<ul> <li>No need to wait for long scans to complete</li> <li>See progress and early findings immediately</li> <li>Can work on other tasks while scans run</li> </ul>"},{"location":"tutorials/mcp-streaming-guide/#for-multiple-projects","title":"For Multiple Projects","text":"<ul> <li>Run scans on different directories simultaneously</li> <li>Track progress across all your projects</li> <li>Prioritize which results to review first</li> </ul>"},{"location":"tutorials/mcp-streaming-guide/#for-development-workflow","title":"For Development Workflow","text":"<ul> <li>Start scans when you begin code review</li> <li>Get immediate feedback on critical issues</li> <li>Cancel scans if you need to make changes</li> </ul>"},{"location":"tutorials/mcp-streaming-guide/#configuration","title":"Configuration","text":"<p>The streaming functionality works with all your existing ASH configurations:</p> <ul> <li>Severity thresholds: Set minimum severity levels</li> <li>Custom scanners: Enable/disable specific security tools</li> <li>Ignore patterns: Skip certain files or directories</li> <li>Output formats: Choose how results are presented</li> </ul> <p>Example requests: <pre><code>\"Start a high-severity security scan with progress tracking\"\n\"Scan this directory but ignore the test files, and show me progress\"\n\"Run only the secrets scanner with streaming updates\"\n</code></pre></p>"},{"location":"tutorials/mcp-streaming-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/mcp-streaming-guide/#scan-stuck-or-taking-too-long","title":"Scan Stuck or Taking Too Long","text":"<pre><code>\"Cancel my security scan and start a new one\"\n\"Show me what scanner is currently running\"\n</code></pre>"},{"location":"tutorials/mcp-streaming-guide/#multiple-scans-consuming-resources","title":"Multiple Scans Consuming Resources","text":"<pre><code>\"List all my active scans\"\n\"Cancel all running scans except the most recent one\"\n</code></pre>"},{"location":"tutorials/mcp-streaming-guide/#missing-progress-updates","title":"Missing Progress Updates","text":"<pre><code>\"Check the status of scan [scan-id]\"\n\"Is my security scan still running?\"\n</code></pre>"},{"location":"tutorials/mcp-streaming-guide/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/mcp-streaming-guide/#1-use-for-large-codebases","title":"1. Use for Large Codebases","text":"<p>Streaming is most beneficial for projects with many files or complex security configurations.</p>"},{"location":"tutorials/mcp-streaming-guide/#2-monitor-critical-scans","title":"2. Monitor Critical Scans","text":"<p>For production or release-critical code, monitor progress to catch high-severity issues early.</p>"},{"location":"tutorials/mcp-streaming-guide/#3-manage-resources","title":"3. Manage Resources","text":"<p>Don't run too many concurrent scans - they can consume significant CPU and memory.</p>"},{"location":"tutorials/mcp-streaming-guide/#4-set-appropriate-thresholds","title":"4. Set Appropriate Thresholds","text":"<p>Use higher severity thresholds (HIGH or CRITICAL) for faster scans when you only need the most important issues.</p>"},{"location":"tutorials/mcp-streaming-guide/#integration-with-development-workflow","title":"Integration with Development Workflow","text":""},{"location":"tutorials/mcp-streaming-guide/#code-review-process","title":"Code Review Process","text":"<ol> <li>Start streaming scan when beginning code review</li> <li>Monitor for critical issues while reviewing</li> <li>Address high-priority security findings first</li> </ol>"},{"location":"tutorials/mcp-streaming-guide/#cicd-integration","title":"CI/CD Integration","text":"<ol> <li>Start background scans early in the pipeline</li> <li>Monitor progress and fail fast on critical issues</li> <li>Generate detailed reports for successful builds</li> </ol>"},{"location":"tutorials/mcp-streaming-guide/#daily-development","title":"Daily Development","text":"<ol> <li>Start scans on modified directories</li> <li>Work on other tasks while scans run</li> <li>Review findings when convenient</li> </ol>"},{"location":"tutorials/mcp-streaming-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about Using ASH with MCP for basic setup</li> <li>Explore ASH Configuration for customizing scans</li> <li>Check out Running ASH in CI for automation workflows</li> </ul> <p>The streaming functionality makes security scanning a more integrated part of your development workflow, providing immediate feedback without blocking your productivity.</p>"},{"location":"tutorials/running-ash-in-ci/","title":"Running ASH in CI","text":"<p>This guide explains how to integrate ASH v3 into various CI/CD platforms.</p>"},{"location":"tutorials/running-ash-in-ci/#continuous-integration-ci-execution","title":"Continuous Integration (CI) Execution","text":"<p>ASH supports running in CI environments as an executable container (e.g., via <code>docker run</code>) as well as via Container Job mechanisms, depending on CI platform support.</p>"},{"location":"tutorials/running-ash-in-ci/#building-ash-container-images-for-ci-usage","title":"Building ASH Container Images for CI Usage","text":"<p>Building ASH images for use in CI platforms requires targeting the <code>ci</code> stage of the <code>Dockerfile</code>:</p> <pre><code># Via ash CLI\nash build-image --build-target ci\n\n# Via docker or other OCI CLI\ndocker build --tag automated-security-helper:ci --target ci .\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#github-actions","title":"GitHub Actions","text":""},{"location":"tutorials/running-ash-in-ci/#basic-integration","title":"Basic Integration","text":"<pre><code>name: ASH Security Scan\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n      - name: Install ASH\n        run: pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n      - name: Run ASH scan\n        run: ash --mode local\n      - name: Upload scan results\n        uses: actions/upload-artifact@v3\n        with:\n          name: ash-results\n          path: .ash/ash_output\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#using-container-mode","title":"Using Container Mode","text":"<pre><code>name: ASH Security Scan (Container)\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n      - name: Install ASH\n        run: pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n      - name: Run ASH scan\n        run: ash --mode container\n      - name: Upload scan results\n        uses: actions/upload-artifact@v3\n        with:\n          name: ash-results\n          path: .ash/ash_output\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#adding-scan-results-to-pr-comments","title":"Adding Scan Results to PR Comments","text":"<pre><code>name: ASH Security Scan with PR Comments\n\non:\n  pull_request:\n    branches: [ main ]\n\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n      - name: Install ASH\n        run: pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n      - name: Run ASH scan\n        run: ash --mode local\n      - name: Add PR comment\n        uses: actions/github-script@v6\n        if: always()\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n          script: |\n            const fs = require('fs');\n            const reportPath = '.ash/ash_output/reports/ash.summary.md';\n\n            if (fs.existsSync(reportPath)) {\n              const reportContent = fs.readFileSync(reportPath, 'utf8');\n              const issueNumber = context.issue.number;\n\n              github.rest.issues.createComment({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: issueNumber,\n                body: reportContent\n              });\n            }\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#gitlab-ci","title":"GitLab CI","text":""},{"location":"tutorials/running-ash-in-ci/#basic-integration_1","title":"Basic Integration","text":"<pre><code>ash-scan:\n  image: python:3.10\n  script:\n    - pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n    - ash --mode local\n  artifacts:\n    paths:\n      - .ash/ash_output\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#using-container-mode_1","title":"Using Container Mode","text":"<pre><code>ash-scan-container:\n  image: docker:20.10.16\n  services:\n    - docker:20.10.16-dind\n  variables:\n    DOCKER_TLS_CERTDIR: \"/certs\"\n  script:\n    - apk add --no-cache python3 py3-pip git\n    - pip3 install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n    - ash --mode container\n  artifacts:\n    paths:\n      - .ash/ash_output\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#gitlab-security-dashboard","title":"Gitlab Security Dashboard","text":"<p>According to Gitlab documentation, if any of the jobs in a pipeline fails, the results will not be visible in the Security Dashboard.</p> <p>A pipeline consists of multiple jobs, including SAST and DAST scanning. If any job fails to finish for any reason, the security dashboard does not show SAST scanner output. For example, if the SAST job finishes but the DAST job fails, the security dashboard does not show SAST results. On failure, the analyzer outputs an exit code.</p> <p>If you want to see the results of ASH in Gitlab's Security Dashboard, you must pass the <code>--no-fail-on-findings</code> to ASH.</p> <p>Example using local mode:</p> <pre><code>ash-scan:\n  image: python:3.10\n  script:\n    - pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n    - ash --mode local --no-fail-on-findings\n  artifacts:\n    paths:\n      - .ash/ash_output\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#aws-codebuild","title":"AWS CodeBuild","text":""},{"location":"tutorials/running-ash-in-ci/#basic-integration_2","title":"Basic Integration","text":"<pre><code>version: 0.2\n\nphases:\n  install:\n    runtime-versions:\n      python: 3.10\n    commands:\n      - pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n\n  build:\n    commands:\n      - ash --mode local\n\nartifacts:\n  files:\n    - .ash/ash_output/**/*\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#using-container-mode_2","title":"Using Container Mode","text":"<pre><code>version: 0.2\n\nphases:\n  install:\n    runtime-versions:\n      python: 3.10\n    commands:\n      - pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n\n  pre_build:\n    commands:\n      - nohup /usr/local/bin/dockerd --host=unix:///var/run/docker.sock --host=tcp://127.0.0.1:2375 --storage-driver=overlay2 &amp;\n      - timeout 15 sh -c \"until docker info; do echo .; sleep 1; done\"\n\n  build:\n    commands:\n      - ash --mode container\n\nartifacts:\n  files:\n    - .ash/ash_output/**/*\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#jenkins","title":"Jenkins","text":""},{"location":"tutorials/running-ash-in-ci/#jenkinsfile-declarative-pipeline","title":"Jenkinsfile (Declarative Pipeline)","text":"<pre><code>pipeline {\n    agent {\n        docker {\n            image 'python:3.10'\n        }\n    }\n    stages {\n        stage('Install ASH') {\n            steps {\n                sh 'pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2'\n            }\n        }\n        stage('Run ASH Scan') {\n            steps {\n                sh 'ash --mode local'\n            }\n        }\n    }\n    post {\n        always {\n            archiveArtifacts artifacts: '.ash/ash_output/**/*', allowEmptyArchive: true\n        }\n    }\n}\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#using-container-mode_3","title":"Using Container Mode","text":"<pre><code>pipeline {\n    agent {\n        docker {\n            image 'docker:20.10.16'\n            args '-v /var/run/docker.sock:/var/run/docker.sock'\n        }\n    }\n    stages {\n        stage('Install ASH') {\n            steps {\n                sh 'apk add --no-cache python3 py3-pip git'\n                sh 'pip3 install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2'\n            }\n        }\n        stage('Run ASH Scan') {\n            steps {\n                sh 'ash --mode container'\n            }\n        }\n    }\n    post {\n        always {\n            archiveArtifacts artifacts: '.ash/ash_output/**/*', allowEmptyArchive: true\n        }\n    }\n}\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#circleci","title":"CircleCI","text":""},{"location":"tutorials/running-ash-in-ci/#basic-integration_3","title":"Basic Integration","text":"<pre><code>version: 2.1\njobs:\n  scan:\n    docker:\n      - image: cimg/python:3.10\n    steps:\n      - checkout\n      - run:\n          name: Install ASH\n          command: pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n      - run:\n          name: Run ASH scan\n          command: ash --mode local\n      - store_artifacts:\n          path: .ash/ash_output\n          destination: ash-results\n\nworkflows:\n  version: 2\n  scan-workflow:\n    jobs:\n      - scan\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#using-container-mode_4","title":"Using Container Mode","text":"<pre><code>version: 2.1\njobs:\n  scan:\n    machine:\n      image: ubuntu-2204:current\n    steps:\n      - checkout\n      - run:\n          name: Install ASH\n          command: pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n      - run:\n          name: Run ASH scan\n          command: ash --mode container\n      - store_artifacts:\n          path: .ash/ash_output\n          destination: ash-results\n\nworkflows:\n  version: 2\n  scan-workflow:\n    jobs:\n      - scan\n</code></pre>"},{"location":"tutorials/running-ash-in-ci/#best-practices-for-ci-integration","title":"Best Practices for CI Integration","text":"<ol> <li> <p>Fail builds on critical findings:    <pre><code>ash --mode local --fail-on-findings\n</code></pre></p> </li> <li> <p>Use specific scanners for faster CI runs:    <pre><code>ash --mode local --scanners bandit,semgrep,detect-secrets\n</code></pre></p> </li> <li> <p>Generate CI-friendly reports:    <pre><code>ash --mode local --output-formats sarif,markdown,json\n</code></pre></p> </li> <li> <p>Cache container images to speed up builds:    <pre><code># GitHub Actions example\n- name: Cache ASH container\n  uses: actions/cache@v3\n  with:\n    path: /var/lib/docker\n    key: ${{ runner.os }}-ash-container\n</code></pre></p> </li> <li> <p>Set severity thresholds appropriate for your CI pipeline:    <pre><code>ash --config-overrides 'global_settings.severity_threshold=HIGH'\n</code></pre></p> </li> </ol>"},{"location":"tutorials/running-ash-in-ci/#ash-execution-environment-viability","title":"ASH Execution Environment Viability","text":"<p>If you are unsure whether ASH will run in your CI environment, the primary requirement is the ability to run Linux containers for container mode. For local mode, you only need Python 3.10+.</p> <p>For container mode, ensure your CI environment: 1. Has a container runtime installed (Docker, Podman, etc.) 2. Has permissions to run containers 3. Has sufficient disk space for container images</p> <p>For local mode, ensure your CI environment: 1. Has Python 3.10+ installed 2. Has permissions to install Python packages</p>"},{"location":"tutorials/running-ash-locally/","title":"Running ASH Locally","text":"<p>Please see the Installation Guide page to ensure your local workspace is configured as needed before continuing.</p> <p>ASH v3 can run in multiple modes: <code>local</code>, <code>container</code>, or <code>precommit</code>. This guide covers how to install and run ASH locally.</p>"},{"location":"tutorials/running-ash-locally/#installation-options","title":"Installation Options","text":""},{"location":"tutorials/running-ash-locally/#option-1-using-uvx-recommended","title":"Option 1: Using <code>uvx</code> (Recommended)","text":"<pre><code># Install uv if you don't have it\ncurl -sSf https://astral.sh/uv/install.sh | sh\n\n# Create an alias for ASH\nalias ash=\"uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\"\n\n# Add this alias to your shell profile (~/.bashrc, ~/.zshrc, etc.)\n</code></pre>"},{"location":"tutorials/running-ash-locally/#option-2-using-pipx","title":"Option 2: Using <code>pipx</code>","text":"<pre><code># Install pipx if you don't have it\npython -m pip install --user pipx\npython -m pipx ensurepath\n\n# Install ASH\npipx install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n</code></pre>"},{"location":"tutorials/running-ash-locally/#option-3-using-pip","title":"Option 3: Using <code>pip</code>","text":"<pre><code>pip install git+https://github.com/awslabs/automated-security-helper.git@v3.2.2\n</code></pre>"},{"location":"tutorials/running-ash-locally/#option-4-clone-the-repository-legacy-method","title":"Option 4: Clone the Repository (Legacy Method)","text":"<pre><code># Set up some variables\nREPO_DIR=\"${HOME}\"/Documents/repos/reference\nREPO_NAME=automated-security-helper\n\n# Create a folder to hold reference git repositories\nmkdir -p ${REPO_DIR}\n\n# Clone the repository into the reference area\ngit clone https://github.com/awslabs/automated-security-helper.git \"${REPO_DIR}/${REPO_NAME}\"\n\n# Set the repo path in your shell for easier access\nexport PATH=\"${PATH}:${REPO_DIR}/${REPO_NAME}\"\n\n# Add this to your shell profile for persistence\n</code></pre>"},{"location":"tutorials/running-ash-locally/#running-ash","title":"Running ASH","text":"<p>After installation, you can run ASH in different modes:</p>"},{"location":"tutorials/running-ash-locally/#local-mode-default","title":"Local Mode (Default)","text":"<p>Local mode runs scanners that are available in your PATH:</p> <pre><code># Basic scan\nash --source-dir /path/to/code\n\n# Specify output directory\nash --source-dir /path/to/code --output-dir /path/to/output\n</code></pre>"},{"location":"tutorials/running-ash-locally/#container-mode","title":"Container Mode","text":"<p>Container mode ensures all scanners are available by running in a container:</p> <pre><code>ash --mode container --source-dir /path/to/code\n</code></pre>"},{"location":"tutorials/running-ash-locally/#initializing-configuration","title":"Initializing Configuration","text":"<p>Create a default configuration file:</p> <pre><code>ash config init\n</code></pre> <p>This creates <code>.ash/.ash.yaml</code> in your current directory with default settings.</p>"},{"location":"tutorials/running-ash-locally/#windows-support","title":"Windows Support","text":"<p>ASH v3 provides improved Windows support:</p>"},{"location":"tutorials/running-ash-locally/#local-mode-on-windows","title":"Local Mode on Windows","text":"<p>ASH v3 runs natively on Windows with Python 3.10+:</p> <pre><code># Install uv if you don't have it\nirm https://astral.sh/uv/install.ps1 | iex\n\n# Create a function for ASH\nfunction ash { uvx git+https://github.com/awslabs/automated-security-helper.git@v3.2.2 $args }\n\n# Use as normal\nash --help\n</code></pre>"},{"location":"tutorials/running-ash-locally/#container-mode-on-windows","title":"Container Mode on Windows","text":"<p>For container mode, you'll need:</p> <ol> <li>Windows Subsystem for Linux (WSL2) installed</li> <li>A container runtime like Docker Desktop with WSL2 integration enabled</li> </ol> <p>To use ASH in container mode on Windows:</p> <ol> <li>Install and configure WSL 2</li> <li>Install and configure Docker Desktop for Windows with WSL2 integration</li> <li>Open Windows Terminal and connect to your WSL2 environment</li> <li>Install ASH using one of the methods above</li> <li>Run ASH with <code>--mode container</code> flag</li> </ol> <p>Note: When using container mode on Windows, clone repositories into the WSL2 filesystem for best results. Scanning Windows filesystem paths from WSL2 may produce unpredictable results.</p> <p>Tip: If you use VS Code, you can configure a remote connection to WSL2 to work with repositories stored in the WSL2 filesystem.</p>"},{"location":"tutorials/running-ash-locally/#viewing-results","title":"Viewing Results","text":"<p>ASH v3 outputs results to <code>.ash/ash_output/</code> by default:</p> <ul> <li><code>ash_aggregated_results.json</code>: Complete machine-readable results</li> <li><code>reports/ash.summary.txt</code>: Human-readable text summary</li> <li><code>reports/ash.summary.md</code>: Markdown summary</li> <li><code>reports/ash.html</code>: Interactive HTML report</li> </ul> <p>You can also use the report command to view results:</p> <pre><code>ash report\n</code></pre>"},{"location":"tutorials/running-ash-locally/#next-steps","title":"Next Steps","text":"<p>After running ASH locally:</p> <ol> <li>Learn about ASH's CLI options</li> <li>Explore configuration options</li> <li>Set up pre-commit integration</li> </ol>"},{"location":"tutorials/using-ash-with-mcp/","title":"Using ASH with MCP","text":"<p>This tutorial guides you through using the Automated Security Helper (ASH) with the Model Context Protocol (MCP) server. The MCP server provides a reliable interface for AI assistants to perform security scans on your codebase.</p>"},{"location":"tutorials/using-ash-with-mcp/#overview","title":"Overview","text":"<p>ASH's MCP server implementation uses a file-based approach to track scan progress and completion, making it more reliable than event-based tracking. This approach ensures that scan progress and results are accurately tracked even in complex threading scenarios.</p>"},{"location":"tutorials/using-ash-with-mcp/#prerequisites","title":"Prerequisites","text":"<ul> <li>ASH v3.0.0 or later installed</li> <li>Basic understanding of ASH security scanning</li> <li>An AI assistant that supports MCP tools</li> </ul>"},{"location":"tutorials/using-ash-with-mcp/#mcp-tools-overview","title":"MCP Tools Overview","text":"<p>ASH provides the following MCP tools:</p> <ol> <li>scan_directory: Start a security scan asynchronously</li> <li>get_scan_progress: Check the progress of a running scan</li> <li>get_scan_results: Get the results of a completed scan</li> <li>list_active_scans: List all active and recent scans</li> <li>cancel_scan: Cancel a running scan</li> <li>check_installation: Check if ASH is properly installed</li> </ol>"},{"location":"tutorials/using-ash-with-mcp/#starting-a-scan","title":"Starting a Scan","text":"<p>To start a security scan, use the <code>scan_directory</code> tool:</p> <pre><code>result = await mcp_scan_directory(\n    directory_path=\"/path/to/your/code\",\n    severity_threshold=\"MEDIUM\",  # Optional, default is \"MEDIUM\"\n    config_path=None  # Optional, path to ASH configuration file\n)\n</code></pre> <p>The tool returns a dictionary with: - <code>scan_id</code>: A unique identifier for tracking the scan - <code>status</code>: Initial status of the scan (usually \"pending\") - <code>directory_path</code>: Path to the directory being scanned - <code>output_directory</code>: Path where scan results will be stored - <code>start_time</code>: When the scan was started</p> <p>Example response: <pre><code>{\n  \"success\": true,\n  \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"pending\",\n  \"directory_path\": \"/path/to/your/code\",\n  \"output_directory\": \"/path/to/your/code/.ash/ash_output\",\n  \"severity_threshold\": \"MEDIUM\",\n  \"config_path\": null,\n  \"start_time\": \"2025-07-16T12:34:56.789012\",\n  \"message\": \"Scan started successfully. Use get_scan_progress to track progress.\"\n}\n</code></pre></p>"},{"location":"tutorials/using-ash-with-mcp/#tracking-scan-progress","title":"Tracking Scan Progress","text":"<p>To check the progress of a running scan, use the <code>get_scan_progress</code> tool:</p> <pre><code>progress = await mcp_get_scan_progress(\n    scan_id=\"550e8400-e29b-41d4-a716-446655440000\"\n)\n</code></pre> <p>The tool returns a dictionary with: - <code>status</code>: Current status of the scan (\"pending\", \"running\", \"completed\", \"failed\", or \"cancelled\") - <code>completed_scanners</code>: Number of scanners that have completed - <code>total_scanners</code>: Total number of scanners being run - <code>scanners</code>: Detailed information about each scanner's progress - <code>duration</code>: How long the scan has been running</p> <p>Example response: <pre><code>{\n  \"success\": true,\n  \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"running\",\n  \"completed_scanners\": 2,\n  \"total_scanners\": 5,\n  \"scanners\": {\n    \"bandit\": {\n      \"status\": \"completed\",\n      \"target_type\": \"source\",\n      \"finding_count\": 3,\n      \"severity_counts\": {\n        \"LOW\": 1,\n        \"MEDIUM\": 2,\n        \"HIGH\": 0,\n        \"CRITICAL\": 0\n      }\n    },\n    \"detect-secrets\": {\n      \"status\": \"completed\",\n      \"target_type\": \"source\",\n      \"finding_count\": 0,\n      \"severity_counts\": {}\n    },\n    \"semgrep\": {\n      \"status\": \"running\",\n      \"target_type\": \"source\"\n    },\n    \"checkov\": {\n      \"status\": \"pending\",\n      \"target_type\": \"source\"\n    },\n    \"cdk-nag\": {\n      \"status\": \"pending\",\n      \"target_type\": \"source\"\n    }\n  },\n  \"start_time\": \"2025-07-16T12:34:56.789012\",\n  \"current_time\": \"2025-07-16T12:36:23.456789\",\n  \"duration\": 86.667777,\n  \"timestamp\": \"2025-07-16T12:36:23.456789\"\n}\n</code></pre></p>"},{"location":"tutorials/using-ash-with-mcp/#getting-scan-results","title":"Getting Scan Results","text":"<p>Once a scan is complete, use the <code>get_scan_results</code> tool to retrieve the results:</p> <pre><code>results = await mcp_get_scan_results(\n    scan_id=\"550e8400-e29b-41d4-a716-446655440000\"\n)\n</code></pre> <p>The tool returns a dictionary with: - <code>status</code>: Status of the scan (should be \"completed\") - <code>finding_count</code>: Total number of findings - <code>severity_counts</code>: Counts of findings by severity - <code>findings</code>: Detailed information about each finding - <code>scan_duration</code>: How long the scan took to complete</p> <p>Example response: <pre><code>{\n  \"success\": true,\n  \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"completed\",\n  \"finding_count\": 5,\n  \"severity_counts\": {\n    \"LOW\": 1,\n    \"MEDIUM\": 3,\n    \"HIGH\": 1,\n    \"CRITICAL\": 0\n  },\n  \"findings\": [\n    {\n      \"id\": \"finding-1\",\n      \"title\": \"Hardcoded password\",\n      \"severity\": \"HIGH\",\n      \"scanner\": \"detect-secrets\",\n      \"file_path\": \"config.py\",\n      \"line_number\": 42,\n      \"description\": \"Hardcoded password found in configuration file\"\n    },\n    // Additional findings...\n  ],\n  \"scan_duration\": 124.567890,\n  \"start_time\": \"2025-07-16T12:34:56.789012\",\n  \"end_time\": \"2025-07-16T12:37:01.356902\",\n  \"timestamp\": \"2025-07-16T12:37:01.356902\"\n}\n</code></pre></p>"},{"location":"tutorials/using-ash-with-mcp/#managing-active-scans","title":"Managing Active Scans","text":"<p>To list all active and recent scans, use the <code>list_active_scans</code> tool:</p> <pre><code>scans = await mcp_list_active_scans()\n</code></pre> <p>The tool returns a dictionary with: - <code>active_scans</code>: List of currently running scans - <code>all_scans</code>: List of all scans in the registry (including completed ones) - <code>stats</code>: Statistics about scan counts and statuses</p> <p>Example response: <pre><code>{\n  \"success\": true,\n  \"active_scans\": [\n    {\n      \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"status\": \"running\",\n      \"directory_path\": \"/path/to/your/code\",\n      \"start_time\": \"2025-07-16T12:34:56.789012\"\n    }\n  ],\n  \"all_scans\": [\n    {\n      \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"status\": \"running\",\n      \"directory_path\": \"/path/to/your/code\",\n      \"start_time\": \"2025-07-16T12:34:56.789012\"\n    },\n    {\n      \"scan_id\": \"660e8400-e29b-41d4-a716-446655440000\",\n      \"status\": \"completed\",\n      \"directory_path\": \"/path/to/another/code\",\n      \"start_time\": \"2025-07-16T11:22:33.444555\",\n      \"end_time\": \"2025-07-16T11:25:12.345678\"\n    }\n  ],\n  \"stats\": {\n    \"total_scans\": 2,\n    \"active_scans\": 1,\n    \"status_counts\": {\n      \"running\": 1,\n      \"completed\": 1,\n      \"failed\": 0,\n      \"cancelled\": 0\n    }\n  },\n  \"timestamp\": \"2025-07-16T12:36:23.456789\"\n}\n</code></pre></p>"},{"location":"tutorials/using-ash-with-mcp/#cancelling-a-scan","title":"Cancelling a Scan","text":"<p>To cancel a running scan, use the <code>cancel_scan</code> tool:</p> <pre><code>result = await mcp_cancel_scan(\n    scan_id=\"550e8400-e29b-41d4-a716-446655440000\"\n)\n</code></pre> <p>The tool returns a dictionary with: - <code>success</code>: Whether the cancellation was successful - <code>scan_id</code>: ID of the cancelled scan - <code>status</code>: New status of the scan (should be \"cancelled\")</p> <p>Example response: <pre><code>{\n  \"success\": true,\n  \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"cancelled\",\n  \"message\": \"Scan cancelled successfully\",\n  \"timestamp\": \"2025-07-16T12:36:45.123456\"\n}\n</code></pre></p>"},{"location":"tutorials/using-ash-with-mcp/#checking-ash-installation","title":"Checking ASH Installation","text":"<p>To check if ASH is properly installed, use the <code>check_installation</code> tool:</p> <pre><code>result = await mcp_check_installation()\n</code></pre> <p>The tool returns a dictionary with: - <code>installed</code>: Whether ASH is installed - <code>version</code>: ASH version - <code>ash_command_available</code>: Whether the ASH command is available in the PATH - <code>ash_dir_exists</code>: Whether the ASH directory exists</p> <p>Example response: <pre><code>{\n  \"success\": true,\n  \"installed\": true,\n  \"version\": \"3.0.0\",\n  \"ash_command_available\": true,\n  \"ash_command_output\": \"ASH v3.0.0\",\n  \"ash_dir_exists\": true,\n  \"timestamp\": \"2025-07-16T12:37:12.345678\"\n}\n</code></pre></p>"},{"location":"tutorials/using-ash-with-mcp/#file-based-tracking-approach","title":"File-Based Tracking Approach","text":"<p>ASH's MCP server uses a file-based approach to track scan progress and completion. This approach is more reliable than event-based tracking because it doesn't depend on in-memory state that could be lost due to threading issues.</p> <p>The file-based tracking works as follows:</p> <ol> <li>When a scan is started, a unique scan ID is generated and registered in the scan registry</li> <li>The scan process is started asynchronously</li> <li>The scan progress is tracked by checking for the existence of result files:</li> <li><code>ash_aggregated_results.json</code>: Indicates the scan has completed</li> <li>Individual scanner result files: Indicate which scanners have completed</li> <li>The scan results are retrieved by parsing these files</li> </ol> <p>This approach ensures that scan progress and results are accurately tracked even if events are missed or not properly received due to threading issues.</p>"},{"location":"tutorials/using-ash-with-mcp/#error-handling","title":"Error Handling","text":"<p>ASH's MCP server provides comprehensive error handling for various scenarios:</p> <ul> <li>Invalid parameters: Validates all input parameters and returns meaningful error messages</li> <li>File not found: Handles missing files gracefully</li> <li>Permission denied: Checks for appropriate permissions before accessing files</li> <li>Invalid format: Validates file formats and provides detailed error messages</li> <li>Scan not found: Checks if the scan exists in the registry</li> <li>Scan incomplete: Verifies if the scan has completed before returning results</li> <li>Unexpected errors: Catches and reports unexpected errors with context</li> </ul> <p>Each error response includes: - <code>success</code>: Always <code>false</code> for error responses - <code>error</code>: Error message - <code>error_type</code>: Type of error - <code>error_category</code>: Category of error (e.g., \"file_not_found\", \"permission_denied\") - <code>suggestions</code>: List of suggestions for resolving the error</p> <p>Example error response: <pre><code>{\n  \"success\": false,\n  \"operation\": \"get_scan_results\",\n  \"error\": \"Scan 550e8400-e29b-41d4-a716-446655440000 not found\",\n  \"error_type\": \"MCPResourceError\",\n  \"error_category\": \"scan_not_found\",\n  \"context\": {\n    \"scan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n    \"error_category\": \"scan_not_found\"\n  },\n  \"suggestions\": [\n    \"Check that the scan ID is correct\",\n    \"Verify that the scan exists in the registry\",\n    \"The scan may have been cleaned up if it was completed a long time ago\"\n  ],\n  \"timestamp\": \"2025-07-16T12:37:12.345678\"\n}\n</code></pre></p>"},{"location":"tutorials/using-ash-with-mcp/#best-practices","title":"Best Practices","text":"<p>When using ASH with MCP, follow these best practices:</p> <ol> <li>Store scan IDs: Always store the scan ID returned by <code>scan_directory</code> for later use</li> <li>Check scan progress: Periodically check scan progress using <code>get_scan_progress</code></li> <li>Handle errors gracefully: Check the <code>success</code> field in responses and handle errors appropriately</li> <li>Clean up resources: Cancel scans that are no longer needed using <code>cancel_scan</code></li> <li>Validate parameters: Ensure all parameters are valid before calling MCP tools</li> <li>Check installation: Use <code>check_installation</code> to verify ASH is properly installed</li> </ol>"},{"location":"tutorials/using-ash-with-mcp/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about ASH configuration options</li> <li>Explore MCP performance and scalability</li> </ul>"},{"location":"tutorials/using-ash-with-pre-commit/","title":"Using <code>ash</code> with <code>pre-commit</code>","text":"<p>The <code>ash</code> tool can be used interactively on a workstation or run using the <code>pre-commit</code> command. If <code>pre-commit</code> is used to run <code>ash</code>, then the <code>pre-commit</code> processing takes care of installing a copy of the <code>ash</code> git repository and setting up to run the <code>ash</code> program from that installed repository.  Using <code>pre-commit</code> still requires usage of WSL 2 when running on Windows.</p> <p>Using <code>ash</code> as a <code>pre-commit</code> hook enables development teams to use the <code>ash</code> tool in two ways.  First, developers can use <code>ash</code> as a part of their local development process on whatever development workstation or environment they are using.  Second, <code>ash</code> can be run in a build automation stage by running <code>pre-commit run --hook-stage manual ash</code> in build automation stage. When using <code>pre-commit</code>, run the <code>pre-commit</code> commands while in a folder/directory within the git repository that is configured with <code>pre-commit</code> hooks.</p> <p>Refer to the pre-commit-hooks file for information about the <code>pre-commit</code> hook itself.</p>"},{"location":"tutorials/using-ash-with-pre-commit/#configuration","title":"Configuration","text":"<p>To configure a git repository to use the <code>ash</code> hook, start with the following <code>pre-commit-config</code> configuration:</p> <pre><code>repos:\n  - repo: https://github.com/awslabs/automated-security-helper\n    rev: v3.0.0  # update with the latest tagged version in the repository\n    hooks:\n      - id: ash-simple-scan\n</code></pre>"},{"location":"tutorials/using-ash-with-pre-commit/#running-the-pre-commit-hook","title":"Running the Pre-commit Hook","text":"<p>Once the <code>.pre-commit-config.yaml</code> file is updated, the <code>ash</code> tool can be run using the following command:</p> <pre><code>pre-commit run ash-simple-scan --all-files\n</code></pre>"},{"location":"tutorials/using-ash-with-pre-commit/#output-files","title":"Output Files","text":"<p>Results from the run of the <code>ash</code> tool can be found in the <code>.ash/ash_output/</code> directory:</p> <ul> <li><code>ash_aggregated_results.json</code>: Complete machine-readable results</li> <li><code>reports/ash.summary.txt</code>: Human-readable text summary</li> <li><code>reports/ash.summary.md</code>: Markdown summary for GitHub PRs and other platforms</li> <li><code>reports/ash.html</code>: Interactive HTML report</li> <li><code>reports/ash.csv</code>: CSV report for filtering and sorting findings</li> </ul>"},{"location":"user-guide/scanner-statistics/","title":"Scanner Statistics","text":"<p>This document explains how scanner statistics are calculated and displayed in ASH reports and terminal output.</p>"},{"location":"user-guide/scanner-statistics/#overview","title":"Overview","text":"<p>ASH provides comprehensive statistics for each scanner that is executed during a scan. These statistics include:</p> <ul> <li>Severity Counts: Number of findings at each severity level (Critical, High, Medium, Low, Info)</li> <li>Suppressed Findings: Number of findings that have been suppressed</li> <li>Actionable Findings: Number of findings at or above the threshold severity level</li> <li>Duration: Time taken by the scanner to complete its execution</li> <li>Status: Whether the scanner passed, failed, was skipped, or had missing dependencies</li> </ul> <p>These statistics are calculated consistently across all reports and terminal output, ensuring that you see the same numbers regardless of which report format you are viewing.</p>"},{"location":"user-guide/scanner-statistics/#severity-levels","title":"Severity Levels","text":"<p>ASH uses the following severity levels for findings:</p> <ul> <li>Critical: Highest severity findings that require immediate attention</li> <li>High: Serious findings that should be addressed soon</li> <li>Medium: Moderate risk findings</li> <li>Low: Lower risk findings</li> <li>Info: Informational findings with minimal risk</li> </ul>"},{"location":"user-guide/scanner-statistics/#suppressed-findings","title":"Suppressed Findings","text":"<p>Suppressed findings are those that have been explicitly marked as suppressed, either through:</p> <ol> <li>Native Scanner Suppressions: Suppressions applied by the scanner itself</li> <li>ASH-Applied Suppressions: Suppressions applied by ASH based on configuration</li> </ol> <p>Suppressed findings are counted separately and do not contribute to the total or actionable finding counts.</p>"},{"location":"user-guide/scanner-statistics/#actionable-findings","title":"Actionable Findings","text":"<p>Actionable findings are those that are at or above the threshold severity level. The threshold can be set:</p> <ol> <li>Globally: In the <code>global_settings.severity_threshold</code> section of the ASH configuration</li> <li>Per Scanner: In the scanner's configuration section</li> </ol> <p>The threshold values and their meanings are:</p> <ul> <li>ALL: All findings are actionable (Critical, High, Medium, Low, Info)</li> <li>LOW: Findings of low severity or higher are actionable (Critical, High, Medium, Low)</li> <li>MEDIUM: Findings of medium severity or higher are actionable (Critical, High, Medium)</li> <li>HIGH: Findings of high severity or higher are actionable (Critical, High)</li> <li>CRITICAL: Only critical findings are actionable</li> </ul>"},{"location":"user-guide/scanner-statistics/#scanner-status","title":"Scanner Status","text":"<p>The status of a scanner is determined based on its findings and configuration:</p> <ul> <li>PASSED: The scanner did not find any actionable findings</li> <li>FAILED: The scanner found actionable findings (at or above the threshold)</li> <li>SKIPPED: The scanner was explicitly excluded from the scan</li> <li>MISSING: The scanner has missing dependencies</li> </ul>"},{"location":"user-guide/scanner-statistics/#statistics-calculation","title":"Statistics Calculation","text":"<p>All scanner statistics are calculated from the final aggregated SARIF report, ensuring that they reflect the final state of findings after all suppressions and filters have been applied.</p> <p>The calculation process follows these steps:</p> <ol> <li>Extract all findings from the SARIF report</li> <li>Identify which scanner each finding belongs to</li> <li>Count findings by severity level for each scanner</li> <li>Calculate actionable findings based on the threshold</li> <li>Determine scanner status based on actionable findings and configuration</li> </ol>"},{"location":"user-guide/scanner-statistics/#terminal-output","title":"Terminal Output","text":"<p>The terminal output displays a table with scanner statistics after a scan is completed. The table includes:</p> <ul> <li>Scanner: Name of the scanner</li> <li>Suppressed: Number of suppressed findings</li> <li>Critical: Number of critical findings</li> <li>High: Number of high findings</li> <li>Medium: Number of medium findings</li> <li>Low: Number of low findings</li> <li>Info: Number of informational findings</li> <li>Duration: Time taken by the scanner</li> <li>Actionable: Number of actionable findings</li> <li>Result: Scanner status (PASSED, FAILED, SKIPPED, MISSING)</li> <li>Threshold: Severity threshold used for this scanner</li> </ul>"},{"location":"user-guide/scanner-statistics/#report-formats","title":"Report Formats","text":"<p>ASH provides several report formats, each displaying scanner statistics in a slightly different way:</p> <ul> <li>HTML: Interactive table with color-coded severity levels and status</li> <li>Markdown: Formatted table with scanner statistics</li> <li>Text: Plain text table with scanner statistics</li> <li>JSON: Structured data with scanner statistics</li> </ul> <p>All report formats use the same underlying data, ensuring consistency across different views.</p>"},{"location":"user-guide/scanner-statistics/#interpreting-statistics","title":"Interpreting Statistics","text":"<p>When interpreting scanner statistics, keep the following in mind:</p> <ol> <li>Actionable Findings: Focus on actionable findings, as these are the ones that require attention based on your severity threshold.</li> <li>Suppressed Findings: Suppressed findings are not included in the total or actionable counts, as they have been explicitly marked as not requiring attention.</li> <li>Scanner Status: A scanner with a FAILED status has actionable findings that need to be addressed.</li> <li>Threshold Source: The threshold source indicates where the threshold was defined (global or scanner-specific configuration).</li> </ol>"},{"location":"user-guide/scanner-statistics/#example","title":"Example","text":"<p>Here's an example of how to interpret scanner statistics:</p> <pre><code>Scanner: bandit\nSuppressed: 5\nCritical: 0\nHigh: 2\nMedium: 3\nLow: 1\nInfo: 0\nDuration: 1.2s\nActionable: 5\nResult: FAILED\nThreshold: MEDIUM (global)\n</code></pre> <p>In this example: - The bandit scanner found 2 high, 3 medium, and 1 low severity findings - 5 findings were suppressed - The threshold is MEDIUM, set in the global configuration - Since the threshold is MEDIUM, the actionable findings include high and medium severity (2 + 3 = 5) - The scanner status is FAILED because it found actionable findings</p>"},{"location":"user-guide/scanner-statistics/#conclusion","title":"Conclusion","text":"<p>Scanner statistics provide valuable information about the security posture of your codebase. By understanding how these statistics are calculated and displayed, you can better interpret the results of your security scans and prioritize remediation efforts.</p>"}]}